Id,PostTypeId,AcceptedAnswerId,ParentId,CreationDate,DeletionDate,Score,ViewCount,Body,OwnerUserId,OwnerDisplayName,LastEditorUserId,LastEditorDisplayName,LastEditDate,LastActivityDate,Title,Tags,AnswerCount,CommentCount,FavoriteCount,ClosedDate,CommunityOwnedDate,ContentLicense
"11287278","2","","11285613","2012-07-02 02:43:02","","1961","","<p>The column names (which are strings) cannot be sliced in the manner you tried.</p>
<p>Here you have a couple of options. If you know from context which variables you want to slice out, you can just return a view of only those columns by passing a list into the <a href=""https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#basics"" rel=""noreferrer""><code>__getitem__</code> syntax</a> (the []'s).</p>
<pre><code>df1 = df[['a', 'b']]
</code></pre>
<p>Alternatively, if it matters to index them numerically and not by their name (say your code should automatically do this without knowing the names of the first two columns) then you can do this instead:</p>
<pre><code>df1 = df.iloc[:, 0:2] # Remember that Python does not slice inclusive of the ending index.
</code></pre>
<p>Additionally, you should familiarize yourself with the idea of a view into a Pandas object vs. a copy of that object. The first of the above methods will return a new copy in memory of the desired sub-object (the desired slices).</p>
<p>Sometimes, however, there are indexing conventions in Pandas that don't do this and instead give you a new variable that just refers to the same chunk of memory as the sub-object or slice in the original object. This will happen with the second way of indexing, so you can modify it with the <code>copy()</code> function to get a regular copy. When this happens, changing what you think is the sliced object can sometimes alter the original object. Always good to be on the look out for this.</p>
<pre><code>df1 = df.iloc[0, 0:2].copy() # To avoid the case where changing df1 also changes df
</code></pre>
<p>To use <code>iloc</code>, you need to know the column positions (or indices). As the column positions may change, instead of hard-coding indices, you can use <code>iloc</code> along with <code>get_loc</code> function of <code>columns</code> method of dataframe object to obtain column indices.</p>
<pre><code>{df.columns.get_loc(c): c for idx, c in enumerate(df.columns)}
</code></pre>
<p>Now you can use this dictionary to access columns through names and using <code>iloc</code>.</p>
","567620","","13765085","","2020-07-03 13:06:14","2020-07-03 13:06:14","","","","19","","","","CC BY-SA 4.0"
"11385335","2","","11285613","2012-07-08 17:55:12","","66","","<pre><code>In [39]: df
Out[39]: 
   index  a  b  c
0      1  2  3  4
1      2  3  4  5

In [40]: df1 = df[['b', 'c']]

In [41]: df1
Out[41]: 
   b  c
0  3  4
1  4  5
</code></pre>
","776560","","","","","2012-07-08 17:55:12","","","","2","","","","CC BY-SA 3.0"
"13165753","2","","11285613","2012-10-31 18:57:33","","114","","<p>Assuming your column names (<code>df.columns</code>) are <code>['index','a','b','c']</code>, then the data you want is in the 
3rd &amp; 4th columns. If you don't know their names when your script runs, you can do this</p>

<pre><code>newdf = df[df.columns[2:4]] # Remember, Python is 0-offset! The ""3rd"" entry is at slot 2.
</code></pre>

<p>As EMS points out in <a href=""https://stackoverflow.com/a/11287278/623735"">his answer</a>, <code>df.ix</code> slices columns a bit more concisely, but the <code>.columns</code> slicing interface might be more natural because it uses the vanilla 1-D python list indexing/slicing syntax.</p>

<p>WARN: <code>'index'</code> is a bad name for a <code>DataFrame</code> column. That same label is also used for the real <code>df.index</code> attribute, a <code>Index</code> array. So your column is returned by <code>df['index']</code> and the real DataFrame index is returned by <code>df.index</code>. An <code>Index</code> is a special kind of <code>Series</code> optimized for lookup of it's elements' values. For df.index it's for looking up rows by their label. That <code>df.columns</code> attribute is also a <code>pd.Index</code> array, for looking up columns by their labels.</p>
","623735","","-1","","2017-05-23 12:10:48","2017-03-29 17:26:41","","","","5","","","","CC BY-SA 3.0"
"25643178","2","","11285613","2014-09-03 11:30:59","","24","","<p>You could provide a list of columns to be dropped and return back the DataFrame with only the columns needed using the <code>drop()</code> function on a Pandas DataFrame.</p>

<p>Just saying</p>

<pre><code>colsToDrop = ['a']
df.drop(colsToDrop, axis=1)
</code></pre>

<p>would return a DataFrame with just the columns <code>b</code> and <code>c</code>.</p>

<p>The <code>drop</code> method is documented <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html"" rel=""noreferrer"">here</a>.</p>
","590215","","3923281","","2014-11-03 22:16:59","2014-11-03 22:16:59","","","","0","","","","CC BY-SA 3.0"
"35203149","2","","11285613","2016-02-04 14:05:35","","71","","<p>I realize this question is quite old, but in the latest version of pandas there is an easy way to do exactly this. Column names (which are strings) <strong>can</strong> be sliced in whatever manner you like.</p>

<pre><code>columns = ['b', 'c']
df1 = pd.DataFrame(df, columns=columns)
</code></pre>
","3616299","","","","","2016-02-04 14:05:35","","","","1","","","","CC BY-SA 3.0"
"36955053","2","","11285613","2016-04-30 12:39:08","","168","","<p>As of version 0.11.0, columns <em>can be</em> sliced in the manner you tried using the <a href=""http://pandas-docs.github.io/pandas-docs-travis/whatsnew/v0.11.0.html#selection-choices"" rel=""noreferrer""><code>.loc</code></a> indexer: </p>

<pre><code>df.loc[:, 'C':'E']
</code></pre>

<p>is equivalent of</p>

<pre><code>df[['C', 'D', 'E']]  # or df.loc[:, ['C', 'D', 'E']]
</code></pre>

<p>and returns columns <code>C</code> through <code>E</code>.</p>

<hr>

<p>A demo on a randomly generated DataFrame:</p>

<pre><code>import pandas as pd
import numpy as np
np.random.seed(5)
df = pd.DataFrame(np.random.randint(100, size=(100, 6)), 
                  columns=list('ABCDEF'), 
                  index=['R{}'.format(i) for i in range(100)])
df.head()

Out: 
     A   B   C   D   E   F
R0  99  78  61  16  73   8
R1  62  27  30  80   7  76
R2  15  53  80  27  44  77
R3  75  65  47  30  84  86
R4  18   9  41  62   1  82
</code></pre>

<p>To get the columns from C to E (note that unlike integer slicing, 'E' is included in the columns):</p>

<pre><code>df.loc[:, 'C':'E']

Out: 
      C   D   E
R0   61  16  73
R1   30  80   7
R2   80  27  44
R3   47  30  84
R4   41  62   1
R5    5  58   0
...
</code></pre>

<p>Same works for selecting rows based on labels. Get the rows 'R6' to 'R10' from those columns:</p>

<pre><code>df.loc['R6':'R10', 'C':'E']

Out: 
      C   D   E
R6   51  27  31
R7   83  19  18
R8   11  67  65
R9   78  27  29
R10   7  16  94
</code></pre>

<p><code>.loc</code> also accepts a boolean array so you can select the columns whose corresponding entry in the array is <code>True</code>. For example, <code>df.columns.isin(list('BCD'))</code> returns <code>array([False,  True,  True,  True, False, False], dtype=bool)</code> - True if the column name is in the list <code>['B', 'C', 'D']</code>; False, otherwise.</p>

<pre><code>df.loc[:, df.columns.isin(list('BCD'))]

Out: 
      B   C   D
R0   78  61  16
R1   27  30  80
R2   53  80  27
R3   65  47  30
R4    9  41  62
R5   78   5  58
...
</code></pre>
","2285236","","2285236","","2019-01-25 11:12:36","2019-01-25 11:12:36","","","","0","","","","CC BY-SA 4.0"
"43734680","2","","11285613","2017-05-02 09:41:52","","21","","<p>I found this method to be very useful:</p>

<pre class=""lang-py prettyprint-override""><code># iloc[row slicing, column slicing]
surveys_df.iloc [0:3, 1:4]
</code></pre>

<p>More details can be found <a href=""http://www.datacarpentry.org/python-ecology-lesson/02-index-slice-subset/"" rel=""noreferrer"">here</a></p>
","4294506","","7747942","","2018-04-02 18:38:13","2018-04-02 18:38:13","","","","2","","","","CC BY-SA 3.0"
"48073336","2","","11285613","2018-01-03 07:56:07","","9","","<p>If you want to get one element by row index and column name, you can do it just like <code>df['b'][0]</code>. It is as simple as you can image. </p>

<p>Or you can use <code>df.ix[0,'b']</code>,mixed usage of index and label.</p>

<p><strong>Note:</strong> Since v0.20 <code>ix</code> has been deprecated in favour of <code>loc</code> / <code>iloc</code>.</p>
","7720976","","9209546","","2018-08-09 14:38:55","2018-08-09 14:38:55","","","","0","","","","CC BY-SA 4.0"
"51864208","2","","11285613","2018-08-15 18:13:41","","19","","<p>Starting with 0.21.0, using <code>.loc</code> or <code>[]</code> with a list with one or more missing labels is deprecated in favor of <code>.reindex</code>. So, the answer to your question is:</p>

<p><code>df1 = df.reindex(columns=['b','c'])</code></p>

<p>In prior versions, using <code>.loc[list-of-labels]</code> would work as long as at least 1 of the keys was found (otherwise it would raise a <code>KeyError</code>). This behavior is deprecated and now shows a warning message. The recommended alternative is to use <code>.reindex()</code>.</p>

<p>Read more at <a href=""https://pandas.pydata.org/pandas-docs/stable/indexing.html#reindexing"" rel=""noreferrer"">Indexing and Selecting Data</a></p>
","1054154","","1054154","","2019-09-18 14:35:26","2019-09-18 14:35:26","","","","0","","","","CC BY-SA 4.0"
"52816036","2","","11285613","2018-10-15 11:43:43","","7","","<p>One different and easy approach : iterating rows</p>
<h1>using iterows</h1>
<pre><code> df1= pd.DataFrame() #creating an empty dataframe
 for index,i in df.iterrows():
    df1.loc[index,'A']=df.loc[index,'A']
    df1.loc[index,'B']=df.loc[index,'B']
    df1.head()
</code></pre>
","7920927","","-1","","2020-06-20 09:12:55","2020-01-14 03:01:21","","","","3","","","","CC BY-SA 4.0"
"53415454","2","","11285613","2018-11-21 15:32:56","","38","","<p>With pandas, </p>

<p>wit column names </p>

<pre><code>dataframe[['column1','column2']]
</code></pre>

<p>to select by iloc and specific columns with index number:</p>

<pre><code>dataframe.iloc[:,[1,2]]
</code></pre>

<p>with loc column names can be used like</p>

<pre><code>dataframe.loc[:,['column1','column2']]
</code></pre>
","1727543","","154157","","2020-03-29 01:47:10","2020-03-29 01:47:10","","","","0","","","","CC BY-SA 4.0"
"54777265","2","","11285613","2019-02-20 01:01:58","","12","","<p>You can use pandas.
I create the DataFrame:</p>

<pre><code>    import pandas as pd
    df = pd.DataFrame([[1, 2,5], [5,4, 5], [7,7, 8], [7,6,9]], 
                      index=['Jane', 'Peter','Alex','Ann'],
                      columns=['Test_1', 'Test_2', 'Test_3'])
</code></pre>

<p>The DataFrame:</p>

<pre><code>           Test_1  Test_2  Test_3
    Jane        1       2       5
    Peter       5       4       5
    Alex        7       7       8
    Ann         7       6       9
</code></pre>

<p>To select 1 or more columns by name:</p>

<pre><code>    df[['Test_1','Test_3']]

           Test_1  Test_3
    Jane        1       5
    Peter       5       5
    Alex        7       8
    Ann         7       9
</code></pre>

<p>You can also use:</p>

<pre><code>    df.Test_2
</code></pre>

<p>And yo get column <code>Test_2</code></p>

<pre><code>    Jane     2
    Peter    4
    Alex     7
    Ann      6
</code></pre>

<p>You can also select columns and rows from these rows using <strong><code>.loc()</code></strong>. This is called <strong>""slicing""</strong>. Notice that I take from column <code>Test_1</code>to <code>Test_3</code></p>

<pre><code>    df.loc[:,'Test_1':'Test_3']
</code></pre>

<p>The ""Slice"" is:</p>

<pre><code>            Test_1  Test_2  Test_3
     Jane        1       2       5
     Peter       5       4       5
     Alex        7       7       8
     Ann         7       6       9
</code></pre>

<p>And if you just want <code>Peter</code> and <code>Ann</code> from columns <code>Test_1</code> and <code>Test_3</code>:</p>

<pre><code>    df.loc[['Peter', 'Ann'],['Test_1','Test_3']]
</code></pre>

<p>You get:</p>

<pre><code>           Test_1  Test_3
    Peter       5       5
    Ann         7       9
</code></pre>
","6177877","","","","","2019-02-20 01:01:58","","","","0","","","","CC BY-SA 4.0"
"61335275","2","","11285613","2020-04-21 03:03:00","","11","","<p>You can use <code>pandas.DataFrame.filter</code> method to either filter or reorder columns like this:</p>
<pre><code>df1 = df.filter(['a', 'b'])
</code></pre>
<p>This is also very useful when you are chaining methods.</p>
","7782271","","7782271","","2020-07-10 18:26:18","2020-07-10 18:26:18","","","","0","","","","CC BY-SA 4.0"
"11346337","2","","11346283","2012-07-05 14:23:27","","2041","","<p>Just assign it to the <code>.columns</code> attribute:</p>
<pre><code>&gt;&gt;&gt; df = pd.DataFrame({'$a':[1,2], '$b': [10,20]})
&gt;&gt;&gt; df.columns = ['a', 'b']
&gt;&gt;&gt; df
   a   b
0  1  10
1  2  20
</code></pre>
","449449","","449449","","2020-07-03 19:15:46","2020-07-03 19:15:46","","","","12","","","","CC BY-SA 3.0"
"11354850","2","","11346283","2012-07-06 01:48:15","","3116","","<h2><strong>RENAME SPECIFIC COLUMNS</strong></h2>
<p>Use the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rename.html"" rel=""noreferrer""><code>df.rename()</code></a> function and refer the columns to be renamed. Not all the columns have to be renamed:</p>
<pre><code>df = df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'})
# Or rename the existing DataFrame (rather than creating a copy) 
df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'}, inplace=True)
</code></pre>
<p><strong>Minimal Code Example</strong></p>
<pre><code>df = pd.DataFrame('x', index=range(3), columns=list('abcde'))
df

   a  b  c  d  e
0  x  x  x  x  x
1  x  x  x  x  x
2  x  x  x  x  x
</code></pre>
<p>The following methods all work and produce the same output:</p>
<pre><code>df2 = df.rename({'a': 'X', 'b': 'Y'}, axis=1)  # new method
df2 = df.rename({'a': 'X', 'b': 'Y'}, axis='columns')
df2 = df.rename(columns={'a': 'X', 'b': 'Y'})  # old method  

df2

   X  Y  c  d  e
0  x  x  x  x  x
1  x  x  x  x  x
2  x  x  x  x  x
</code></pre>
<p>Remember to assign the result back, as the modification is not-inplace. Alternatively, specify <code>inplace=True</code>:</p>
<pre><code>df.rename({'a': 'X', 'b': 'Y'}, axis=1, inplace=True)
df

   X  Y  c  d  e
0  x  x  x  x  x
1  x  x  x  x  x
2  x  x  x  x  x
 
</code></pre>
<p>From v0.25, you can also specify <code>errors='raise'</code> to raise errors if an invalid column-to-rename is specified. See <a href=""https://pandas-docs.github.io/pandas-docs-travis/reference/api/pandas.DataFrame.rename.html#pandas.DataFrame.rename"" rel=""noreferrer"">v0.25 <code>rename()</code> docs</a>.</p>
<hr />
<h2>REASSIGN COLUMN HEADERS</h2>
<p>Use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.set_axis.html"" rel=""noreferrer""><code>df.set_axis()</code></a> with <code>axis=1</code> and <code>inplace=False</code> (to return a copy).</p>
<pre><code>df2 = df.set_axis(['V', 'W', 'X', 'Y', 'Z'], axis=1, inplace=False)
df2

   V  W  X  Y  Z
0  x  x  x  x  x
1  x  x  x  x  x
2  x  x  x  x  x
</code></pre>
<p>This returns a copy, but you can modify the DataFrame in-place by setting <code>inplace=True</code> (this is the default behaviour for versions &lt;=0.24 but is likely to change in the future).</p>
<p>You can also assign headers directly:</p>
<pre><code>df.columns = ['V', 'W', 'X', 'Y', 'Z']
df

   V  W  X  Y  Z
0  x  x  x  x  x
1  x  x  x  x  x
2  x  x  x  x  x
</code></pre>
","1505540","","-1","","2020-06-20 09:12:55","2019-05-29 19:50:01","","","","8","","","","CC BY-SA 4.0"
"16667215","2","","11346283","2013-05-21 09:58:59","","427","","<p>The <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rename.html"" rel=""noreferrer""><code>rename</code></a> method can take a <strong><em>function</em></strong>, for example:</p>

<pre><code>In [11]: df.columns
Out[11]: Index([u'$a', u'$b', u'$c', u'$d', u'$e'], dtype=object)

In [12]: df.rename(columns=lambda x: x[1:], inplace=True)

In [13]: df.columns
Out[13]: Index([u'a', u'b', u'c', u'd', u'e'], dtype=object)
</code></pre>
","1240268","","202229","","2019-10-20 22:06:04","2019-10-20 22:06:04","","","","3","","","","CC BY-SA 4.0"
"22657894","2","","11346283","2014-03-26 10:20:45","","136","","<p>Since you only want to remove the $ sign in all column names, you could just do:</p>

<pre><code>df = df.rename(columns=lambda x: x.replace('$', ''))
</code></pre>

<p>OR</p>

<pre><code>df.rename(columns=lambda x: x.replace('$', ''), inplace=True)
</code></pre>
","2700072","","","","","2014-03-26 10:20:45","","","","1","","","","CC BY-SA 3.0"
"30380922","2","","11346283","2015-05-21 17:48:33","","68","","<pre><code>old_names = ['$a', '$b', '$c', '$d', '$e'] 
new_names = ['a', 'b', 'c', 'd', 'e']
df.rename(columns=dict(zip(old_names, new_names)), inplace=True)
</code></pre>

<p>This way you can manually edit the <code>new_names</code> as you wish.
Works great when you need to rename only a few columns to correct mispellings, accents, remove special characters etc.</p>
","4116995","","4116995","","2015-05-21 17:54:25","2015-05-21 17:54:25","","","","5","","","","CC BY-SA 3.0"
"30546734","2","","11346283","2015-05-30 13:24:05","","207","","<p>As documented in <a href=""http://pandas.pydata.org/pandas-docs/stable/text.html"" rel=""noreferrer"">Working with text data</a>:</p>

<pre><code>df.columns = df.columns.str.replace('$','')
</code></pre>
","1551810","","9698684","","2020-04-10 18:41:02","2020-04-10 18:41:02","","","","0","","","","CC BY-SA 4.0"
"32322596","2","","11346283","2015-09-01 02:24:17","","20","","<p>If you've got the dataframe, df.columns dumps everything into a list you can manipulate and then reassign into your dataframe as the names of columns...</p>

<pre><code>columns = df.columns
columns = [row.replace(""$"","""") for row in columns]
df.rename(columns=dict(zip(columns, things)), inplace=True)
df.head() #to validate the output
</code></pre>

<p>Best way? IDK. A way - yes.</p>

<p>A better way of evaluating all the main techniques put forward in the answers to the question is below using cProfile to gage memory &amp; execution time. @kadee, @kaitlyn, &amp; @eumiro had the functions with the fastest execution times - though these functions are so fast we're comparing the rounding of .000 and .001 seconds for all the answers. Moral: my answer above likely isn't the 'Best' way.</p>

<pre><code>import pandas as pd
import cProfile, pstats, re

old_names = ['$a', '$b', '$c', '$d', '$e']
new_names = ['a', 'b', 'c', 'd', 'e']
col_dict = {'$a': 'a', '$b': 'b','$c':'c','$d':'d','$e':'e'}

df = pd.DataFrame({'$a':[1,2], '$b': [10,20],'$c':['bleep','blorp'],'$d':[1,2],'$e':['texa$','']})

df.head()

def eumiro(df,nn):
    df.columns = nn
    #This direct renaming approach is duplicated in methodology in several other answers: 
    return df

def lexual1(df):
    return df.rename(columns=col_dict)

def lexual2(df,col_dict):
    return df.rename(columns=col_dict, inplace=True)

def Panda_Master_Hayden(df):
    return df.rename(columns=lambda x: x[1:], inplace=True)

def paulo1(df):
    return df.rename(columns=lambda x: x.replace('$', ''))

def paulo2(df):
    return df.rename(columns=lambda x: x.replace('$', ''), inplace=True)

def migloo(df,on,nn):
    return df.rename(columns=dict(zip(on, nn)), inplace=True)

def kadee(df):
    return df.columns.str.replace('$','')

def awo(df):
    columns = df.columns
    columns = [row.replace(""$"","""") for row in columns]
    return df.rename(columns=dict(zip(columns, '')), inplace=True)

def kaitlyn(df):
    df.columns = [col.strip('$') for col in df.columns]
    return df

print 'eumiro'
cProfile.run('eumiro(df,new_names)')
print 'lexual1'
cProfile.run('lexual1(df)')
print 'lexual2'
cProfile.run('lexual2(df,col_dict)')
print 'andy hayden'
cProfile.run('Panda_Master_Hayden(df)')
print 'paulo1'
cProfile.run('paulo1(df)')
print 'paulo2'
cProfile.run('paulo2(df)')
print 'migloo'
cProfile.run('migloo(df,old_names,new_names)')
print 'kadee'
cProfile.run('kadee(df)')
print 'awo'
cProfile.run('awo(df)')
print 'kaitlyn'
cProfile.run('kaitlyn(df)')
</code></pre>
","2917381","","2917381","","2016-09-07 02:24:19","2016-09-07 02:24:19","","","","2","","","","CC BY-SA 3.0"
"33872824","2","","11346283","2015-11-23 13:56:10","","16","","<p>Another way we could replace the original column labels is by stripping the unwanted characters (here '$') from the original column labels.</p>

<p>This could have been done by running a for loop over df.columns and appending the stripped columns to df.columns.</p>

<p>Instead , we can do this neatly in a single statement by using list comprehension like below:</p>

<pre><code>df.columns = [col.strip('$') for col in df.columns]
</code></pre>

<p>(<code>strip</code> method in Python strips the given character from beginning and end of the string.)</p>
","3533960","","2664350","","2017-07-05 13:19:35","2017-07-05 13:19:35","","","","1","","","","CC BY-SA 3.0"
"33986975","2","","11346283","2015-11-29 19:22:47","","14","","<p>Real simple just use </p>

<pre><code>df.columns = ['Name1', 'Name2', 'Name3'...]
</code></pre>

<p>and it will assign the column names by the order you put them</p>
","5548422","","","","","2015-11-29 19:22:47","","","","0","","","","CC BY-SA 3.0"
"35068123","2","","11346283","2016-01-28 17:31:39","","12","","<p>You could use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.slice.html"" rel=""noreferrer""><code>str.slice</code></a> for that:</p>

<pre><code>df.columns = df.columns.str.slice(1)
</code></pre>
","4542359","","","","","2016-01-28 17:31:39","","","","1","","","","CC BY-SA 3.0"
"35387028","2","","11346283","2016-02-14 00:31:53","","20","","<pre><code>df = pd.DataFrame({'$a': [1], '$b': [1], '$c': [1], '$d': [1], '$e': [1]})
</code></pre>

<p>If your new list of columns is in the same order as the existing columns, the assignment is simple:</p>

<pre><code>new_cols = ['a', 'b', 'c', 'd', 'e']
df.columns = new_cols
&gt;&gt;&gt; df
   a  b  c  d  e
0  1  1  1  1  1
</code></pre>

<p>If you had a dictionary keyed on old column names to new column names, you could do the following:</p>

<pre><code>d = {'$a': 'a', '$b': 'b', '$c': 'c', '$d': 'd', '$e': 'e'}
df.columns = df.columns.map(lambda col: d[col])  # Or `.map(d.get)` as pointed out by @PiRSquared.
&gt;&gt;&gt; df
   a  b  c  d  e
0  1  1  1  1  1
</code></pre>

<p>If you don't have a list or dictionary mapping, you could strip the leading <code>$</code> symbol via a list comprehension:</p>

<pre><code>df.columns = [col[1:] if col[0] == '$' else col for col in df]
</code></pre>
","2411802","","2411802","","2017-09-13 12:24:31","2017-09-13 12:24:31","","","","1","","","","CC BY-SA 3.0"
"36149967","2","","11346283","2016-03-22 08:59:12","","82","","<pre><code>df.columns = ['a', 'b', 'c', 'd', 'e']
</code></pre>

<p>It will replace the existing names with the names you provide, in the order you provide.</p>
","5440236","","461983","","2018-10-12 05:45:57","2018-10-12 05:45:57","","","","1","","","","CC BY-SA 4.0"
"38776854","2","","11346283","2016-08-04 20:26:50","","11","","<p>I know this question and answer has been chewed to death. But I referred to it for inspiration for one of the problem I was having . I was able to solve it using bits and pieces from different answers hence providing my response in case anyone needs it.</p>

<p>My method is generic wherein you can add additional delimiters by comma separating <code>delimiters=</code> variable and future-proof it.</p>

<p><strong>Working Code:</strong></p>

<pre><code>import pandas as pd
import re


df = pd.DataFrame({'$a':[1,2], '$b': [3,4],'$c':[5,6], '$d': [7,8], '$e': [9,10]})

delimiters = '$'
matchPattern = '|'.join(map(re.escape, delimiters))
df.columns = [re.split(matchPattern, i)[1] for i in df.columns ]
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>&gt;&gt;&gt; df
   $a  $b  $c  $d  $e
0   1   3   5   7   9
1   2   4   6   8  10

&gt;&gt;&gt; df
   a  b  c  d   e
0  1  3  5  7   9
1  2  4  6  8  10
</code></pre>
","5936628","","","","","2016-08-04 20:26:50","","","","0","","","","CC BY-SA 3.0"
"39215492","2","","11346283","2016-08-29 21:27:20","","10","","<p>Note that these approach do not work for a MultiIndex. For a MultiIndex, you need to do something like the following:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame({('$a','$x'):[1,2], ('$b','$y'): [3,4], ('e','f'):[5,6]})
&gt;&gt;&gt; df
   $a $b  e
   $x $y  f
0  1  3  5
1  2  4  6
&gt;&gt;&gt; rename = {('$a','$x'):('a','x'), ('$b','$y'):('b','y')}
&gt;&gt;&gt; df.columns = pandas.MultiIndex.from_tuples([
        rename.get(item, item) for item in df.columns.tolist()])
&gt;&gt;&gt; df
   a  b  e
   x  y  f
0  1  3  5
1  2  4  6
</code></pre>
","5885546","","","","","2016-08-29 21:27:20","","","","0","","","","CC BY-SA 3.0"
"39770407","2","","11346283","2016-09-29 12:30:40","","37","","<h1>Column names vs Names of Series</h1>
<p>I would like to explain a bit what happens behind the scenes.</p>
<p>Dataframes are a set of Series.</p>
<p>Series in turn are an extension of a <code>numpy.array</code></p>
<p><code>numpy.array</code>s have a property <code>.name</code></p>
<p>This is the name of the series. It is seldom that pandas respects this attribute, but it lingers in places and can be used to hack some pandas behaviors.</p>
<h1>Naming the list of columns</h1>
<p>A lot of answers here talks about the <code>df.columns</code> attribute being a <code>list</code> when in fact it is a <code>Series</code>. This means it has a <code>.name</code> attribute.</p>
<p>This is what happens if you decide to fill in the name of the columns <code>Series</code>:</p>
<pre><code>df.columns = ['column_one', 'column_two']
df.columns.names = ['name of the list of columns']
df.index.names = ['name of the index']

name of the list of columns     column_one  column_two
name of the index       
0                                    4           1
1                                    5           2
2                                    6           3
</code></pre>
<p>Note that the name of the index always comes one column lower.</p>
<h2>Artifacts that linger</h2>
<p>The <code>.name</code> attribute lingers on sometimes. If you set <code>df.columns = ['one', 'two']</code> then the <code>df.one.name</code> will be <code>'one'</code>.</p>
<p>If you set <code>df.one.name = 'three'</code> then <code>df.columns</code> will still give you <code>['one', 'two']</code>, and <code>df.one.name</code> will give you <code>'three'</code></p>
<h3>BUT</h3>
<p><code>pd.DataFrame(df.one)</code> will return</p>
<pre><code>    three
0       1
1       2
2       3
</code></pre>
<p>Because pandas reuses the <code>.name</code> of the already defined <code>Series</code>.</p>
<h1>Multi level column names</h1>
<p>Pandas has ways of doing multi layered column names. There is not so much magic involved but I wanted to cover this in my answer too since I don't see anyone picking up on this here.</p>
<pre><code>    |one            |
    |one      |two  |
0   |  4      |  1  |
1   |  5      |  2  |
2   |  6      |  3  |
</code></pre>
<p>This is easily achievable by setting columns to lists, like this:</p>
<pre><code>df.columns = [['one', 'one'], ['one', 'two']]
</code></pre>
","3730397","","-1","","2020-06-20 09:12:55","2016-09-29 12:30:40","","","","0","","","","CC BY-SA 3.0"
"44584447","2","","11346283","2017-06-16 08:27:37","","9","","<p>If you have to deal with loads of columns named by the providing system out of your control, I came up with the following approach that is a combination of a general approach and specific replacments in one go.</p>

<p>First create a dictionary from the dataframe column names using regex expressions in order to throw away certain appendixes of column names 
and then add specific replacements to the dictionary to name core columns as expected later in the receiving database.</p>

<p>This is then applied to the dataframe in one go.</p>

<pre><code>dict=dict(zip(df.columns,df.columns.str.replace('(:S$|:C1$|:L$|:D$|\.Serial:L$)','')))
dict['brand_timeseries:C1']='BTS'
dict['respid:L']='RespID'
dict['country:C1']='CountryID'
dict['pim1:D']='pim_actual'
df.rename(columns=dict, inplace=True)
</code></pre>
","7047332","","10201580","","2019-07-05 11:46:22","2019-07-05 11:46:22","","","","0","","","","CC BY-SA 4.0"
"46192213","2","","11346283","2017-09-13 08:09:23","","38","","<h1>One line or Pipeline solutions</h1>

<p>I'll focus on two things:</p>

<ol>
<li><p>OP clearly states</p>

<blockquote>
  <p>I have the edited column names stored it in a list, but I don't know how to replace the column names.  </p>
</blockquote>

<p>I do not want to solve the problem of how to replace <code>'$'</code> or strip the first character off of each column header.  OP has already done this step.  Instead I want to focus on replacing the existing <code>columns</code> object with a new one given a list of replacement column names.</p></li>
<li><p><code>df.columns = new</code> where <code>new</code> is the list of new columns names is as simple as it gets.  The drawback of this approach is that it requires editing the existing dataframe's <code>columns</code> attribute and it isn't done inline.  I'll show a few ways to perform this via pipelining without editing the existing dataframe.</p></li>
</ol>

<hr>

<p><strong>Setup 1</strong><br>
To focus on the need to rename of replace column names with a pre-existing list, I'll create a new sample dataframe <code>df</code> with initial column names and unrelated new column names.</p>

<pre><code>df = pd.DataFrame({'Jack': [1, 2], 'Mahesh': [3, 4], 'Xin': [5, 6]})
new = ['x098', 'y765', 'z432']

df

   Jack  Mahesh  Xin
0     1       3    5
1     2       4    6
</code></pre>

<hr>

<p><strong>Solution 1</strong><br>
<a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rename.html"" rel=""noreferrer""><strong><code>pd.DataFrame.rename</code></strong></a>  </p>

<p>It has been said already that <strong>if</strong> you had a dictionary mapping the old column names to new column names, you could use <code>pd.DataFrame.rename</code>.</p>

<pre><code>d = {'Jack': 'x098', 'Mahesh': 'y765', 'Xin': 'z432'}
df.rename(columns=d)

   x098  y765  z432
0     1     3     5
1     2     4     6
</code></pre>

<p>However, you can easily create that dictionary and include it in the call to <code>rename</code>.  The following takes advantage of the fact that when iterating over <code>df</code>, we iterate over each column name.</p>

<pre><code># given just a list of new column names
df.rename(columns=dict(zip(df, new)))

   x098  y765  z432
0     1     3     5
1     2     4     6
</code></pre>

<p>This works great if your original column names are unique.  But if they are not, then this breaks down.</p>

<hr>

<p><strong>Setup 2</strong><br>
non-unique columns  </p>

<pre><code>df = pd.DataFrame(
    [[1, 3, 5], [2, 4, 6]],
    columns=['Mahesh', 'Mahesh', 'Xin']
)
new = ['x098', 'y765', 'z432']

df

   Mahesh  Mahesh  Xin
0       1       3    5
1       2       4    6
</code></pre>

<hr>

<p><strong>Solution 2</strong><br>
<a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html"" rel=""noreferrer""><strong><code>pd.concat</code></strong></a> using the <code>keys</code> argument  </p>

<p>First, notice what happens when we attempt to use solution 1:</p>

<pre><code>df.rename(columns=dict(zip(df, new)))

   y765  y765  z432
0     1     3     5
1     2     4     6
</code></pre>

<p>We didn't map the <code>new</code> list as the column names.  We ended up repeating <code>y765</code>.  Instead, we can use the <code>keys</code> argument of the <code>pd.concat</code> function while iterating through the columns of <code>df</code>.</p>

<pre><code>pd.concat([c for _, c in df.items()], axis=1, keys=new) 

   x098  y765  z432
0     1     3     5
1     2     4     6
</code></pre>

<hr>

<p><strong>Solution 3</strong><br>
Reconstruct.  This should only be used if you have a single <code>dtype</code> for all columns.  Otherwise, you'll end up with <code>dtype</code> <code>object</code> for all columns and converting them back requires more dictionary work.</p>

<p><em>Single <code>dtype</code></em>  </p>

<pre><code>pd.DataFrame(df.values, df.index, new)

   x098  y765  z432
0     1     3     5
1     2     4     6
</code></pre>

<p><em>Mixed <code>dtype</code></em>  </p>

<pre><code>pd.DataFrame(df.values, df.index, new).astype(dict(zip(new, df.dtypes)))

   x098  y765  z432
0     1     3     5
1     2     4     6
</code></pre>

<hr>

<p><strong>Solution 4</strong><br>
This is a gimmicky trick with <code>transpose</code> and <code>set_index</code>.  <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.set_index.html"" rel=""noreferrer""><strong><code>pd.DataFrame.set_index</code></strong></a> allows us to set an index inline but there is no corresponding <code>set_columns</code>.  So we can transpose, then <code>set_index</code>, and transpose back.  However, the same single <code>dtype</code> versus mixed <code>dtype</code> caveat from solution 3 applies here.</p>

<p><em>Single <code>dtype</code></em>  </p>

<pre><code>df.T.set_index(np.asarray(new)).T

   x098  y765  z432
0     1     3     5
1     2     4     6
</code></pre>

<p><em>Mixed <code>dtype</code></em>  </p>

<pre><code>df.T.set_index(np.asarray(new)).T.astype(dict(zip(new, df.dtypes)))

   x098  y765  z432
0     1     3     5
1     2     4     6
</code></pre>

<hr>

<p><strong>Solution 5</strong><br>
Use a <code>lambda</code> in <code>pd.DataFrame.rename</code> that cycles through each element of <code>new</code><br>
In this solution, we pass a lambda that takes <code>x</code> but then ignores it.  It also takes a <code>y</code> but doesn't expect it.  Instead, an iterator is given as a default value and I can then use that to cycle through one at a time without regard to what the value of <code>x</code> is.</p>

<pre><code>df.rename(columns=lambda x, y=iter(new): next(y))

   x098  y765  z432
0     1     3     5
1     2     4     6
</code></pre>

<p>And as pointed out to me by the folks in <em>sopython</em> chat, if I add a <code>*</code> in between <code>x</code> and <code>y</code>, I can protect my <code>y</code> variable.  Though, in this context I don't believe it needs protecting.  It is still worth mentioning.</p>

<pre><code>df.rename(columns=lambda x, *, y=iter(new): next(y))

   x098  y765  z432
0     1     3     5
1     2     4     6
</code></pre>
","2336654","","2336654","","2017-09-13 09:10:15","2017-09-13 09:10:15","","","","2","","","","CC BY-SA 3.0"
"46912050","2","","11346283","2017-10-24 13:39:15","","172","","<h1>Pandas 0.21+ Answer</h1>

<p>There have been some significant updates to column renaming in version 0.21. </p>

<ul>
<li>The <a href=""http://pandas.pydata.org/pandas-docs/version/0.21/whatsnew.html#rename-reindex-now-also-accept-axis-keyword"" rel=""noreferrer""><code>rename</code> method</a> has added the <code>axis</code> parameter which may be set to <code>columns</code> or <code>1</code>. This update makes this method match the rest of the pandas API. It still has the <code>index</code> and <code>columns</code> parameters but you are no longer forced to use them. </li>
<li>The <a href=""http://pandas.pydata.org/pandas-docs/version/0.21/generated/pandas.DataFrame.set_axis.html#pandas.DataFrame.set_axis"" rel=""noreferrer""><code>set_axis</code> method</a> with the <code>inplace</code> set to <code>False</code> enables you to rename all the index or column labels with a list.</li>
</ul>

<h2>Examples for Pandas 0.21+</h2>

<p>Construct sample DataFrame:</p>

<pre><code>df = pd.DataFrame({'$a':[1,2], '$b': [3,4], 
                   '$c':[5,6], '$d':[7,8], 
                   '$e':[9,10]})

   $a  $b  $c  $d  $e
0   1   3   5   7   9
1   2   4   6   8  10
</code></pre>

<h3>Using <code>rename</code> with <code>axis='columns'</code> or <code>axis=1</code></h3>

<pre><code>df.rename({'$a':'a', '$b':'b', '$c':'c', '$d':'d', '$e':'e'}, axis='columns')
</code></pre>

<p>or </p>

<pre><code>df.rename({'$a':'a', '$b':'b', '$c':'c', '$d':'d', '$e':'e'}, axis=1)
</code></pre>

<p>Both result in the following:</p>

<pre><code>   a  b  c  d   e
0  1  3  5  7   9
1  2  4  6  8  10
</code></pre>

<p>It is still possible to use the old method signature:</p>

<pre><code>df.rename(columns={'$a':'a', '$b':'b', '$c':'c', '$d':'d', '$e':'e'})
</code></pre>

<p>The <code>rename</code> function also accepts functions that will be applied to each column name.</p>

<pre><code>df.rename(lambda x: x[1:], axis='columns')
</code></pre>

<p>or</p>

<pre><code>df.rename(lambda x: x[1:], axis=1)
</code></pre>

<hr>

<h3>Using <code>set_axis</code> with a list and <code>inplace=False</code></h3>

<p>You can supply a list to the <code>set_axis</code> method that is equal in length to the number of columns (or index). Currently, <code>inplace</code> defaults to <code>True</code>, but <code>inplace</code> will be defaulted to <code>False</code> in future releases.</p>

<pre><code>df.set_axis(['a', 'b', 'c', 'd', 'e'], axis='columns', inplace=False)
</code></pre>

<p>or</p>

<pre><code>df.set_axis(['a', 'b', 'c', 'd', 'e'], axis=1, inplace=False)
</code></pre>

<hr>

<h3>Why not use <code>df.columns = ['a', 'b', 'c', 'd', 'e']</code>?</h3>

<p>There is nothing wrong with assigning columns directly like this. It is a perfectly good solution. </p>

<p>The advantage of using <code>set_axis</code> is that it can be used as part of a method chain and that it returns a new copy of the DataFrame. Without it, you would have to store your intermediate steps of the chain to another variable before reassigning the columns.</p>

<pre><code># new for pandas 0.21+
df.some_method1()
  .some_method2()
  .set_axis()
  .some_method3()

# old way
df1 = df.some_method1()
        .some_method2()
df1.columns = columns
df1.some_method3()
</code></pre>
","3707607","","3877338","","2017-11-17 19:31:57","2017-11-17 19:31:57","","","","5","","","","CC BY-SA 3.0"
"49915518","2","","11346283","2018-04-19 07:48:53","","6","","<p>Here's a nifty little function I like to use to cut down on typing:</p>

<pre><code>def rename(data, oldnames, newname): 
    if type(oldnames) == str: #input can be a string or list of strings 
        oldnames = [oldnames] #when renaming multiple columns 
        newname = [newname] #make sure you pass the corresponding list of new names
    i = 0 
    for name in oldnames:
        oldvar = [c for c in data.columns if name in c]
        if len(oldvar) == 0: 
            raise ValueError(""Sorry, couldn't find that column in the dataset"")
        if len(oldvar) &gt; 1: #doesn't have to be an exact match 
            print(""Found multiple columns that matched "" + str(name) + "" :"")
            for c in oldvar:
                print(str(oldvar.index(c)) + "": "" + str(c))
            ind = input('please enter the index of the column you would like to rename: ')
            oldvar = oldvar[int(ind)]
        if len(oldvar) == 1:
            oldvar = oldvar[0]
        data = data.rename(columns = {oldvar : newname[i]})
        i += 1 
    return data   
</code></pre>

<p>Here is an example of how it works: </p>

<pre><code>In [2]: df = pd.DataFrame(np.random.randint(0,10,size=(10, 4)), columns=['col1','col2','omg','idk'])
#first list = existing variables
#second list = new names for those variables
In [3]: df = rename(df, ['col','omg'],['first','ohmy']) 
Found multiple columns that matched col :
0: col1
1: col2

please enter the index of the column you would like to rename: 0

In [4]: df.columns
Out[5]: Index(['first', 'col2', 'ohmy', 'idk'], dtype='object')
</code></pre>
","1920550","","1920550","","2018-05-18 23:36:09","2018-05-18 23:36:09","","","","2","","","","CC BY-SA 4.0"
"51219353","2","","11346283","2018-07-07 02:07:23","","11","","<p>Another option is to rename using a regular expression:</p>

<pre><code>import pandas as pd
import re

df = pd.DataFrame({'$a':[1,2], '$b':[3,4], '$c':[5,6]})

df = df.rename(columns=lambda x: re.sub('\$','',x))
&gt;&gt;&gt; df
   a  b  c
0  1  3  5
1  2  4  6
</code></pre>
","3058123","","","","","2018-07-07 02:07:23","","","","0","","","","CC BY-SA 4.0"
"51414545","2","","11346283","2018-07-19 04:50:15","","19","","<pre><code>df.rename(index=str,columns={'A':'a','B':'b'})
</code></pre>

<p><a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rename.html"" rel=""noreferrer"">https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rename.html</a></p>
","7361872","","7361872","","2018-08-29 13:35:39","2018-08-29 13:35:39","","","","0","","","","CC BY-SA 4.0"
"55635790","2","","11346283","2019-04-11 15:08:57","","6","","<p>Assuming you can use regular expression. This solution removes the need of manual encoding using regex</p>

<pre><code>import pandas as pd
import re

srch=re.compile(r""\w+"")

data=pd.read_csv(""CSV_FILE.csv"")
cols=data.columns
new_cols=list(map(lambda v:v.group(),(list(map(srch.search,cols)))))
data.columns=new_cols
</code></pre>
","6242274","","6242274","","2019-04-12 03:34:15","2019-04-12 03:34:15","","","","3","","","","CC BY-SA 4.0"
"57670616","2","","11346283","2019-08-27 08:30:27","","23","","<p>Let's say this is your dataframe.</p>

<p><a href=""https://i.stack.imgur.com/vazEj.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/vazEj.png"" alt=""enter image description here""></a></p>

<p>You can rename the columns using two methods.</p>

<ol>
<li><p>Using <code>dataframe.columns=[#list]</code></p>

<pre><code>df.columns=['a','b','c','d','e']
</code></pre>

<p><a href=""https://i.stack.imgur.com/uFQzo.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/uFQzo.png"" alt=""enter image description here""></a></p>

<p>The limitation of this method is that if one column has to be changed, full column list has to be passed. Also, this method is not applicable on index labels.
For example, if you passed this:</p>

<pre><code>df.columns = ['a','b','c','d']
</code></pre>

<p>This will throw an error. Length mismatch: Expected axis has 5 elements, new values have 4 elements.</p></li>
<li><p>Another method is the Pandas <code>rename()</code> method which is used to rename any index, column or row</p>

<pre><code>df = df.rename(columns={'$a':'a'})
</code></pre>

<p><a href=""https://i.stack.imgur.com/sqPu9.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/sqPu9.png"" alt=""enter image description here""></a></p></li>
</ol>

<p>Similarly, you can change any rows or columns.</p>
","9287879","","1968","","2019-10-31 11:57:09","2019-10-31 11:57:09","","","","0","","","","CC BY-SA 4.0"
"60584778","2","","11346283","2020-03-08 05:35:42","","20","","<p>Let's Understand renaming by a small example...</p>

<p>1.Renaming columns using mapping:</p>

<pre><code>df = pd.DataFrame({""A"": [1, 2, 3], ""B"": [4, 5, 6]}) #creating a df with column name A and B
df.rename({""A"": ""new_a"", ""B"": ""new_b""},axis='columns',inplace =True) #renaming column A with 'new_a' and B with 'new_b'

output:
   new_a  new_b
0  1       4
1  2       5
2  3       6
</code></pre>

<p>2.Renaming index/Row_Name using mapping:</p>

<pre><code>df.rename({0: ""x"", 1: ""y"", 2: ""z""},axis='index',inplace =True) #Row name are getting replaced by 'x','y','z'.

output:
       new_a  new_b
    x  1       4
    y  2       5
    z  3       6
</code></pre>
","11183478","","11183478","","2020-03-08 07:21:35","2020-03-08 07:21:35","","","","0","","","","CC BY-SA 4.0"
"60589137","2","","11346283","2020-03-08 15:43:28","","8","","<p>In addition to the solution already provided, you can replace all the columns while you are reading the file. We can use <code>names</code> and <code>header=0</code> to do that.</p>

<p>First, we create a list of the names that we like to use as our column names:</p>

<pre><code>import pandas as pd

ufo_cols = ['city', 'color reported', 'shape reported', 'state', 'time']
ufo.columns = ufo_cols

ufo = pd.read_csv('link to the file you are using', names = ufo_cols, header = 0)
</code></pre>

<p>In this case, all the column names will be replaced with the names you have in your list.</p>
","1406420","","7851470","","2020-04-27 17:25:18","2020-04-27 17:25:18","","","","0","","","","CC BY-SA 4.0"
"61679284","2","","11346283","2020-05-08 12:34:49","","32","","<p>Renaming columns in pandas is an easy task.</p>
<pre><code>df.rename(columns={'$a': 'a', '$b': 'b', '$c': 'c', '$d': 'd', '$e': 'e'}, inplace=True)
</code></pre>
","10506347","","12149730","","2020-08-07 09:47:03","2020-08-07 09:47:03","","","","0","","","","CC BY-SA 4.0"
"12555491","2","","12555323","2012-09-23 19:22:27","","50","","<p>Doing this directly via <a href=""http://en.wikipedia.org/wiki/NumPy"">NumPy</a> will be the most efficient:</p>

<pre><code>df1['e'] = np.random.randn(sLength)
</code></pre>

<hr>

<p>Note my original (very old) suggestion was to use <code>map</code> (which is much slower):</p>

<pre><code>df1['e'] = df1['a'].map(lambda x: np.random.random())
</code></pre>
","1240268","","63550","","2015-10-20 13:05:01","2015-10-20 13:05:01","","","","2","","","","CC BY-SA 3.0"
"12555510","2","","12555323","2012-09-23 19:24:45","","1120","","<p>Use the original df1 indexes to create the series:</p>

<pre><code>df1['e'] = pd.Series(np.random.randn(sLength), index=df1.index)
</code></pre>

<hr>

<p><strong>Edit 2015</strong><br>
Some reported getting the <code>SettingWithCopyWarning</code> with this code.<br>
However, the code still runs perfectly with the current pandas version 0.16.1.</p>

<pre><code>&gt;&gt;&gt; sLength = len(df1['a'])
&gt;&gt;&gt; df1
          a         b         c         d
6 -0.269221 -0.026476  0.997517  1.294385
8  0.917438  0.847941  0.034235 -0.448948

&gt;&gt;&gt; df1['e'] = pd.Series(np.random.randn(sLength), index=df1.index)
&gt;&gt;&gt; df1
          a         b         c         d         e
6 -0.269221 -0.026476  0.997517  1.294385  1.757167
8  0.917438  0.847941  0.034235 -0.448948  2.228131

&gt;&gt;&gt; p.version.short_version
'0.16.1'
</code></pre>

<p>The <code>SettingWithCopyWarning</code> aims to inform of a possibly invalid assignment on a copy of the Dataframe. It doesn't necessarily say you did it wrong (it can trigger false positives) but from 0.13.0 it let you know there are more adequate methods for the same purpose. Then, if you get the warning, just follow its advise: <em>Try using .loc[row_index,col_indexer] = value instead</em></p>

<pre><code>&gt;&gt;&gt; df1.loc[:,'f'] = pd.Series(np.random.randn(sLength), index=df1.index)
&gt;&gt;&gt; df1
          a         b         c         d         e         f
6 -0.269221 -0.026476  0.997517  1.294385  1.757167 -0.050927
8  0.917438  0.847941  0.034235 -0.448948  2.228131  0.006109
&gt;&gt;&gt; 
</code></pre>

<p>In fact, this is currently the more efficient method as <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy"" rel=""noreferrer"">described in pandas docs</a></p>

<hr>

<p><strong>Edit 2017</strong></p>

<p>As indicated in the comments and by @Alexander, currently the best method to add the values of a Series as a new column of a DataFrame could be using <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.assign.html"" rel=""noreferrer""><strong><code>assign</code></strong></a>:</p>

<pre><code>df1 = df1.assign(e=pd.Series(np.random.randn(sLength)).values)
</code></pre>
","308903","","4960855","","2019-11-04 15:49:16","2019-11-04 15:49:16","","","","26","","","","CC BY-SA 4.0"
"13843741","2","","12555323","2012-12-12 16:04:31","","241","","<p>This is the simple way of adding a new column: <code>df['e'] = e</code></p>
","1645853","","1645853","","2016-12-10 06:53:58","2016-12-10 06:53:58","","","","6","","","","CC BY-SA 3.0"
"28634878","2","","12555323","2015-02-20 17:32:19","","6","","<p>One thing to note, though, is that if you do</p>

<pre><code>df1['e'] = Series(np.random.randn(sLength), index=df1.index)
</code></pre>

<p>this will effectively be a <strong>left</strong> join on the df1.index. So if you want to have an <strong>outer</strong> join effect, my probably imperfect solution is to create a dataframe with index values covering the universe of your data, and then use the code above. For example,</p>

<pre><code>data = pd.DataFrame(index=all_possible_values)
df1['e'] = Series(np.random.randn(sLength), index=df1.index)
</code></pre>
","3123992","","63550","","2015-10-20 13:05:50","2015-10-20 13:05:50","","","","0","","","","CC BY-SA 3.0"
"30777185","2","","12555323","2015-06-11 09:45:04","","22","","<p>I got the dreaded <code>SettingWithCopyWarning</code>, and it wasn't fixed by using the iloc syntax. My DataFrame was created by read_sql from an ODBC source. Using a suggestion by lowtech above, the following worked for me:</p>

<pre><code>df.insert(len(df.columns), 'e', pd.Series(np.random.randn(sLength),  index=df.index))
</code></pre>

<p>This worked fine to insert the column at the end. I don't know if it is the most efficient, but I don't like warning messages. I think there is a better solution, but I can't find it, and I think it depends on some aspect of the index.<br>
<em>Note</em>. That this only works once and will give an error message if trying to overwrite and existing column.<br>
<strong>Note</strong> As above and from 0.16.0 assign is the best solution. See documentation <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.assign.html#pandas.DataFrame.assign"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.assign.html#pandas.DataFrame.assign</a> 
Works well for data flow type where you don't overwrite your intermediate values.</p>
","3649456","","3649456","","2016-10-21 11:32:43","2016-10-21 11:32:43","","","","0","","","","CC BY-SA 3.0"
"30835776","2","","12555323","2015-06-14 23:57:36","","6","","<p>Before assigning a new column, if you have indexed data, you need to sort the index. At least in my case I had to:</p>

<pre><code>data.set_index(['index_column'], inplace=True)
""if index is unsorted, assignment of a new column will fail""        
data.sort_index(inplace = True)
data.loc['index_value1', 'column_y'] = np.random.randn(data.loc['index_value1', 'column_x'].shape[0])
</code></pre>
","1716733","","1716733","","2015-06-16 20:27:15","2015-06-16 20:27:15","","","","0","","","","CC BY-SA 3.0"
"33283360","2","","12555323","2015-10-22 14:21:45","","7","","<p>Let me just add that, just like for <a href=""https://stackoverflow.com/users/3649456/hum3"">hum3</a>, <code>.loc</code> didn't solve the <code>SettingWithCopyWarning</code> and I had to resort to <code>df.insert()</code>. In my case false positive was generated by ""fake"" chain indexing  <code>dict['a']['e']</code>, where <code>'e'</code> is the new column, and <code>dict['a']</code> is a DataFrame coming from dictionary.</p>

<p>Also note that if you know what you are doing, you can switch of the warning using
<code>pd.options.mode.chained_assignment = None</code>
and than use one of the other solutions given here.</p>
","5422817","","-1","","2017-05-23 11:47:36","2015-10-22 14:21:45","","","","0","","","","CC BY-SA 3.0"
"35387129","2","","12555323","2016-02-14 00:49:58","","168","","<blockquote>
  <p>I would like to add a new column, 'e', to the existing data frame and do not change anything in the data frame. (The series always got the same length as a dataframe.) </p>
</blockquote>

<p>I assume that the index values in <code>e</code> match those in <code>df1</code>.</p>

<p>The easiest way to initiate a new column named <code>e</code>, and assign it the values from your series <code>e</code>:</p>

<pre><code>df['e'] = e.values
</code></pre>

<p><strong>assign (Pandas 0.16.0+)</strong></p>

<p>As of Pandas 0.16.0, you can also use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.assign.html"" rel=""noreferrer""><code>assign</code></a>, which assigns new columns to a DataFrame and returns a new object (a copy) with all the original columns in addition to the new ones.</p>

<pre><code>df1 = df1.assign(e=e.values)
</code></pre>

<p>As per <a href=""https://stackoverflow.com/questions/42101382/pandas-dataframe-assign-arguments"">this example</a> (which also includes the source code of the <code>assign</code> function), you can also include more than one column:</p>

<pre><code>df = pd.DataFrame({'a': [1, 2], 'b': [3, 4]})
&gt;&gt;&gt; df.assign(mean_a=df.a.mean(), mean_b=df.b.mean())
   a  b  mean_a  mean_b
0  1  3     1.5     3.5
1  2  4     1.5     3.5
</code></pre>

<p>In context with your example: </p>

<pre><code>np.random.seed(0)
df1 = pd.DataFrame(np.random.randn(10, 4), columns=['a', 'b', 'c', 'd'])
mask = df1.applymap(lambda x: x &lt;-0.7)
df1 = df1[-mask.any(axis=1)]
sLength = len(df1['a'])
e = pd.Series(np.random.randn(sLength))

&gt;&gt;&gt; df1
          a         b         c         d
0  1.764052  0.400157  0.978738  2.240893
2 -0.103219  0.410599  0.144044  1.454274
3  0.761038  0.121675  0.443863  0.333674
7  1.532779  1.469359  0.154947  0.378163
9  1.230291  1.202380 -0.387327 -0.302303

&gt;&gt;&gt; e
0   -1.048553
1   -1.420018
2   -1.706270
3    1.950775
4   -0.509652
dtype: float64

df1 = df1.assign(e=e.values)

&gt;&gt;&gt; df1
          a         b         c         d         e
0  1.764052  0.400157  0.978738  2.240893 -1.048553
2 -0.103219  0.410599  0.144044  1.454274 -1.420018
3  0.761038  0.121675  0.443863  0.333674 -1.706270
7  1.532779  1.469359  0.154947  0.378163  1.950775
9  1.230291  1.202380 -0.387327 -0.302303 -0.509652
</code></pre>

<p>The description of this new feature when it was first introduced can be found <a href=""http://pandas.pydata.org/pandas-docs/stable/whatsnew/v0.16.0.html#dataframe-assign"" rel=""noreferrer"">here</a>.</p>
","2411802","","3427777","","2019-05-29 02:20:52","2019-05-29 02:20:52","","","","8","","","","CC BY-SA 4.0"
"38510820","2","","12555323","2016-07-21 17:35:37","","54","","<p>It seems that in recent Pandas versions the way to go is to use <a href=""http://pandas.pydata.org/pandas-docs/stable/dsintro.html#assigning-new-columns-in-method-chains"" rel=""noreferrer"">df.assign</a>:</p>

<p><code>df1 = df1.assign(e=np.random.randn(sLength))</code></p>

<p>It doesn't produce <code>SettingWithCopyWarning</code>.</p>
","114795","","7932273","","2018-10-03 07:39:25","2018-10-03 07:39:25","","","","1","","","","CC BY-SA 4.0"
"43180437","2","","12555323","2017-04-03 08:59:22","","56","","<h1>Super simple column assignment</h1>

<p>A pandas dataframe is implemented as an ordered dict of columns.</p>

<p>This means that the <code>__getitem__</code> <code>[]</code> can not only be used to get a certain column, but <code>__setitem__</code> <code>[] =</code> can be used to assign a new column.</p>

<p>For example, this dataframe can have a column added to it by simply using the <code>[]</code> accessor</p>

<pre><code>    size      name color
0    big      rose   red
1  small    violet  blue
2  small     tulip   red
3  small  harebell  blue

df['protected'] = ['no', 'no', 'no', 'yes']

    size      name color protected
0    big      rose   red        no
1  small    violet  blue        no
2  small     tulip   red        no
3  small  harebell  blue       yes
</code></pre>

<p>Note that this works even if the index of the dataframe is off.</p>

<pre><code>df.index = [3,2,1,0]
df['protected'] = ['no', 'no', 'no', 'yes']
    size      name color protected
3    big      rose   red        no
2  small    violet  blue        no
1  small     tulip   red        no
0  small  harebell  blue       yes
</code></pre>

<h3>[]= is the way to go, but watch out!</h3>

<p>However, if you have a <code>pd.Series</code> and try to assign it to a dataframe where the indexes are off, you will run in to trouble. See example:</p>

<pre><code>df['protected'] = pd.Series(['no', 'no', 'no', 'yes'])
    size      name color protected
3    big      rose   red       yes
2  small    violet  blue        no
1  small     tulip   red        no
0  small  harebell  blue        no
</code></pre>

<p>This is because a <code>pd.Series</code> by default has an index enumerated from 0 to n. And the pandas <code>[] =</code> method <strong>tries</strong> <em>to be ""smart""</em></p>

<h2>What actually is going on.</h2>

<p>When you use the <code>[] =</code> method pandas is quietly performing an outer join or outer merge using the index of the left hand dataframe and the index of the right hand series. <code>df['column'] = series</code></p>

<h3>Side note</h3>

<p>This quickly causes cognitive dissonance, since the <code>[]=</code> method is trying to do a lot of different things depending on the input, and the outcome cannot be predicted unless you <em>just know</em> how pandas works. I would therefore advice against the <code>[]=</code> in code bases, but when exploring data in a notebook, it is fine.</p>

<h2>Going around the problem</h2>

<p>If you have a <code>pd.Series</code> and want it assigned from top to bottom, or if you are coding productive code and you are not sure of the index order, it is worth it to safeguard for this kind of issue.</p>

<p>You could downcast the <code>pd.Series</code> to a <code>np.ndarray</code> or a <code>list</code>, this will do the trick.</p>

<pre><code>df['protected'] = pd.Series(['no', 'no', 'no', 'yes']).values
</code></pre>

<p>or</p>

<pre><code>df['protected'] = list(pd.Series(['no', 'no', 'no', 'yes']))
</code></pre>

<p><strong>But this is not very explicit.</strong></p>

<p>Some coder may come along and say ""Hey, this looks redundant, I'll just optimize this away"".</p>

<h3>Explicit way</h3>

<p>Setting the index of the <code>pd.Series</code> to be the index of the <code>df</code> is explicit.</p>

<pre><code>df['protected'] = pd.Series(['no', 'no', 'no', 'yes'], index=df.index)
</code></pre>

<p>Or more realistically, you probably have a <code>pd.Series</code> already available.</p>

<pre><code>protected_series = pd.Series(['no', 'no', 'no', 'yes'])
protected_series.index = df.index

3     no
2     no
1     no
0    yes
</code></pre>

<p>Can now be assigned</p>

<pre><code>df['protected'] = protected_series

    size      name color protected
3    big      rose   red        no
2  small    violet  blue        no
1  small     tulip   red        no
0  small  harebell  blue       yes
</code></pre>

<h2>Alternative way with <code>df.reset_index()</code></h2>

<p>Since the index dissonance is the problem, if you feel that the index of the dataframe <em>should</em> not dictate things, you can simply drop the index, this should be faster, but it is not very clean, since your function now <em>probably</em> does two things.</p>

<pre><code>df.reset_index(drop=True)
protected_series.reset_index(drop=True)
df['protected'] = protected_series

    size      name color protected
0    big      rose   red        no
1  small    violet  blue        no
2  small     tulip   red        no
3  small  harebell  blue       yes
</code></pre>

<h2>Note on <code>df.assign</code></h2>

<p>While <code>df.assign</code> make it more explicit what you are doing, it actually has all the same problems as the above <code>[]=</code></p>

<pre><code>df.assign(protected=pd.Series(['no', 'no', 'no', 'yes']))
    size      name color protected
3    big      rose   red       yes
2  small    violet  blue        no
1  small     tulip   red        no
0  small  harebell  blue        no
</code></pre>

<p>Just watch out with <code>df.assign</code> that your column is not called <code>self</code>. It will cause errors. This makes <code>df.assign</code> <strong>smelly</strong>, since there are these kind of artifacts in the function.</p>

<pre><code>df.assign(self=pd.Series(['no', 'no', 'no', 'yes'])
TypeError: assign() got multiple values for keyword argument 'self'
</code></pre>

<p>You may say, ""Well, I'll just not use <code>self</code> then"". But who knows how this function changes in the future to support new arguments. Maybe your column name will be an argument in a new update of pandas, causing problems with upgrading.</p>
","3730397","","","","","2017-04-03 08:59:22","","","","1","","","","CC BY-SA 3.0"
"43268324","2","","12555323","2017-04-07 01:38:37","","10","","<p>If the data frame and Series object have <strong><em>the same index</em></strong>, <code>pandas.concat</code> also works here:</p>

<pre><code>import pandas as pd
df
#          a            b           c           d
#0  0.671399     0.101208   -0.181532    0.241273
#1  0.446172    -0.243316    0.051767    1.577318
#2  0.614758     0.075793   -0.451460   -0.012493

e = pd.Series([-0.335485, -1.166658, -0.385571])    
e
#0   -0.335485
#1   -1.166658
#2   -0.385571
#dtype: float64

# here we need to give the series object a name which converts to the new  column name 
# in the result
df = pd.concat([df, e.rename(""e"")], axis=1)
df

#          a            b           c           d           e
#0  0.671399     0.101208   -0.181532    0.241273   -0.335485
#1  0.446172    -0.243316    0.051767    1.577318   -1.166658
#2  0.614758     0.075793   -0.451460   -0.012493   -0.385571
</code></pre>

<p>In case they don't have the same index:</p>

<pre><code>e.index = df.index
df = pd.concat([df, e.rename(""e"")], axis=1)
</code></pre>
","4983450","","4983450","","2017-04-07 01:46:08","2017-04-07 01:46:08","","","","0","","","","CC BY-SA 3.0"
"43368149","2","","12555323","2017-04-12 11:22:03","","10","","<p><strong>Foolproof:</strong></p>

<pre><code>df.loc[:, 'NewCol'] = 'New_Val'
</code></pre>

<p>Example:</p>

<pre><code>df = pd.DataFrame(data=np.random.randn(20, 4), columns=['A', 'B', 'C', 'D'])

df

           A         B         C         D
0  -0.761269  0.477348  1.170614  0.752714
1   1.217250 -0.930860 -0.769324 -0.408642
2  -0.619679 -1.227659 -0.259135  1.700294
3  -0.147354  0.778707  0.479145  2.284143
4  -0.529529  0.000571  0.913779  1.395894
5   2.592400  0.637253  1.441096 -0.631468
6   0.757178  0.240012 -0.553820  1.177202
7  -0.986128 -1.313843  0.788589 -0.707836
8   0.606985 -2.232903 -1.358107 -2.855494
9  -0.692013  0.671866  1.179466 -1.180351
10 -1.093707 -0.530600  0.182926 -1.296494
11 -0.143273 -0.503199 -1.328728  0.610552
12 -0.923110 -1.365890 -1.366202 -1.185999
13 -2.026832  0.273593 -0.440426 -0.627423
14 -0.054503 -0.788866 -0.228088 -0.404783
15  0.955298 -1.430019  1.434071 -0.088215
16 -0.227946  0.047462  0.373573 -0.111675
17  1.627912  0.043611  1.743403 -0.012714
18  0.693458  0.144327  0.329500 -0.655045
19  0.104425  0.037412  0.450598 -0.923387


df.drop([3, 5, 8, 10, 18], inplace=True)

df

           A         B         C         D
0  -0.761269  0.477348  1.170614  0.752714
1   1.217250 -0.930860 -0.769324 -0.408642
2  -0.619679 -1.227659 -0.259135  1.700294
4  -0.529529  0.000571  0.913779  1.395894
6   0.757178  0.240012 -0.553820  1.177202
7  -0.986128 -1.313843  0.788589 -0.707836
9  -0.692013  0.671866  1.179466 -1.180351
11 -0.143273 -0.503199 -1.328728  0.610552
12 -0.923110 -1.365890 -1.366202 -1.185999
13 -2.026832  0.273593 -0.440426 -0.627423
14 -0.054503 -0.788866 -0.228088 -0.404783
15  0.955298 -1.430019  1.434071 -0.088215
16 -0.227946  0.047462  0.373573 -0.111675
17  1.627912  0.043611  1.743403 -0.012714
19  0.104425  0.037412  0.450598 -0.923387

df.loc[:, 'NewCol'] = 0

df
           A         B         C         D  NewCol
0  -0.761269  0.477348  1.170614  0.752714       0
1   1.217250 -0.930860 -0.769324 -0.408642       0
2  -0.619679 -1.227659 -0.259135  1.700294       0
4  -0.529529  0.000571  0.913779  1.395894       0
6   0.757178  0.240012 -0.553820  1.177202       0
7  -0.986128 -1.313843  0.788589 -0.707836       0
9  -0.692013  0.671866  1.179466 -1.180351       0
11 -0.143273 -0.503199 -1.328728  0.610552       0
12 -0.923110 -1.365890 -1.366202 -1.185999       0
13 -2.026832  0.273593 -0.440426 -0.627423       0
14 -0.054503 -0.788866 -0.228088 -0.404783       0
15  0.955298 -1.430019  1.434071 -0.088215       0
16 -0.227946  0.047462  0.373573 -0.111675       0
17  1.627912  0.043611  1.743403 -0.012714       0
19  0.104425  0.037412  0.450598 -0.923387       0
</code></pre>
","7856122","","","","","2017-04-12 11:22:03","","","","1","","","","CC BY-SA 3.0"
"44360325","2","","12555323","2017-06-05 00:53:20","","14","","<ol>
<li>First create a python's <code>list_of_e</code> that has relevant data. </li>
<li>Use this: 
    <code>df['e'] = list_of_e</code></li>
</ol>
","2690723","","7932273","","2018-12-05 09:13:21","2018-12-05 09:13:21","","","","1","","","","CC BY-SA 4.0"
"46734631","2","","12555323","2017-10-13 16:53:18","","23","","<p>If you want to set the whole new column to an initial base value (e.g. <code>None</code>), you can do this: <code>df1['e'] = None</code></p>

<p>This actually would assign ""object"" type to the cell. So later you're free to put complex data types, like list, into individual cells.</p>
","4880002","","4880002","","2017-12-18 20:51:00","2017-12-18 20:51:00","","","","2","","","","CC BY-SA 3.0"
"47093066","2","","12555323","2017-11-03 10:05:58","","11","","<p>If the column you are trying to add is a series variable then just :</p>

<pre><code>df[""new_columns_name""]=series_variable_name #this will do it for you
</code></pre>

<p>This works well even if you are replacing an existing column.just type the new_columns_name same as the column you want to replace.It will just overwrite the existing column data with the new series data.</p>
","7850477","","958529","","2017-11-03 10:44:11","2017-11-03 10:44:11","","","","0","","","","CC BY-SA 3.0"
"52232245","2","","12555323","2018-09-08 05:17:14","","29","","<p>Easiest ways:-</p>

<pre><code>data['new_col'] = list_of_values

data.loc[ : , 'new_col'] = list_of_values
</code></pre>

<p>This way you avoid what is called chained indexing when setting new values in a pandas object. <a href=""https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy"" rel=""nofollow noreferrer"">Click here to read further</a>.</p>
","7430988","","9571713","","2020-05-28 15:25:56","2020-05-28 15:25:56","","","","0","","","","CC BY-SA 4.0"
"55560478","2","","12555323","2019-04-07 15:12:59","","7","","<p>to insert a new column at a given location (0 &lt;= loc &lt;= amount of columns) in a data frame, just use Dataframe.insert:</p>

<pre><code>DataFrame.insert(loc, column, value)
</code></pre>

<p>Therefore, if you want to add the column <strong><em>e</em></strong> at the end of a data frame called <strong><em>df</em></strong>, you can use:</p>

<pre><code>e = [-0.335485, -1.166658, -0.385571]    
DataFrame.insert(loc=len(df.columns), column='e', value=e)
</code></pre>

<p><strong><em>value</em></strong> can be a Series, an integer (in which case all cells get filled with this one value), or an array-like structure</p>

<p><a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.insert.html"" rel=""noreferrer"">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.insert.html</a></p>
","4204448","","4204448","","2019-04-07 15:18:32","2019-04-07 15:18:32","","","","0","","","","CC BY-SA 4.0"
"59082735","2","","12555323","2019-11-28 06:12:52","","9","","<p>To create an empty column</p>

<pre><code>df['i'] = None
</code></pre>
","2437872","","7851470","","2020-05-08 19:50:19","2020-05-08 19:50:19","","","","0","","","","CC BY-SA 4.0"
"13148611","2","","13148429","2012-10-30 22:38:49","","954","","<p>One easy way would be to reassign the dataframe with a list of the columns, rearranged as needed. </p>

<p>This is what you have now: </p>

<pre><code>In [6]: df
Out[6]:
          0         1         2         3         4      mean
0  0.445598  0.173835  0.343415  0.682252  0.582616  0.445543
1  0.881592  0.696942  0.702232  0.696724  0.373551  0.670208
2  0.662527  0.955193  0.131016  0.609548  0.804694  0.632596
3  0.260919  0.783467  0.593433  0.033426  0.512019  0.436653
4  0.131842  0.799367  0.182828  0.683330  0.019485  0.363371
5  0.498784  0.873495  0.383811  0.699289  0.480447  0.587165
6  0.388771  0.395757  0.745237  0.628406  0.784473  0.588529
7  0.147986  0.459451  0.310961  0.706435  0.100914  0.345149
8  0.394947  0.863494  0.585030  0.565944  0.356561  0.553195
9  0.689260  0.865243  0.136481  0.386582  0.730399  0.561593

In [7]: cols = df.columns.tolist()

In [8]: cols
Out[8]: [0L, 1L, 2L, 3L, 4L, 'mean']
</code></pre>

<p>Rearrange <code>cols</code> in any way you want. This is how I moved the last element to the first position: </p>

<pre><code>In [12]: cols = cols[-1:] + cols[:-1]

In [13]: cols
Out[13]: ['mean', 0L, 1L, 2L, 3L, 4L]
</code></pre>

<p>Then reorder the dataframe like this: </p>

<pre><code>In [16]: df = df[cols]  #    OR    df = df.ix[:, cols]

In [17]: df
Out[17]:
       mean         0         1         2         3         4
0  0.445543  0.445598  0.173835  0.343415  0.682252  0.582616
1  0.670208  0.881592  0.696942  0.702232  0.696724  0.373551
2  0.632596  0.662527  0.955193  0.131016  0.609548  0.804694
3  0.436653  0.260919  0.783467  0.593433  0.033426  0.512019
4  0.363371  0.131842  0.799367  0.182828  0.683330  0.019485
5  0.587165  0.498784  0.873495  0.383811  0.699289  0.480447
6  0.588529  0.388771  0.395757  0.745237  0.628406  0.784473
7  0.345149  0.147986  0.459451  0.310961  0.706435  0.100914
8  0.553195  0.394947  0.863494  0.585030  0.565944  0.356561
9  0.561593  0.689260  0.865243  0.136481  0.386582  0.730399
</code></pre>
","484596","","484596","","2012-10-31 15:51:22","2012-10-31 15:51:22","","","","8","","","","CC BY-SA 3.0"
"13316001","2","","13148429","2012-11-09 21:04:03","","153","","<p>How about:</p>

<pre><code>df.insert(0, 'mean', df.mean(1))
</code></pre>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/dsintro.html#column-selection-addition-deletion"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/dsintro.html#column-selection-addition-deletion</a></p>
","776560","","","","","2012-11-09 21:04:03","","","","6","","","","CC BY-SA 3.0"
"23741480","2","","13148429","2014-05-19 15:20:33","","521","","<p>You could also do something like this:</p>

<pre><code>df = df[['mean', '0', '1', '2', '3']]
</code></pre>

<p>You can get the list of columns with:</p>

<pre><code>cols = list(df.columns.values)
</code></pre>

<p>The output will produce:</p>

<pre><code>['0', '1', '2', '3', 'mean']
</code></pre>

<p>...which is then easy to rearrange manually before dropping it into the first function</p>
","2518486","","","","","2014-05-19 15:20:33","","","","7","","","","CC BY-SA 3.0"
"25023460","2","","13148429","2014-07-29 19:30:18","","17","","<p>This function avoids you having to list out every variable in your dataset just to order a few of them. </p>

<pre><code>def order(frame,var):
    if type(var) is str:
        var = [var] #let the command take a string or list
    varlist =[w for w in frame.columns if w not in var]
    frame = frame[var+varlist]
    return frame 
</code></pre>

<p>It takes two arguments, the first is the dataset, the second are the columns in the data set that you want to bring to the front. </p>

<p>So in my case I have a data set called Frame with variables A1, A2, B1, B2, Total and Date. If I want to bring Total to the front then all I have to do is: </p>

<pre><code>frame = order(frame,['Total'])
</code></pre>

<p>If I want to bring Total and Date to the front then I do:</p>

<pre><code>frame = order(frame,['Total','Date'])
</code></pre>

<p>EDIT:</p>

<p>Another useful way to use this is, if you have an unfamiliar table and you're looking with variables with a particular term in them, like VAR1, VAR2,... you may execute something like: </p>

<pre><code>frame = order(frame,[v for v in frame.columns if ""VAR"" in v])
</code></pre>
","1920550","","1920550","","2018-07-23 23:10:21","2018-07-23 23:10:21","","","","0","","","","CC BY-SA 4.0"
"25535803","2","","13148429","2014-08-27 19:49:31","","19","","<p>I ran into a similar question myself, and just wanted to add what I settled on. I liked the <code>reindex_axis() method</code> for changing column order. This worked:</p>

<pre><code>df = df.reindex_axis(['mean'] + list(df.columns[:-1]), axis=1)
</code></pre>

<p>An alternate method based on the comment from @Jorge:</p>

<pre><code>df = df.reindex(columns=['mean'] + list(df.columns[:-1]))
</code></pre>

<p>Although <code>reindex_axis</code> seems to be slightly faster in micro benchmarks than <code>reindex</code>, I think I prefer the latter for its directness.</p>
","3878013","","3878013","","2018-08-10 07:11:13","2018-08-10 07:11:13","","","","1","","","","CC BY-SA 4.0"
"29916004","2","","13148429","2015-04-28 09:50:19","","16","","<p>Simply do,</p>

<pre><code>df = df[['mean'] + df.columns[:-1].tolist()]
</code></pre>
","1296136","","1296136","","2017-07-12 08:23:10","2017-07-12 08:23:10","","","","7","","","","CC BY-SA 3.0"
"29922207","2","","13148429","2015-04-28 14:19:49","","337","","<p>Just assign the column names in the order you want them:</p>

<pre><code>In [39]: df
Out[39]: 
          0         1         2         3         4  mean
0  0.172742  0.915661  0.043387  0.712833  0.190717     1
1  0.128186  0.424771  0.590779  0.771080  0.617472     1
2  0.125709  0.085894  0.989798  0.829491  0.155563     1
3  0.742578  0.104061  0.299708  0.616751  0.951802     1
4  0.721118  0.528156  0.421360  0.105886  0.322311     1
5  0.900878  0.082047  0.224656  0.195162  0.736652     1
6  0.897832  0.558108  0.318016  0.586563  0.507564     1
7  0.027178  0.375183  0.930248  0.921786  0.337060     1
8  0.763028  0.182905  0.931756  0.110675  0.423398     1
9  0.848996  0.310562  0.140873  0.304561  0.417808     1

In [40]: df = df[['mean', 4,3,2,1]]
</code></pre>

<p>Now, 'mean' column comes out in the front:</p>

<pre><code>In [41]: df
Out[41]: 
   mean         4         3         2         1
0     1  0.190717  0.712833  0.043387  0.915661
1     1  0.617472  0.771080  0.590779  0.424771
2     1  0.155563  0.829491  0.989798  0.085894
3     1  0.951802  0.616751  0.299708  0.104061
4     1  0.322311  0.105886  0.421360  0.528156
5     1  0.736652  0.195162  0.224656  0.082047
6     1  0.507564  0.586563  0.318016  0.558108
7     1  0.337060  0.921786  0.930248  0.375183
8     1  0.423398  0.110675  0.931756  0.182905
9     1  0.417808  0.304561  0.140873  0.310562
</code></pre>
","170005","","170005","","2018-03-28 08:13:43","2018-03-28 08:13:43","","","","5","","","","CC BY-SA 3.0"
"32131398","2","","13148429","2015-08-21 02:18:52","","70","","<p>You need to create a new list of your columns in the desired order, then use <code>df = df[cols]</code> to rearrange the columns in this new order.</p>

<pre><code>cols = ['mean']  + [col for col in df if col != 'mean']
df = df[cols]
</code></pre>

<p>You can also use a more general approach.  In this example, the last column (indicated by -1) is inserted as the first column.</p>

<pre><code>cols = [df.columns[-1]] + [col for col in df if col != df.columns[-1]]
df = df[cols]
</code></pre>

<p>You can also use this approach for reordering columns in a desired order if they are present in the DataFrame.</p>

<pre><code>inserted_cols = ['a', 'b', 'c']
cols = ([col for col in inserted_cols if col in df] 
        + [col for col in df if col not in inserted_cols])
df = df[cols]
</code></pre>
","2411802","","1636598","","2019-11-19 13:16:40","2019-11-19 13:16:40","","","","0","","","","CC BY-SA 4.0"
"37071454","2","","13148429","2016-05-06 11:39:33","","10","","<p>Just type the column name you want to change, and set the index for the new location.</p>

<pre><code>def change_column_order(df, col_name, index):
    cols = df.columns.tolist()
    cols.remove(col_name)
    cols.insert(index, col_name)
    return df[cols]
</code></pre>

<p>For your case, this would be like:</p>

<pre><code>df = change_column_order(df, 'mean', 0)
</code></pre>
","3257147","","","","","2016-05-06 11:39:33","","","","1","","","","CC BY-SA 3.0"
"39237712","2","","13148429","2016-08-30 21:57:36","","142","","<p>In your case,</p>

<pre><code>df = df.reindex(columns=['mean',0,1,2,3,4])
</code></pre>

<p>will do exactly what you want.</p>

<p><strong>In my case (general form):</strong></p>

<pre><code>df = df.reindex(columns=sorted(df.columns))
df = df.reindex(columns=(['opened'] + list([a for a in df.columns if a != 'opened']) ))
</code></pre>
","4803173","","281545","","2019-07-08 23:01:32","2019-07-08 23:01:32","","","","2","","","","CC BY-SA 4.0"
"41042896","2","","13148429","2016-12-08 15:22:39","","11","","<p>You could do the following (borrowing parts from Aman's answer):</p>

<pre><code>cols = df.columns.tolist()
cols.insert(0, cols.pop(-1))

cols
&gt;&gt;&gt;['mean', 0L, 1L, 2L, 3L, 4L]

df = df[cols]
</code></pre>
","5817865","","","","","2016-12-08 15:22:39","","","","0","","","","CC BY-SA 3.0"
"49010588","2","","13148429","2018-02-27 14:05:01","","8","","<p>Moving any column to any position: </p>

<pre><code>import pandas as pd
df = pd.DataFrame({""A"": [1,2,3], 
                   ""B"": [2,4,8], 
                   ""C"": [5,5,5]})

cols = df.columns.tolist()
column_to_move = ""C""
new_position = 1

cols.insert(new_position, cols.pop(cols.index(column_to_move)))
df = df[cols]
</code></pre>
","1325646","","","","","2018-02-27 14:05:01","","","","0","","","","CC BY-SA 3.0"
"51935892","2","","13148429","2018-08-20 17:35:46","","45","","<h1>From August 2018:</h1>
<p>If your column names are too long to type then you could specify the new order through a list of integers with the positions:</p>
<p>Data:</p>
<pre><code>          0         1         2         3         4      mean
0  0.397312  0.361846  0.719802  0.575223  0.449205  0.500678
1  0.287256  0.522337  0.992154  0.584221  0.042739  0.485741
2  0.884812  0.464172  0.149296  0.167698  0.793634  0.491923
3  0.656891  0.500179  0.046006  0.862769  0.651065  0.543382
4  0.673702  0.223489  0.438760  0.468954  0.308509  0.422683
5  0.764020  0.093050  0.100932  0.572475  0.416471  0.389390
6  0.259181  0.248186  0.626101  0.556980  0.559413  0.449972
7  0.400591  0.075461  0.096072  0.308755  0.157078  0.207592
8  0.639745  0.368987  0.340573  0.997547  0.011892  0.471749
9  0.050582  0.714160  0.168839  0.899230  0.359690  0.438500
</code></pre>
<p>Generic example:</p>
<pre><code>new_order = [3,2,1,4,5,0]
print(df[df.columns[new_order]])  

          3         2         1         4      mean         0
0  0.575223  0.719802  0.361846  0.449205  0.500678  0.397312
1  0.584221  0.992154  0.522337  0.042739  0.485741  0.287256
2  0.167698  0.149296  0.464172  0.793634  0.491923  0.884812
3  0.862769  0.046006  0.500179  0.651065  0.543382  0.656891
4  0.468954  0.438760  0.223489  0.308509  0.422683  0.673702
5  0.572475  0.100932  0.093050  0.416471  0.389390  0.764020
6  0.556980  0.626101  0.248186  0.559413  0.449972  0.259181
7  0.308755  0.096072  0.075461  0.157078  0.207592  0.400591
8  0.997547  0.340573  0.368987  0.011892  0.471749  0.639745
9  0.899230  0.168839  0.714160  0.359690  0.438500  0.050582

      
</code></pre>
<p>And for the specific case of OP's question:</p>
<pre><code>new_order = [-1,0,1,2,3,4]
df = df[df.columns[new_order]]
print(df)

       mean         0         1         2         3         4
0  0.500678  0.397312  0.361846  0.719802  0.575223  0.449205
1  0.485741  0.287256  0.522337  0.992154  0.584221  0.042739
2  0.491923  0.884812  0.464172  0.149296  0.167698  0.793634
3  0.543382  0.656891  0.500179  0.046006  0.862769  0.651065
4  0.422683  0.673702  0.223489  0.438760  0.468954  0.308509
5  0.389390  0.764020  0.093050  0.100932  0.572475  0.416471
6  0.449972  0.259181  0.248186  0.626101  0.556980  0.559413
7  0.207592  0.400591  0.075461  0.096072  0.308755  0.157078
8  0.471749  0.639745  0.368987  0.340573  0.997547  0.011892
9  0.438500  0.050582  0.714160  0.168839  0.899230  0.359690
</code></pre>
<p>The main problem with this approach is that calling the same code multiple times will create different results each time, so one needs to be careful :)</p>
","9754169","","-1","","2020-06-20 09:12:55","2019-04-02 17:17:26","","","","0","","","","CC BY-SA 4.0"
"58715675","2","","13148429","2019-11-05 16:33:40","","11","","<p>I think this is a slightly neater solution:</p>

<pre class=""lang-py prettyprint-override""><code>df.insert(0,'mean', df.pop(""mean""))
</code></pre>

<p>This solution is somewhat similar to @JoeHeffer 's solution but this is one liner.</p>

<p>Here we remove the column <code>""mean""</code> from the dataframe and attach it to index <code>0</code> with the same column name.</p>
","4703367","","","","","2019-11-05 16:33:40","","","","2","","","","CC BY-SA 4.0"
"58776941","2","","13148429","2019-11-09 06:24:12","","64","","<pre><code>import numpy as np
import pandas as pd
df = pd.DataFrame()
column_names = ['x','y','z','mean']
for col in column_names: 
    df[col] = np.random.randint(0,100, size=10000)
</code></pre>

<p>You can try out the following solutions :</p>

<p><strong>Solution 1:</strong></p>

<pre><code>df = df[ ['mean'] + [ col for col in df.columns if col != 'mean' ] ]
</code></pre>

<p><strong>Solution 2:</strong></p>

<hr>

<pre><code>df = df[['mean', 'x', 'y', 'z']]
</code></pre>

<hr>

<p><strong>Solution 3:</strong></p>

<pre><code>col = df.pop(""mean"")
df = df.insert(0, col.name, col)
</code></pre>

<hr>

<p><strong>Solution 4:</strong></p>

<pre><code>df.set_index(df.columns[-1], inplace=True)
df.reset_index(inplace=True)
</code></pre>

<hr>

<p><strong>Solution 5:</strong></p>

<pre><code>cols = list(df)
cols = [cols[-1]] + cols[:-1]
df = df[cols]
</code></pre>

<hr>

<p><strong>solution 6:</strong></p>

<pre><code>order = [1,2,3,0] # setting column's order
df = df[[df.columns[i] for i in order]]
</code></pre>

<h2>Time Comparison:</h2>

<p><strong>Solution 1:</strong></p>

<blockquote>
  <p>CPU times: user 1.05 ms, sys: 35 s, total: 1.08 ms Wall time: 995 s</p>
</blockquote>

<p><strong>Solution 2</strong>:</p>

<blockquote>
  <p>CPU times: user 933 s, sys: 0 ns, total: 933 s
  Wall time: 800 s</p>
</blockquote>

<p><strong>Solution 3</strong>:</p>

<blockquote>
  <p>CPU times: user 0 ns, sys: 1.35 ms, total: 1.35 ms
  Wall time: 1.08 ms</p>
</blockquote>

<p><strong>Solution 4</strong>:</p>

<blockquote>
  <p>CPU times: user 1.23 ms, sys: 45 s, total: 1.27 ms
  Wall time: 986 s</p>
</blockquote>

<p><strong>Solution 5</strong>:</p>

<blockquote>
  <p>CPU times: user 1.09 ms, sys: 19 s, total: 1.11 ms
  Wall time: 949 s</p>
</blockquote>

<p><strong>Solution 6</strong>:</p>

<blockquote>
  <p>CPU times: user 955 s, sys: 34 s, total: 989 s
  Wall time: 859 s</p>
</blockquote>
","6660373","","6660373","","2019-11-09 06:57:36","2019-11-09 06:57:36","","","","5","","","","CC BY-SA 4.0"
"62098121","2","","13148429","2020-05-30 05:12:36","","33","","<p>The Most simple way
Suppose you have <code>df</code> with columns <code>A</code> <code>B</code> <code>C</code>,
you can just <code>df.reindex(['B','C','A'],axis=1)</code></p>
","10856173","","","","","2020-05-30 05:12:36","","","","1","","","","CC BY-SA 4.0"
"13415772","2","","13411544","2012-11-16 11:33:47","","34","","<p>It's good practice to always use the <code>[]</code> notation. One reason is that attribute notation (<code>df.column_name</code>) does not work for numbered indices:</p>

<pre><code>In [1]: df = DataFrame([[1, 2, 3], [4, 5, 6]])

In [2]: df[1]
Out[2]:
0    2
1    5
Name: 1

In [3]: df.1
  File ""&lt;ipython-input-3-e4803c0d1066&gt;"", line 1
    df.1
       ^
SyntaxError: invalid syntax
</code></pre>
","1240268","","63550","","2018-05-23 19:43:02","2018-05-23 19:43:02","","","","0","","","","CC BY-SA 4.0"
"13485766","2","","13411544","2012-11-21 03:12:31","","1019","","<p>As you've guessed, the right syntax is </p>

<pre><code>del df['column_name']
</code></pre>

<p>It's difficult to make <code>del df.column_name</code> work simply as the result of syntactic limitations in Python. <code>del df[name]</code> gets translated to <code>df.__delitem__(name)</code> under the covers by Python.</p>
","776560","","4909087","","2019-04-07 22:01:11","2019-04-07 22:01:11","","","","8","","","","CC BY-SA 4.0"
"18145399","2","","13411544","2013-08-09 11:12:09","","2424","","<p>The best way to do this in pandas is to use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html"" rel=""noreferrer""><code>drop</code></a>:</p>
<pre><code>df = df.drop('column_name', 1)
</code></pre>
<p>where <code>1</code> is the <em>axis</em> number (<code>0</code> for rows and <code>1</code> for columns.)</p>
<p>To delete the column without having to reassign <code>df</code> you can do:</p>
<pre><code>df.drop('column_name', axis=1, inplace=True)
</code></pre>
<p>Finally, to drop by column <em>number</em> instead of by column <em>label</em>, try this to delete, e.g. the 1st, 2nd and 4th columns:</p>
<pre><code>df = df.drop(df.columns[[0, 1, 3]], axis=1)  # df.columns is zero-based pd.Index 
</code></pre>
<p>Also working with &quot;text&quot; syntax for the columns:</p>
<pre><code>df.drop(['column_nameA', 'column_nameB'], axis=1, inplace=True)
</code></pre>
<p><em>Note:</em> Introduced in <a href=""https://pandas.pydata.org/pandas-docs/stable/whatsnew/v0.21.0.html#new-features"" rel=""noreferrer"">v0.21.0</a> (October 27, 2017), the drop() method accepts index/columns keywords as an alternative to specifying the axis.</p>
<p>So we can now just do:</p>
<p><code>df.drop(columns=['B', 'C'])</code></p>
","2071807","","2071807","","2020-07-27 15:42:01","2020-07-27 15:42:01","","","","23","","","","CC BY-SA 4.0"
"22596982","2","","13411544","2014-03-23 20:57:57","","261","","<p>Use:</p>

<pre><code>columns = ['Col1', 'Col2', ...]
df.drop(columns, inplace=True, axis=1)
</code></pre>

<p>This will delete one or more columns in-place. Note that <code>inplace=True</code> was added in pandas v0.13 and won't work on older versions. You'd have to assign the result back in that case:</p>

<pre><code>df = df.drop(columns, axis=1)
</code></pre>
","501379","","63550","","2018-05-23 19:44:08","2018-05-23 19:44:08","","","","4","","","","CC BY-SA 4.0"
"31431997","2","","13411544","2015-07-15 13:37:23","","123","","<h2>Drop by index</h2>

<p>Delete first, second and fourth columns:</p>

<pre><code>df.drop(df.columns[[0,1,3]], axis=1, inplace=True)
</code></pre>

<p>Delete first column:</p>

<pre><code>df.drop(df.columns[[0]], axis=1, inplace=True)
</code></pre>

<p>There is an optional parameter <code>inplace</code> so that the original
data can be modified without creating a copy.</p>

<h2>Popped</h2>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/dsintro.html#column-selection-addition-deletion"" rel=""noreferrer"">Column selection, addition, deletion</a></p>

<p>Delete column <code>column-name</code>:</p>

<pre><code>df.pop('column-name')
</code></pre>

<h2>Examples:</h2>

<pre><code>df = DataFrame.from_items([('A', [1, 2, 3]), ('B', [4, 5, 6]), ('C', [7,8, 9])], orient='index', columns=['one', 'two', 'three'])
</code></pre>

<p><code>print df</code>:</p>

<pre><code>   one  two  three
A    1    2      3
B    4    5      6
C    7    8      9
</code></pre>

<p><code>df.drop(df.columns[[0]], axis=1, inplace=True)</code>
<code>print df</code>:</p>

<pre><code>   two  three
A    2      3
B    5      6
C    8      9
</code></pre>

<p><code>three = df.pop('three')</code>
<code>print df</code>:</p>

<pre><code>   two
A    2
B    5
C    8
</code></pre>
","2901002","","63550","","2018-05-23 19:44:53","2018-05-23 19:44:53","","","","7","","","","CC BY-SA 4.0"
"34576537","2","","13411544","2016-01-03 12:29:49","","63","","<p>A nice addition is the ability to <strong>drop columns only if they exist</strong>. This way you can cover more use cases, and it will only drop the existing columns from the labels passed to it:</p>

<p>Simply add <strong>errors='ignore'</strong>, for example.:</p>

<pre><code>df.drop(['col_name_1', 'col_name_2', ..., 'col_name_N'], inplace=True, axis=1, errors='ignore')
</code></pre>

<ul>
<li>This is new from pandas 0.16.1 onward. Documentation is <a href=""http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.drop.html"" rel=""noreferrer"">here</a>.</li>
</ul>
","2923704","","63550","","2018-05-23 19:48:00","2018-05-23 19:48:00","","","","0","","","","CC BY-SA 4.0"
"35385805","2","","13411544","2016-02-13 21:58:33","","22","","<p>In pandas 0.16.1+ you can drop columns only if they exist per the solution posted by @eiTanLaVi.  Prior to that version, you can achieve the same result via a conditional list comprehension:</p>

<pre><code>df.drop([col for col in ['col_name_1','col_name_2',...,'col_name_N'] if col in df], 
        axis=1, inplace=True)
</code></pre>
","2411802","","2411802","","2016-11-22 15:15:42","2016-11-22 15:15:42","","","","0","","","","CC BY-SA 3.0"
"36958937","2","","13411544","2016-04-30 18:57:48","","44","","<p>from version 0.16.1 you can do </p>

<pre><code>df.drop(['column_name'], axis = 1, inplace = True, errors = 'ignore')
</code></pre>
","3156200","","1218980","","2016-10-21 21:20:09","2016-10-21 21:20:09","","","","1","","","","CC BY-SA 3.0"
"37000877","2","","13411544","2016-05-03 09:48:51","","74","","<p>The actual question posed, missed by most answers here is:</p>
<h3>Why can't I use <code>del df.column_name</code>?</h3>
<p>At first we need to understand the problem, which requires us to dive into <a href=""https://rszalski.github.io/magicmethods/"" rel=""noreferrer""><em>python magic methods</em></a>.</p>
<p>As Wes points out in his answer <code>del df['column']</code> maps to the python <em>magic method</em> <code>df.__delitem__('column')</code> which is <a href=""https://github.com/pydata/pandas/blob/c6110e25b3eceb2f25022c2aa9ccea03c0b8b359/pandas/core/generic.py#L1580"" rel=""noreferrer"">implemented in pandas to drop the column</a></p>
<p>However, as pointed out in the link above about <a href=""https://rszalski.github.io/magicmethods/"" rel=""noreferrer""><em>python magic methods</em></a>:</p>
<blockquote>
<p>In fact, <code>__del__</code> should almost never be used because of the precarious circumstances under which it is called; use it with caution!</p>
</blockquote>
<p>You could argue that <code>del df['column_name']</code> should not be used or encouraged, and thereby <code>del df.column_name</code> should not even be considered.</p>
<p>However, in theory, <code>del df.column_name</code> could be implemeted to work in pandas using <a href=""https://rszalski.github.io/magicmethods/#access"" rel=""noreferrer"">the <em>magic method <code>__delattr__</code></em></a>. This does however introduce certain problems, problems which the <code>del df['column_name']</code> implementation already has, but in lesser degree.</p>
<h2>Example Problem</h2>
<p>What if I define a column in a dataframe called &quot;dtypes&quot; or &quot;columns&quot;.</p>
<p>Then assume I want to delete these columns.</p>
<p><code>del df.dtypes</code> would make the <code>__delattr__</code> method confused as if it should delete the &quot;dtypes&quot; attribute or the &quot;dtypes&quot; column.</p>
<h2>Architectural questions behind this problem</h2>
<ol>
<li>Is a dataframe a
collection of <em>columns</em>?</li>
<li>Is a dataframe a collection of <em>rows</em>?</li>
<li>Is a column an <em>attribute</em> of a dataframe?</li>
</ol>
<h3>Pandas answers:</h3>
<ol>
<li>Yes, in all ways</li>
<li>No, but if you want it to be, you can use the <code>.ix</code>, <code>.loc</code> or <code>.iloc</code> methods.</li>
<li>Maybe, do you want to <em>read</em> data? Then <strong>yes</strong>, <em>unless</em> the name of the attribute is already taken by another attribute belonging to the dataframe. Do you want to <em>modify</em> data? Then <strong>no</strong>.</li>
</ol>
<h1>TLDR;</h1>
<p>You cannot do <code>del df.column_name</code> because pandas has a quite wildly grown architecture that needs to be reconsidered in order for this kind of <em>cognitive dissonance</em> not to occur to its users.</p>
<h3>Protip:</h3>
<p>Don't use df.column_name, It may be pretty, but it causes <em>cognitive dissonance</em></p>
<h3>Zen of Python quotes that fits in here:</h3>
<p>There are multiple ways of deleting a column.</p>
<blockquote>
<p>There should be one-- and preferably only one --obvious way to do it.</p>
</blockquote>
<p>Columns are sometimes attributes but sometimes not.</p>
<blockquote>
<p>Special cases aren't special enough to break the rules.</p>
</blockquote>
<p>Does <code>del df.dtypes</code> delete the dtypes attribute or the dtypes column?</p>
<blockquote>
<p>In the face of ambiguity, refuse the temptation to guess.</p>
</blockquote>
","3730397","","-1","","2020-06-20 09:12:55","2020-01-07 10:45:41","","","","4","","","","CC BY-SA 4.0"
"46314092","2","","13411544","2017-09-20 05:43:19","","15","","<h1>TL;DR</h1>

<p>A lot of effort to find a marginally more efficient solution.  Difficult to justify the added complexity while sacrificing the simplicity of <code>df.drop(dlst, 1, errors='ignore')</code></p>

<pre><code>df.reindex_axis(np.setdiff1d(df.columns.values, dlst), 1)
</code></pre>

<p><strong>Preamble</strong><br>
Deleting a column is semantically the same as selecting the other columns.  I'll show a few additional methods to consider.  </p>

<p>I'll also focus on the general solution of deleting multiple columns at once and allowing for the attempt to delete columns not present.  </p>

<p>Using these solutions are general and will work for the simple case as well.</p>

<hr>

<p><strong>Setup</strong><br>
Consider the <code>pd.DataFrame</code> <code>df</code> and list to delete <code>dlst</code></p>

<pre><code>df = pd.DataFrame(dict(zip('ABCDEFGHIJ', range(1, 11))), range(3))
dlst = list('HIJKLM')
</code></pre>

<hr>

<pre><code>df

   A  B  C  D  E  F  G  H  I   J
0  1  2  3  4  5  6  7  8  9  10
1  1  2  3  4  5  6  7  8  9  10
2  1  2  3  4  5  6  7  8  9  10
</code></pre>

<hr>

<pre><code>dlst

['H', 'I', 'J', 'K', 'L', 'M']
</code></pre>

<p>The result should look like:</p>

<pre><code>df.drop(dlst, 1, errors='ignore')

   A  B  C  D  E  F  G
0  1  2  3  4  5  6  7
1  1  2  3  4  5  6  7
2  1  2  3  4  5  6  7
</code></pre>

<hr>

<p>Since I'm equating deleting a column to selecting the other columns, I'll break it into two types:</p>

<ol>
<li>Label selection</li>
<li>Boolean selection</li>
</ol>

<hr>

<h1>Label Selection</h1>

<p>We start by manufacturing the list/array of labels that represent the columns we want to keep and without the columns we want to delete.</p>

<ol>
<li><p><code>df.columns.difference(dlst)</code></p>

<pre><code>Index(['A', 'B', 'C', 'D', 'E', 'F', 'G'], dtype='object')
</code></pre></li>
<li><p><code>np.setdiff1d(df.columns.values, dlst)</code></p>

<pre><code>array(['A', 'B', 'C', 'D', 'E', 'F', 'G'], dtype=object)
</code></pre></li>
<li><p><code>df.columns.drop(dlst, errors='ignore')</code></p>

<pre><code>Index(['A', 'B', 'C', 'D', 'E', 'F', 'G'], dtype='object')
</code></pre></li>
<li><p><code>list(set(df.columns.values.tolist()).difference(dlst))</code></p>

<pre><code># does not preserve order
['E', 'D', 'B', 'F', 'G', 'A', 'C']
</code></pre></li>
<li><p><code>[x for x in df.columns.values.tolist() if x not in dlst]</code></p>

<pre><code>['A', 'B', 'C', 'D', 'E', 'F', 'G']
</code></pre>

<hr></li>
</ol>

<p><strong>Columns from Labels</strong><br>
For the sake of comparing the selection process, assume:</p>

<pre><code> cols = [x for x in df.columns.values.tolist() if x not in dlst]
</code></pre>

<p>Then we can evaluate  </p>

<ol>
<li><code>df.loc[:, cols]</code></li>
<li><code>df[cols]</code></li>
<li><code>df.reindex(columns=cols)</code></li>
<li><code>df.reindex_axis(cols, 1)</code></li>
</ol>

<p>Which all evaluate to:</p>

<pre><code>   A  B  C  D  E  F  G
0  1  2  3  4  5  6  7
1  1  2  3  4  5  6  7
2  1  2  3  4  5  6  7
</code></pre>

<hr>

<h1>Boolean Slice</h1>

<p>We can construct an array/list of booleans for slicing</p>

<ol>
<li><code>~df.columns.isin(dlst)</code></li>
<li><code>~np.in1d(df.columns.values, dlst)</code></li>
<li><code>[x not in dlst for x in df.columns.values.tolist()]</code></li>
<li><code>(df.columns.values[:, None] != dlst).all(1)</code></li>
</ol>

<p><strong>Columns from Boolean</strong><br>
For the sake of comparison  </p>

<pre><code>bools = [x not in dlst for x in df.columns.values.tolist()]
</code></pre>

<ol>
<li><code>df.loc[: bools]</code></li>
</ol>

<p>Which all evaluate to:</p>

<pre><code>   A  B  C  D  E  F  G
0  1  2  3  4  5  6  7
1  1  2  3  4  5  6  7
2  1  2  3  4  5  6  7
</code></pre>

<hr>

<p><strong>Robust Timing</strong>  </p>

<p><em>Functions</em>  </p>

<pre><code>setdiff1d = lambda df, dlst: np.setdiff1d(df.columns.values, dlst)
difference = lambda df, dlst: df.columns.difference(dlst)
columndrop = lambda df, dlst: df.columns.drop(dlst, errors='ignore')
setdifflst = lambda df, dlst: list(set(df.columns.values.tolist()).difference(dlst))
comprehension = lambda df, dlst: [x for x in df.columns.values.tolist() if x not in dlst]

loc = lambda df, cols: df.loc[:, cols]
slc = lambda df, cols: df[cols]
ridx = lambda df, cols: df.reindex(columns=cols)
ridxa = lambda df, cols: df.reindex_axis(cols, 1)

isin = lambda df, dlst: ~df.columns.isin(dlst)
in1d = lambda df, dlst: ~np.in1d(df.columns.values, dlst)
comp = lambda df, dlst: [x not in dlst for x in df.columns.values.tolist()]
brod = lambda df, dlst: (df.columns.values[:, None] != dlst).all(1)
</code></pre>

<p><strong>Testing</strong>  </p>

<pre><code>res1 = pd.DataFrame(
    index=pd.MultiIndex.from_product([
        'loc slc ridx ridxa'.split(),
        'setdiff1d difference columndrop setdifflst comprehension'.split(),
    ], names=['Select', 'Label']),
    columns=[10, 30, 100, 300, 1000],
    dtype=float
)

res2 = pd.DataFrame(
    index=pd.MultiIndex.from_product([
        'loc'.split(),
        'isin in1d comp brod'.split(),
    ], names=['Select', 'Label']),
    columns=[10, 30, 100, 300, 1000],
    dtype=float
)

res = res1.append(res2).sort_index()

dres = pd.Series(index=res.columns, name='drop')

for j in res.columns:
    dlst = list(range(j))
    cols = list(range(j // 2, j + j // 2))
    d = pd.DataFrame(1, range(10), cols)
    dres.at[j] = timeit('d.drop(dlst, 1, errors=""ignore"")', 'from __main__ import d, dlst', number=100)
    for s, l in res.index:
        stmt = '{}(d, {}(d, dlst))'.format(s, l)
        setp = 'from __main__ import d, dlst, {}, {}'.format(s, l)
        res.at[(s, l), j] = timeit(stmt, setp, number=100)

rs = res / dres
</code></pre>

<hr>

<pre><code>rs

                          10        30        100       300        1000
Select Label                                                           
loc    brod           0.747373  0.861979  0.891144  1.284235   3.872157
       columndrop     1.193983  1.292843  1.396841  1.484429   1.335733
       comp           0.802036  0.732326  1.149397  3.473283  25.565922
       comprehension  1.463503  1.568395  1.866441  4.421639  26.552276
       difference     1.413010  1.460863  1.587594  1.568571   1.569735
       in1d           0.818502  0.844374  0.994093  1.042360   1.076255
       isin           1.008874  0.879706  1.021712  1.001119   0.964327
       setdiff1d      1.352828  1.274061  1.483380  1.459986   1.466575
       setdifflst     1.233332  1.444521  1.714199  1.797241   1.876425
ridx   columndrop     0.903013  0.832814  0.949234  0.976366   0.982888
       comprehension  0.777445  0.827151  1.108028  3.473164  25.528879
       difference     1.086859  1.081396  1.293132  1.173044   1.237613
       setdiff1d      0.946009  0.873169  0.900185  0.908194   1.036124
       setdifflst     0.732964  0.823218  0.819748  0.990315   1.050910
ridxa  columndrop     0.835254  0.774701  0.907105  0.908006   0.932754
       comprehension  0.697749  0.762556  1.215225  3.510226  25.041832
       difference     1.055099  1.010208  1.122005  1.119575   1.383065
       setdiff1d      0.760716  0.725386  0.849949  0.879425   0.946460
       setdifflst     0.710008  0.668108  0.778060  0.871766   0.939537
slc    columndrop     1.268191  1.521264  2.646687  1.919423   1.981091
       comprehension  0.856893  0.870365  1.290730  3.564219  26.208937
       difference     1.470095  1.747211  2.886581  2.254690   2.050536
       setdiff1d      1.098427  1.133476  1.466029  2.045965   3.123452
       setdifflst     0.833700  0.846652  1.013061  1.110352   1.287831
</code></pre>

<hr>

<pre><code>fig, axes = plt.subplots(2, 2, figsize=(8, 6), sharey=True)
for i, (n, g) in enumerate([(n, g.xs(n)) for n, g in rs.groupby('Select')]):
    ax = axes[i // 2, i % 2]
    g.plot.bar(ax=ax, title=n)
    ax.legend_.remove()
fig.tight_layout()
</code></pre>

<p>This is relative to the time it takes to run <code>df.drop(dlst, 1, errors='ignore')</code>.  It seems like after all that effort, we only improve performance modestly.</p>

<p><a href=""https://i.stack.imgur.com/208z6.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/208z6.png"" alt=""enter image description here""></a></p>

<p>If fact the best solutions use <code>reindex</code> or <code>reindex_axis</code> on the hack <code>list(set(df.columns.values.tolist()).difference(dlst))</code>.  A close second and still very marginally better than <code>drop</code> is <code>np.setdiff1d</code>.</p>

<pre><code>rs.idxmin().pipe(
    lambda x: pd.DataFrame(
        dict(idx=x.values, val=rs.lookup(x.values, x.index)),
        x.index
    )
)

                      idx       val
10     (ridx, setdifflst)  0.653431
30    (ridxa, setdifflst)  0.746143
100   (ridxa, setdifflst)  0.816207
300    (ridx, setdifflst)  0.780157
1000  (ridxa, setdifflst)  0.861622
</code></pre>
","2336654","","2336654","","2017-09-20 14:28:55","2017-09-20 14:28:55","","","","0","","","","CC BY-SA 3.0"
"46913256","2","","13411544","2017-10-24 14:31:03","","28","","<h1>Pandas 0.21+ answer</h1>

<p>Pandas version 0.21 has changed the <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html"" rel=""noreferrer""><code>drop</code></a> method slightly to include both the <code>index</code> and <code>columns</code> parameters to match the signature of the <code>rename</code> and <code>reindex</code> methods. </p>

<pre><code>df.drop(columns=['column_a', 'column_c'])
</code></pre>

<p>Personally, I prefer using the <code>axis</code> parameter to denote columns or index because it is the predominant keyword parameter used in nearly all pandas methods. But, now you have some added choices in version 0.21.</p>
","3707607","","832230","","2018-09-20 18:48:41","2018-09-20 18:48:41","","","","1","","","","CC BY-SA 4.0"
"61305524","2","","13411544","2020-04-19 13:58:43","","9","","<p>We can <strong>Remove</strong> or <strong>Delete</strong> a specified column or sprcified columns by <strong>drop()</strong> method. </p>

<p>Suppose <strong>df</strong> is a dataframe. </p>

<p>Column to be removed = column0</p>

<blockquote>
  <p>Code:</p>
</blockquote>

<pre><code>df = df.drop(column0, axis=1)
</code></pre>

<p>To remove multiple columns col1, col2, . . . , coln, we have to insert all the columns that needed to be removed in a list. Then remove them by drop() method.</p>

<blockquote>
  <p>Code:</p>
</blockquote>

<pre><code>df = df.drop([col1, col2, . . . , coln], axis=1)
</code></pre>

<p>I hope it would be helpful. </p>
","12266677","","","","","2020-04-19 13:58:43","","","","1","","","","CC BY-SA 4.0"
"14268804","2","","14262433","2013-01-10 22:57:22","","651","","<p>I routinely use tens of gigabytes of data in just this fashion
e.g. I have tables on disk that I read via queries, create data and append back.</p>

<p>It's worth reading <a href=""http://pandas-docs.github.io/pandas-docs-travis/io.html#hdf5-pytables"" rel=""noreferrer"">the docs</a> and <a href=""https://groups.google.com/forum/m/?fromgroups#!topic/pydata/cmw1F3OFJSc"" rel=""noreferrer"">late in this thread</a> for several suggestions for how to store your data.</p>

<p>Details which will affect how you store your data, like:<br>
<em>Give as much detail as you can; and I can help you develop a structure.</em></p>

<ol>
<li>Size of data, # of rows, columns, types of columns; are you appending
rows, or just columns? </li>
<li>What will typical operations look like. E.g. do a query on columns to select a bunch of rows and specific columns, then do an operation (in-memory), create new columns, save these.<br>
(Giving a toy example could enable us to offer more specific recommendations.)</li>
<li>After that processing, then what do you do? Is step 2 ad hoc, or repeatable?</li>
<li>Input flat files: how many, rough total size in Gb. How are these organized e.g. by records? Does each one contains different fields, or do they have some records per file with all of the fields in each file?</li>
<li>Do you ever select subsets of rows (records) based on criteria (e.g. select the rows with field A > 5)? and then do something, or do you just select fields A, B, C with all of the records (and then do something)?</li>
<li>Do you 'work on' all of your columns (in groups), or are there a good proportion that you may only use for reports (e.g. you want to keep the data around, but don't need to pull in that column explicity until final results time)?</li>
</ol>

<h2>Solution</h2>

<p><em>Ensure you have <a href=""http://pandas.pydata.org/getpandas.html"" rel=""noreferrer"">pandas at least <code>0.10.1</code></a> installed.</em></p>

<p>Read <a href=""http://pandas-docs.github.io/pandas-docs-travis/io.html#iterating-through-files-chunk-by-chunk"" rel=""noreferrer"">iterating files chunk-by-chunk</a> and <a href=""http://pandas-docs.github.io/pandas-docs-travis/io.html#multiple-table-queries"" rel=""noreferrer"">multiple table queries</a>.</p>

<p>Since pytables is optimized to operate on row-wise (which is what you query on), we will create a table for each group of fields. This way it's easy to select a small group of fields (which will work with a big table, but it's more efficient to do it this way... I think I may be able to fix this limitation in the future... this is more intuitive anyhow):<br>
(The following is pseudocode.)</p>

<pre><code>import numpy as np
import pandas as pd

# create a store
store = pd.HDFStore('mystore.h5')

# this is the key to your storage:
#    this maps your fields to a specific group, and defines 
#    what you want to have as data_columns.
#    you might want to create a nice class wrapping this
#    (as you will want to have this map and its inversion)  
group_map = dict(
    A = dict(fields = ['field_1','field_2',.....], dc = ['field_1',....,'field_5']),
    B = dict(fields = ['field_10',......        ], dc = ['field_10']),
    .....
    REPORTING_ONLY = dict(fields = ['field_1000','field_1001',...], dc = []),

)

group_map_inverted = dict()
for g, v in group_map.items():
    group_map_inverted.update(dict([ (f,g) for f in v['fields'] ]))
</code></pre>

<p>Reading in the files and creating the storage (essentially doing what <code>append_to_multiple</code> does):</p>

<pre><code>for f in files:
   # read in the file, additional options may be necessary here
   # the chunksize is not strictly necessary, you may be able to slurp each 
   # file into memory in which case just eliminate this part of the loop 
   # (you can also change chunksize if necessary)
   for chunk in pd.read_table(f, chunksize=50000):
       # we are going to append to each table by group
       # we are not going to create indexes at this time
       # but we *ARE* going to create (some) data_columns

       # figure out the field groupings
       for g, v in group_map.items():
             # create the frame for this group
             frame = chunk.reindex(columns = v['fields'], copy = False)    

             # append it
             store.append(g, frame, index=False, data_columns = v['dc'])
</code></pre>

<p>Now you have all of the tables in the file (actually you could store them in separate files if you wish, you would prob have to add the filename to the group_map, but probably this isn't necessary).</p>

<p>This is how you get columns and create new ones:</p>

<pre><code>frame = store.select(group_that_I_want)
# you can optionally specify:
# columns = a list of the columns IN THAT GROUP (if you wanted to
#     select only say 3 out of the 20 columns in this sub-table)
# and a where clause if you want a subset of the rows

# do calculations on this frame
new_frame = cool_function_on_frame(frame)

# to 'add columns', create a new group (you probably want to
# limit the columns in this new_group to be only NEW ones
# (e.g. so you don't overlap from the other tables)
# add this info to the group_map
store.append(new_group, new_frame.reindex(columns = new_columns_created, copy = False), data_columns = new_columns_created)
</code></pre>

<p>When you are ready for post_processing:</p>

<pre><code># This may be a bit tricky; and depends what you are actually doing.
# I may need to modify this function to be a bit more general:
report_data = store.select_as_multiple([groups_1,groups_2,.....], where =['field_1&gt;0', 'field_1000=foo'], selector = group_1)
</code></pre>

<p>About data_columns, you don't actually need to define <strong>ANY</strong> data_columns; they allow you to sub-select rows based on the column. E.g. something like:</p>

<pre><code>store.select(group, where = ['field_1000=foo', 'field_1001&gt;0'])
</code></pre>

<p>They may be most interesting to you in the final report generation stage (essentially a data column is segregated from other columns, which might impact efficiency somewhat if you define a lot).</p>

<p>You also might want to:</p>

<ul>
<li>create a function which takes a list of fields, looks up the groups in the groups_map, then selects these and concatenates the results so you get the resulting frame (this is essentially what select_as_multiple does). <em>This way the structure would be pretty transparent to you.</em></li>
<li>indexes on certain data columns (makes row-subsetting much faster).</li>
<li>enable compression.</li>
</ul>

<p>Let me know when you have questions!</p>
","644898","","9513184","","2020-03-11 20:29:47","2020-03-11 20:29:47","","","","17","","","","CC BY-SA 4.0"
"14287518","2","","14262433","2013-01-11 22:11:52","","54","","<p>This is the case for pymongo.  I have also prototyped using sql server, sqlite, HDF, ORM (SQLAlchemy) in python.  First and foremost pymongo is a document based DB, so each person would be a document (<code>dict</code> of attributes).  Many people form a collection and you can have many collections (people, stock market, income).</p>

<p>pd.dateframe -> pymongo Note: I use the <code>chunksize</code> in <code>read_csv</code> to keep it to 5 to 10k records(pymongo drops the socket if larger)</p>

<pre><code>aCollection.insert((a[1].to_dict() for a in df.iterrows()))
</code></pre>

<p>querying: gt = greater than...</p>

<pre><code>pd.DataFrame(list(mongoCollection.find({'anAttribute':{'$gt':2887000, '$lt':2889000}})))
</code></pre>

<p><code>.find()</code> returns an iterator so I commonly use <code>ichunked</code> to chop into smaller iterators.  </p>

<p>How about a join since I normally get 10 data sources to paste together:</p>

<pre><code>aJoinDF = pandas.DataFrame(list(mongoCollection.find({'anAttribute':{'$in':Att_Keys}})))
</code></pre>

<p>then (in my case sometimes I have to agg on <code>aJoinDF</code> first before its ""mergeable"".)</p>

<pre><code>df = pandas.merge(df, aJoinDF, on=aKey, how='left')
</code></pre>

<p>And you can then write the new info to your main collection via the update method below. (logical collection vs physical datasources).</p>

<pre><code>collection.update({primarykey:foo},{key:change})
</code></pre>

<p>On smaller lookups, just denormalize.  For example, you have code in the document and you just add the field code text and do a <code>dict</code> lookup as you create documents.</p>

<p>Now you have a nice dataset based around a person, you can unleash your logic on each case and make more attributes. Finally you can read into pandas your 3 to memory max key indicators and do pivots/agg/data exploration.  This works for me for 3 million records with numbers/big text/categories/codes/floats/...</p>

<p>You can also use the two methods built into MongoDB (MapReduce and aggregate framework). <a href=""http://docs.mongodb.org/manual/tutorial/aggregation-examples/"">See here for more info about the aggregate framework</a>, as it seems to be easier than MapReduce and looks handy for quick aggregate work.  Notice I didn't need to define my fields or relations, and I can add items to a document.  At the current state of the rapidly changing numpy, pandas, python toolset, MongoDB helps me just get to work :)</p>
","1649635","","1461210","","2014-08-14 13:50:41","2014-08-14 13:50:41","","","","2","","","","CC BY-SA 3.0"
"15558350","2","","14262433","2013-03-21 21:19:30","","46","","<p>I spotted this a little late, but I work with a similar problem (mortgage prepayment models). My solution has been to skip the pandas HDFStore layer and use straight pytables. I save each column as an individual HDF5 array in my final file.</p>

<p>My basic workflow is to first get a CSV file from the database. I gzip it, so it's not as huge. Then I convert that to a row-oriented HDF5 file, by iterating over it in python, converting each row to a real data type, and writing it to a HDF5 file. That takes some tens of minutes, but it doesn't use any memory, since it's only operating row-by-row. Then I ""transpose"" the row-oriented HDF5 file into a column-oriented HDF5 file.</p>

<p>The table transpose looks like:</p>

<pre><code>def transpose_table(h_in, table_path, h_out, group_name=""data"", group_path=""/""):
    # Get a reference to the input data.
    tb = h_in.getNode(table_path)
    # Create the output group to hold the columns.
    grp = h_out.createGroup(group_path, group_name, filters=tables.Filters(complevel=1))
    for col_name in tb.colnames:
        logger.debug(""Processing %s"", col_name)
        # Get the data.
        col_data = tb.col(col_name)
        # Create the output array.
        arr = h_out.createCArray(grp,
                                 col_name,
                                 tables.Atom.from_dtype(col_data.dtype),
                                 col_data.shape)
        # Store the data.
        arr[:] = col_data
    h_out.flush()
</code></pre>

<p>Reading it back in then looks like:</p>

<pre><code>def read_hdf5(hdf5_path, group_path=""/data"", columns=None):
    """"""Read a transposed data set from a HDF5 file.""""""
    if isinstance(hdf5_path, tables.file.File):
        hf = hdf5_path
    else:
        hf = tables.openFile(hdf5_path)

    grp = hf.getNode(group_path)
    if columns is None:
        data = [(child.name, child[:]) for child in grp]
    else:
        data = [(child.name, child[:]) for child in grp if child.name in columns]

    # Convert any float32 columns to float64 for processing.
    for i in range(len(data)):
        name, vec = data[i]
        if vec.dtype == np.float32:
            data[i] = (name, vec.astype(np.float64))

    if not isinstance(hdf5_path, tables.file.File):
        hf.close()
    return pd.DataFrame.from_items(data)
</code></pre>

<p>Now, I generally run this on a machine with a ton of memory, so I may not be careful enough with my memory usage. For example, by default the load operation reads the whole data set.</p>

<p>This generally works for me, but it's a bit clunky, and I can't use the fancy pytables magic.</p>

<p>Edit: The real advantage of this approach, over the array-of-records pytables default, is that I can then load the data into R using h5r, which can't handle tables. Or, at least, I've been unable to get it to load heterogeneous tables.</p>
","250839","","250839","","2013-03-22 15:38:07","2013-03-22 15:38:07","","","","4","","","","CC BY-SA 3.0"
"19739768","2","","14262433","2013-11-02 07:14:07","","66","","<p>If your datasets are between 1 and 20GB, you should get a workstation with 48GB of RAM. Then Pandas can hold the entire dataset in RAM. I know its not the answer you're looking for here, but doing scientific computing on a notebook with 4GB of RAM isn't reasonable.</p>
","13969","","","","","2013-11-02 07:14:07","","","","7","","","","CC BY-SA 3.0"
"20690383","2","","14262433","2013-12-19 19:46:48","","148","","<p>I think the answers above are missing a simple approach that I've found very useful. </p>

<p>When I have a file that is too large to load in memory, I break up the file into multiple smaller files (either by row or cols)</p>

<p>Example: In case of 30 days worth of trading data of ~30GB size, I break it into a file per day of ~1GB size. I subsequently process each file separately and aggregate results at the end</p>

<p>One of the biggest advantages is that it allows parallel processing of the files (either multiple threads or processes)</p>

<p>The other advantage is that file manipulation (like adding/removing dates in the example) can be accomplished by regular shell commands, which is not be possible in more advanced/complicated file formats</p>

<p>This approach doesn't cover all scenarios, but is very useful in a lot of them</p>
","1827356","","1827356","","2013-12-23 15:21:27","2013-12-23 15:21:27","","","","2","","","","CC BY-SA 3.0"
"26286140","2","","14262433","2014-10-09 19:07:16","","13","","<p>Consider <a href=""http://www.ruffus.org.uk/"" rel=""noreferrer"">Ruffus</a> if you go the simple path of creating a data pipeline which is broken down into multiple smaller files. </p>
","3966500","","","","","2014-10-09 19:07:16","","","","0","","","","CC BY-SA 3.0"
"27282644","2","","14262433","2014-12-03 22:09:40","","62","","<p>I know this is an old thread but I think the <a href=""https://github.com/ContinuumIO/blaze"">Blaze</a> library is worth checking out.  It's built for these types of situations.</p>

<p><strong>From the docs:</strong></p>

<p>Blaze extends the usability of NumPy and Pandas to distributed and out-of-core computing. Blaze provides an interface similar to that of the NumPy ND-Array or Pandas DataFrame but maps these familiar interfaces onto a variety of other computational engines like Postgres or Spark.</p>

<p><strong>Edit:</strong> By the way, it's supported by ContinuumIO and Travis Oliphant, author of NumPy.</p>
","2464684","","","","","2014-12-03 22:09:40","","","","1","","","","CC BY-SA 3.0"
"29910919","2","","14262433","2015-04-28 05:22:21","","20","","<p>One more variation</p>

<p>Many of the operations done in pandas can also be done as a db query (sql, mongo)</p>

<p>Using a RDBMS or mongodb allows you to perform some of the aggregations in the DB Query (which is optimized for large data, and uses cache and indexes efficiently)</p>

<p>Later, you can perform post processing using pandas.</p>

<p>The advantage of this method is that you gain the DB optimizations for working with large data, while still defining the logic in a high level declarative syntax - and not having to deal with the details of deciding what to do in memory and what to do out of core.</p>

<p>And although the query language and pandas are different, it's usually not complicated to translate part of the logic from one to another.</p>
","590335","","","","","2015-04-28 05:22:21","","","","0","","","","CC BY-SA 3.0"
"36188131","2","","14262433","2016-03-23 20:30:53","","89","","<p>There is now, two years after the question, an 'out-of-core' pandas equivalent: <a href=""http://dask.pydata.org/en/latest/"" rel=""noreferrer"">dask</a>. It is excellent! Though it does not support all of pandas functionality, you can get really far with it.</p>
","2115409","","","","","2016-03-23 20:30:53","","","","3","","","","CC BY-SA 3.0"
"39856364","2","","14262433","2016-10-04 15:32:56","","10","","<p>I recently came across a similar issue. I found simply reading the data in chunks and appending it as I write it in chunks to the same csv works well. My problem was adding a date column based on information in another table, using the value of certain columns as follows. This may help those confused by dask and hdf5 but more familiar with pandas like myself. </p>

<pre><code>def addDateColumn():
""""""Adds time to the daily rainfall data. Reads the csv as chunks of 100k 
   rows at a time and outputs them, appending as needed, to a single csv. 
   Uses the column of the raster names to get the date.
""""""
    df = pd.read_csv(pathlist[1]+""CHIRPS_tanz.csv"", iterator=True, 
                     chunksize=100000) #read csv file as 100k chunks

    '''Do some stuff'''

    count = 1 #for indexing item in time list 
    for chunk in df: #for each 100k rows
        newtime = [] #empty list to append repeating times for different rows
        toiterate = chunk[chunk.columns[2]] #ID of raster nums to base time
        while count &lt;= toiterate.max():
            for i in toiterate: 
                if i ==count:
                    newtime.append(newyears[count])
            count+=1
        print ""Finished"", str(chunknum), ""chunks""
        chunk[""time""] = newtime #create new column in dataframe based on time
        outname = ""CHIRPS_tanz_time2.csv""
        #append each output to same csv, using no header
        chunk.to_csv(pathlist[2]+outname, mode='a', header=None, index=None)
</code></pre>
","4624988","","","","","2016-10-04 15:32:56","","","","0","","","","CC BY-SA 3.0"
"43025589","2","","14262433","2017-03-26 05:59:45","","46","","<p>One trick I found helpful for <strong>large data</strong> use cases is to reduce the volume of the data by reducing float precision to 32-bit. It's not applicable in all cases, but in many applications 64-bit precision is overkill and the 2x memory savings are worth it. To make an obvious point even more obvious:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame(np.random.randn(int(1e8), 5))
&gt;&gt;&gt; df.info()
&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 100000000 entries, 0 to 99999999
Data columns (total 5 columns):
...
dtypes: float64(5)
memory usage: 3.7 GB

&gt;&gt;&gt; df.astype(np.float32).info()
&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 100000000 entries, 0 to 99999999
Data columns (total 5 columns):
...
dtypes: float32(5)
memory usage: 1.9 GB
</code></pre>
","1541036","","7932273","","2018-11-03 06:51:24","2018-11-03 06:51:24","","","","1","","","","CC BY-SA 4.0"
"47426521","2","","14262433","2017-11-22 03:55:40","","34","","<p>As noted by others, after some years an 'out-of-core' pandas equivalent has emerged: <a href=""http://dask.pydata.org/en/latest/"" rel=""noreferrer"">dask</a>. Though dask is not a drop-in replacement of pandas and all of its functionality it stands out for several reasons:</p>

<p>Dask is a flexible parallel computing library for analytic computing that is optimized for dynamic task scheduling for interactive computational workloads of
Big Data collections like parallel arrays, dataframes, and lists that extend common interfaces like NumPy, Pandas, or Python iterators to larger-than-memory or distributed environments and scales from laptops to clusters.</p>

<blockquote>
  <p>Dask emphasizes the following virtues:  </p>
  
  <ul>
  <li>Familiar: Provides parallelized NumPy array and Pandas DataFrame    objects</li>
  <li>Flexible: Provides a task scheduling interface for more custom workloads and integration with other projects.</li>
  <li>Native: Enables distributed computing in Pure Python with access to the    PyData stack.</li>
  <li>Fast: Operates with low overhead, low latency, and    minimal serialization necessary for fast numerical algorithms  </li>
  <li>Scales up: Runs resiliently on clusters with 1000s of cores   Scales    down: Trivial to set up and run on a laptop in a single process  </li>
  <li>Responsive: Designed with interactive computing in mind it provides    rapid feedback and diagnostics to aid humans</li>
  </ul>
</blockquote>

<p>and to add a simple code sample:</p>

<pre><code>import dask.dataframe as dd
df = dd.read_csv('2015-*-*.csv')
df.groupby(df.user_id).value.mean().compute()
</code></pre>

<p>replaces some pandas code like this:</p>

<pre><code>import pandas as pd
df = pd.read_csv('2015-01-01.csv')
df.groupby(df.user_id).value.mean()
</code></pre>

<p>and, especially noteworthy, provides through the <code>concurrent.futures</code> interface a general infrastructure for the submission of custom tasks:</p>

<pre><code>from dask.distributed import Client
client = Client('scheduler:port')

futures = []
for fn in filenames:
    future = client.submit(load, fn)
    futures.append(future)

summary = client.submit(summarize, futures)
summary.result()
</code></pre>
","8291949","","8291949","","2020-01-08 17:07:24","2020-01-08 17:07:24","","","","2","","","","CC BY-SA 4.0"
"49346364","2","","14262433","2018-03-18 09:30:39","","20","","<p>It is worth mentioning here <a href=""https://github.com/ray-project/ray"" rel=""noreferrer"">Ray</a> as well,<br>
it's a distributed computation framework, that has it's own implementation for pandas in a distributed way.  </p>

<p>Just replace the pandas import, and the code should work as is:</p>

<pre><code># import pandas as pd
import ray.dataframe as pd

#use pd as usual
</code></pre>

<p>can read more details here:</p>

<p><a href=""https://rise.cs.berkeley.edu/blog/pandas-on-ray/"" rel=""noreferrer"">https://rise.cs.berkeley.edu/blog/pandas-on-ray/</a></p>
","245024","","","","","2018-03-18 09:30:39","","","","0","","","","CC BY-SA 3.0"
"56424715","2","","14262433","2019-06-03 09:40:50","","11","","<p>I'd like to point out the Vaex package.</p>

<blockquote>
  <p>Vaex is a python library for lazy Out-of-Core DataFrames (similar to Pandas), to visualize and explore big tabular datasets. It can calculate statistics such as mean, sum, count, standard deviation etc, on an N-dimensional grid up to a billion (10<sup>9</sup>) objects/rows per second. Visualization is done using histograms, density plots and 3d volume rendering, allowing interactive exploration of big data. Vaex uses memory mapping, zero memory copy policy and lazy computations for best performance (no memory wasted).</p>
</blockquote>

<p>Have a look at the documentation: <a href=""https://vaex.readthedocs.io/en/latest/"" rel=""noreferrer"">https://vaex.readthedocs.io/en/latest/</a>
The API is very close to the API of pandas.</p>
","2354344","","","","","2019-06-03 09:40:50","","","","0","","","","CC BY-SA 4.0"
"15943975","2","","15943769","2013-04-11 08:24:29","","1484","","<p>You can use the <code>.shape</code> property or just <code>len(DataFrame.index)</code>. However, there are notable performance differences ( <code>len(DataFrame.index)</code> is fastest).</p>
<p><a href=""https://i.stack.imgur.com/YcG3I.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/YcG3I.png"" alt=""enter image description here"" /></a></p>
<hr />
<p>Code to reproduce the plot:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import pandas as pd
import perfplot


perfplot.save(
    &quot;out.png&quot;,
    setup=lambda n: pd.DataFrame(np.arange(n * 3).reshape(n, 3)),
    n_range=[2**k for k in range(25)],
    kernels=[
        lambda data: data.shape[0],
        lambda data: data[0].count(),
        lambda data: len(data.index),
    ],
    labels=[&quot;data.shape[0]&quot;, &quot;data[0].count()&quot;, &quot;len(data.index)&quot;],
    xlabel=&quot;data rows&quot;
)
</code></pre>
<hr />
<p>EDIT: As @Dan Allen noted in the comments <code>len(df.index)</code> and <code>df[0].count()</code> are not interchangeable as <code>count</code> excludes <code>NaN</code>s,</p>
","1199589","","353337","","2020-08-31 13:10:47","2020-08-31 13:10:47","","","","14","","","","CC BY-SA 4.0"
"18317067","2","","15943769","2013-08-19 15:02:45","","169","","<p>Use <code>len(df)</code>.</p>
<p><code>__len__()</code> is currently (0.12) documented with <code>Returns length of index</code>. Timing info, set up the same way as in root's answer:</p>
<pre><code>In [7]: timeit len(df.index)
1000000 loops, best of 3: 248 ns per loop

In [8]: timeit len(df)
1000000 loops, best of 3: 573 ns per loop
</code></pre>
<p>Due to one additional function call it is a bit slower than calling <code>len(df.index)</code> directly, but this should not play any role in most use cases.</p>
","145400","","145400","","2020-10-21 08:32:21","2020-10-21 08:32:21","","","","0","","","","CC BY-SA 4.0"
"32103678","2","","15943769","2015-08-19 19:07:17","","20","","<p>Apart from above answers use can use <code>df.axes</code> to get the tuple with row and column indexes and then use <code>len()</code> function:</p>

<pre><code>total_rows=len(df.axes[0])
total_cols=len(df.axes[1])
</code></pre>
","4651101","","1820286","","2015-08-19 19:28:21","2015-08-19 19:28:21","","","","1","","","","CC BY-SA 3.0"
"35523946","2","","15943769","2016-02-20 13:30:05","","343","","<p>Suppose <code>df</code> is your dataframe then:</p>

<pre><code>count_row = df.shape[0]  # gives number of row count
count_col = df.shape[1]  # gives number of col count
</code></pre>

<p>Or, more succinctly, </p>

<pre><code>r, c = df.shape
</code></pre>
","5282071","","4909087","","2019-05-24 03:53:54","2019-05-24 03:53:54","","","","2","","","","CC BY-SA 4.0"
"38025280","2","","15943769","2016-06-25 05:23:38","","46","","<h2>TL;DR</h2>
<p>Short, clear and clean:  <strong>use <code>len(df)</code></strong></p>
<hr />
<p><strong><code>len()</code></strong> is your friend, it can be used for row counts as <code>len(df)</code>.</p>
<p>Alternatively, you can access all rows by <code>df.index</code> and all columns by
<code>df.columns</code>, and as you can use the <strong><code>len(anyList)</code></strong> for getting the count of list,  use
<code>len(df.index)</code> for getting the number of rows, and <code>len(df.columns)</code> for the column count.</p>
<p>Or, you can use <code>df.shape</code> which returns the number of rows and columns together, if you want to access the number of rows only use <code>df.shape[0]</code> and for the number of columns only use: <code>df.shape[1]</code>.</p>
","2234161","","2234161","","2020-09-14 12:24:58","2020-09-14 12:24:58","","","","0","","","","CC BY-SA 4.0"
"39764266","2","","15943769","2016-09-29 07:41:41","","7","","<p>I come to pandas from <code>R</code> background, and I see that pandas is more complicated when it comes to selecting row or column.
I had to wrestle with it for a while, then I found some ways to deal with:</p>

<p>getting the number of columns:</p>

<pre><code>len(df.columns)  
## Here:
#df is your data.frame
#df.columns return a string, it contains column's titles of the df. 
#Then, ""len()"" gets the length of it.
</code></pre>

<p>getting the number of rows:</p>

<pre><code>len(df.index) #It's similar.
</code></pre>
","3632002","","","","","2016-09-29 07:41:41","","","","1","","","","CC BY-SA 3.0"
"47705589","2","","15943769","2017-12-07 23:37:11","","10","","<p>...building on Jan-Philip Gehrcke's answer. </p>

<p>The reason why <code>len(df)</code> or <code>len(df.index)</code> is faster than <code>df.shape[0]</code>. Look at the code. df.shape is a <code>@property</code> that runs a DataFrame method calling <code>len</code> twice.</p>

<pre><code>df.shape??
Type:        property
String form: &lt;property object at 0x1127b33c0&gt;
Source:     
# df.shape.fget
@property
def shape(self):
    """"""
    Return a tuple representing the dimensionality of the DataFrame.
    """"""
    return len(self.index), len(self.columns)
</code></pre>

<p>And beneath the hood of len(df)</p>

<pre><code>df.__len__??
Signature: df.__len__()
Source:   
    def __len__(self):
        """"""Returns length of info axis, but here we use the index """"""
        return len(self.index)
File:      ~/miniconda2/lib/python2.7/site-packages/pandas/core/frame.py
Type:      instancemethod
</code></pre>

<p><code>len(df.index)</code> will be slightly faster than <code>len(df)</code> since it has one less function call, but this is always faster than <code>df.shape[0]</code></p>
","7723984","","","","","2017-12-07 23:37:11","","","","0","","","","CC BY-SA 3.0"
"55435185","2","","15943769","2019-03-30 19:55:03","","100","","<blockquote>
  <h1>How do I get the row count of a pandas DataFrame?</h1>
</blockquote>

<p>This table summarises the different situations in which you'd want to count something in a DataFrame (or Series, for completeness), along with the recommended method(s).</p>

<p><a href=""https://i.stack.imgur.com/3FXuI.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/3FXuI.png"" alt=""enter image description here""></a></p>

<blockquote>
  <p><strong>Footnotes</strong>   </p>
  
  <ol>
  <li><code>DataFrame.count</code> returns counts for each column as a <code>Series</code> since the non-null count varies by column.</li>
  <li><code>DataFrameGroupBy.size</code> returns a <code>Series</code>, since all columns in the same group share the same row-count.</li>
  <li><code>DataFrameGroupBy.count</code> returns a <code>DataFrame</code>, since the non-null count could differ across columns in the same group. To get the group-wise non-null count for a specific column, use <code>df.groupby(...)['x'].count()</code> where ""x"" is the column to count.</li>
  </ol>
</blockquote>

<hr>

<h1><strong>Minimal Code Examples</strong></h1>

<p>Below, I show examples of each of the methods described in the table above. First, the setup - </p>

<pre><code>df = pd.DataFrame({
    'A': list('aabbc'), 'B': ['x', 'x', np.nan, 'x', np.nan]})
s = df['B'].copy()

df

   A    B
0  a    x
1  a    x
2  b  NaN
3  b    x
4  c  NaN

s

0      x
1      x
2    NaN
3      x
4    NaN
Name: B, dtype: object
</code></pre>

<h3>Row Count of a DataFrame: <code>len(df)</code>, <code>df.shape[0]</code>, or <code>len(df.index)</code></h3>

<pre><code>len(df)
# 5

df.shape[0]
# 5

len(df.index)
# 5
</code></pre>

<p>It seems silly to compare the performance of constant time operations, especially when the difference is on the level of ""seriously, don't worry about it"". But this seems to be a trend with other answers, so I'm doing the same for completeness.</p>

<p>Of the 3 methods above, <code>len(df.index)</code> (as mentioned in other answers) is the fastest. </p>

<blockquote>
  <p><strong>Note</strong></p>
  
  <ul>
  <li>All the methods above are constant time operations as they are simple attribute lookups. </li>
  <li><code>df.shape</code> (similar to <code>ndarray.shape</code>) is an attribute that returns a tuple of <code>(# Rows, # Cols)</code>. For example, <code>df.shape</code> returns <code>(8,
  2)</code> for the example here.</li>
  </ul>
</blockquote>

<h3>Column Count of a DataFrame: <code>df.shape[1]</code>, <code>len(df.columns)</code></h3>

<pre><code>df.shape[1]
# 2

len(df.columns)
# 2
</code></pre>

<p>Analogous to <code>len(df.index)</code>, <code>len(df.columns)</code> is the faster of the two methods (but takes more characters to type).</p>

<h3>Row Count of a Series: <code>len(s)</code>, <code>s.size</code>, <code>len(s.index)</code></h3>

<pre><code>len(s)
# 5

s.size
# 5

len(s.index)
# 5
</code></pre>

<p><code>s.size</code> and <code>len(s.index)</code> are about the same in terms of speed. But I recommend <code>len(df)</code>. </p>

<blockquote>
  <p><strong>Note</strong><br>
  <code>size</code> is an attribute, and it returns the number of elements (=count
  of rows for any Series). DataFrames also define a size attribute which
  returns the same result as <code>df.shape[0] * df.shape[1]</code>.</p>
</blockquote>

<h3>Non-Null Row Count: <code>DataFrame.count</code> and <code>Series.count</code></h3>

<p>The methods described here only count non-null values (meaning NaNs are ignored). </p>

<p>Calling <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.count.html"" rel=""noreferrer""><code>DataFrame.count</code></a> will return non-NaN counts for <em>each</em> column:</p>

<pre><code>df.count()

A    5
B    3
dtype: int64
</code></pre>

<p>For Series, use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.count.html"" rel=""noreferrer""><code>Series.count</code></a> to similar effect:</p>

<pre><code>s.count()
# 3
</code></pre>

<h3>Group-wise Row Count: <code>GroupBy.size</code></h3>

<p>For <code>DataFrames</code>, use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.DataFrameGroupBy.size.html"" rel=""noreferrer""><code>DataFrameGroupBy.size</code></a> to count the number of rows per group.</p>

<pre><code>df.groupby('A').size()

A
a    2
b    2
c    1
dtype: int64
</code></pre>

<p>Similarly, for <code>Series</code>, you'll use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.size.html"" rel=""noreferrer""><code>SeriesGroupBy.size</code></a>.</p>

<pre><code>s.groupby(df.A).size()

A
a    2
b    2
c    1
Name: B, dtype: int64
</code></pre>

<p>In both cases, a <code>Series</code> is returned. This makes sense for <code>DataFrames</code> as well since all groups share the same row-count.</p>

<h3>Group-wise Non-Null Row Count: <code>GroupBy.count</code></h3>

<p>Similar to above, but use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.size.html"" rel=""noreferrer""><code>GroupBy.count</code></a>, not <code>GroupBy.size</code>. Note that <code>size</code> always returns a <code>Series</code>, while <code>count</code> returns a <code>Series</code> if called on a specific column, or else a <code>DataFrame</code>.</p>

<p>The following methods return the same thing:</p>

<pre><code>df.groupby('A')['B'].size()
df.groupby('A').size()

A
a    2
b    2
c    1
Name: B, dtype: int64
</code></pre>

<p>Meanwhile, for <code>count</code>, we have</p>

<pre><code>df.groupby('A').count()

   B
A   
a  2
b  1
c  0
</code></pre>

<p>...called on the entire GroupBy object, v/s, </p>

<pre><code>df.groupby('A')['B'].count()

A
a    2
b    1
c    0
Name: B, dtype: int64
</code></pre>

<p>Called on a specific column.</p>
","4909087","","4909087","","2019-05-24 04:05:28","2019-05-24 04:05:28","","","","0","","","","CC BY-SA 4.0"
"61745860","2","","15943769","2020-05-12 07:14:24","","6","","<p>Hey you can use do this also:</p>

<p>Let say <code>df</code> is your dataframe. Then <code>df.shape</code> gives you the shape of your dataframe i.e <code>(row,col)</code> </p>

<p>Thus, assign below command to get the required</p>

<pre><code> row = df.shape[0], col = df.shape[1]
</code></pre>
","10633191","","11180198","","2020-05-23 16:18:40","2020-05-23 16:18:40","","","","0","","","","CC BY-SA 4.0"
"10739432","2","","16476924","2012-05-24 14:24:52","","211","","<p>You should use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html"" rel=""noreferrer""><code>df.iterrows()</code></a>. Though iterating row-by-row is not especially efficient since <code>Series</code> objects have to be created.</p>
","776560","","4909087","","2019-12-11 18:42:58","2019-12-11 18:42:58","","","","3","","","","CC BY-SA 4.0"
"16476974","2","","16476924","2013-05-10 07:07:58","","3141","","<p><a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iterrows.html#pandas-dataframe-iterrows"" rel=""noreferrer""><code>DataFrame.iterrows</code></a> is a generator which yields both the index and row (as a Series):</p>
<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame({'c1': [10, 11, 12], 'c2': [100, 110, 120]})

for index, row in df.iterrows():
    print(row['c1'], row['c2'])
</code></pre>

<pre><code>10 100
11 110
12 120
</code></pre>
","1426056","","4909087","","2020-08-23 10:17:01","2020-08-23 10:17:01","","","","15","","","","CC BY-SA 4.0"
"30566899","2","","16476924","2015-06-01 06:24:44","","99","","<p>You can also use <code>df.apply()</code> to iterate over rows and access multiple columns for a function.</p>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html"">docs: DataFrame.apply()</a></p>

<pre><code>def valuation_formula(x, y):
    return x * y * 0.5

df['price'] = df.apply(lambda row: valuation_formula(row['x'], row['y']), axis=1)
</code></pre>
","1803298","","","","","2015-06-01 06:24:44","","","","7","","","","CC BY-SA 3.0"
"32680162","2","","16476924","2015-09-20 13:52:48","","162","","<p>While <code>iterrows()</code> is a good option, sometimes <code>itertuples()</code> can be much faster:</p>

<pre><code>df = pd.DataFrame({'a': randn(1000), 'b': randn(1000),'N': randint(100, 1000, (1000)), 'x': 'x'})

%timeit [row.a * 2 for idx, row in df.iterrows()]
# =&gt; 10 loops, best of 3: 50.3 ms per loop

%timeit [row[1] * 2 for row in df.itertuples()]
# =&gt; 1000 loops, best of 3: 541 s per loop
</code></pre>
","1054939","","1054939","","2016-06-01 09:00:01","2016-06-01 09:00:01","","","","9","","","","CC BY-SA 3.0"
"39370553","2","","16476924","2016-09-07 12:56:04","","94","","<p>You can use the df.iloc function as follows:</p>

<pre><code>for i in range(0, len(df)):
    print df.iloc[i]['c1'], df.iloc[i]['c2']
</code></pre>
","6523882","","6523882","","2016-11-07 09:09:10","2016-11-07 09:09:10","","","","5","","","","CC BY-SA 3.0"
"41022840","2","","16476924","2016-12-07 16:41:28","","430","","<p>First consider if you really need to <em>iterate</em> over rows in a DataFrame. See <a href=""https://stackoverflow.com/a/55557758/3844376"">this answer</a> for alternatives.</p>

<p>If you still need to iterate over rows, you can use methods below. Note some  <strong>important caveats</strong> which are not mentioned in any of the other answers.</p>

<ul>
<li><p><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html"" rel=""noreferrer"">DataFrame.iterrows()</a></p>

<pre><code>for index, row in df.iterrows():
    print(row[""c1""], row[""c2""])
</code></pre></li>
<li><p><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.itertuples.html"" rel=""noreferrer"">DataFrame.itertuples()</a></p>

<pre><code>for row in df.itertuples(index=True, name='Pandas'):
    print(row.c1, row.c2)
</code></pre></li>
</ul>

<p><code>itertuples()</code> is supposed to be faster than <code>iterrows()</code></p>

<p>But be aware, according to the docs (pandas 0.24.2 at the moment):</p>

<ul>
<li><p>iterrows: <code>dtype</code> might not match from row to row</p>

<blockquote>
  <p>Because iterrows returns a Series for each row, it <strong>does not preserve</strong> dtypes across the rows (dtypes are preserved across columns for DataFrames). To preserve dtypes while iterating over the rows, it is better to use itertuples() which returns namedtuples of the values and which is generally much faster than iterrows()</p>
</blockquote></li>
<li><p>iterrows: Do not modify rows</p>

<blockquote>
  <p>You should <strong>never modify</strong> something you are iterating over. This is not guaranteed to work in all cases. Depending on the data types, the iterator returns a copy and not a view, and writing to it will have no effect.</p>
</blockquote>

<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html"" rel=""noreferrer"">DataFrame.apply()</a> instead:</p>

<pre><code>new_df = df.apply(lambda x: x * 2)
</code></pre></li>
<li><p>itertuples: </p>

<blockquote>
  <p>The column names will be renamed to positional names if they are invalid Python identifiers, repeated, or start with an underscore. With a large number of columns (>255), regular tuples are returned.</p>
</blockquote></li>
</ul>

<p>See <a href=""https://pandas.pydata.org/pandas-docs/stable/getting_started/basics.html#iteration"" rel=""noreferrer"">pandas docs on iteration</a> for more details.</p>
","3844376","","4909087","","2020-04-23 19:41:52","2020-04-23 19:41:52","","","","8","","","","CC BY-SA 4.0"
"42741552","2","","16476924","2017-03-11 22:44:39","","18","","<p>To loop all rows in a <code>dataframe</code> you can use:</p>

<pre><code>for x in range(len(date_example.index)):
    print date_example['Date'].iloc[x]
</code></pre>
","797495","","797495","","2017-04-04 20:46:53","2017-04-04 20:46:53","","","","3","","","","CC BY-SA 3.0"
"47073107","2","","16476924","2017-11-02 10:33:40","","19","","<pre><code> for ind in df.index:
     print df['c1'][ind], df['c2'][ind]
</code></pre>
","6088984","","4909087","","2019-05-07 06:37:44","2019-05-07 06:37:44","","","","3","","","","CC BY-SA 4.0"
"47149876","2","","16476924","2017-11-07 04:15:19","","21","","<p>You can write your own iterator that implements <code>namedtuple</code></p>

<pre><code>from collections import namedtuple

def myiter(d, cols=None):
    if cols is None:
        v = d.values.tolist()
        cols = d.columns.values.tolist()
    else:
        j = [d.columns.get_loc(c) for c in cols]
        v = d.values[:, j].tolist()

    n = namedtuple('MyTuple', cols)

    for line in iter(v):
        yield n(*line)
</code></pre>

<p>This is directly comparable to <code>pd.DataFrame.itertuples</code>.  I'm aiming at performing the same task with more efficiency.</p>

<hr>

<p>For the given dataframe with my function:</p>

<pre><code>list(myiter(df))

[MyTuple(c1=10, c2=100), MyTuple(c1=11, c2=110), MyTuple(c1=12, c2=120)]
</code></pre>

<p>Or with <code>pd.DataFrame.itertuples</code>:</p>

<pre><code>list(df.itertuples(index=False))

[Pandas(c1=10, c2=100), Pandas(c1=11, c2=110), Pandas(c1=12, c2=120)]
</code></pre>

<hr>

<p><strong>A comprehensive test</strong><br>
We test making all columns available and subsetting the columns.  </p>

<pre><code>def iterfullA(d):
    return list(myiter(d))

def iterfullB(d):
    return list(d.itertuples(index=False))

def itersubA(d):
    return list(myiter(d, ['col3', 'col4', 'col5', 'col6', 'col7']))

def itersubB(d):
    return list(d[['col3', 'col4', 'col5', 'col6', 'col7']].itertuples(index=False))

res = pd.DataFrame(
    index=[10, 30, 100, 300, 1000, 3000, 10000, 30000],
    columns='iterfullA iterfullB itersubA itersubB'.split(),
    dtype=float
)

for i in res.index:
    d = pd.DataFrame(np.random.randint(10, size=(i, 10))).add_prefix('col')
    for j in res.columns:
        stmt = '{}(d)'.format(j)
        setp = 'from __main__ import d, {}'.format(j)
        res.at[i, j] = timeit(stmt, setp, number=100)

res.groupby(res.columns.str[4:-1], axis=1).plot(loglog=True);
</code></pre>

<p><a href=""https://i.stack.imgur.com/rt88e.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/rt88e.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/azbOF.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/azbOF.png"" alt=""enter image description here""></a></p>
","2336654","","2336654","","2017-11-07 04:29:57","2017-11-07 04:29:57","","","","1","","","","CC BY-SA 3.0"
"48297889","2","","16476924","2018-01-17 09:41:29","","43","","<p>I was looking for <strong>How to iterate on rows</strong> <strong><em>and</em></strong> <strong>columns</strong> and ended here so:</p>

<pre><code>for i, row in df.iterrows():
    for j, column in row.iteritems():
        print(column)
</code></pre>
","4458246","","63550","","2020-06-11 13:37:53","2020-06-11 13:37:53","","","","1","","","","CC BY-SA 4.0"
"49984074","2","","16476924","2018-04-23 14:53:49","","9","","<p>To loop all rows in a <code>dataframe</code> and <strong>use</strong> values of each row <strong>conveniently</strong>, <code>namedtuples</code> can be converted to <code>ndarray</code>s. For example:</p>

<pre><code>df = pd.DataFrame({'col1': [1, 2], 'col2': [0.1, 0.2]}, index=['a', 'b'])
</code></pre>

<p>Iterating over the rows:</p>

<pre><code>for row in df.itertuples(index=False, name='Pandas'):
    print np.asarray(row)
</code></pre>

<p>results in:</p>

<pre><code>[ 1.   0.1]
[ 2.   0.2]
</code></pre>

<p>Please note that if <code>index=True</code>, <strong>the index is added as the first element of the tuple</strong>, which may be undesirable for some applications.</p>
","3393574","","3393574","","2018-04-24 08:48:05","2018-04-24 08:48:05","","","","0","","","","CC BY-SA 3.0"
"51069586","2","","16476924","2018-06-27 18:48:28","","13","","<p>Sometimes a useful pattern is:</p>

<pre><code># Borrowing @KutalmisB df example
df = pd.DataFrame({'col1': [1, 2], 'col2': [0.1, 0.2]}, index=['a', 'b'])
# The to_dict call results in a list of dicts
# where each row_dict is a dictionary with k:v pairs of columns:value for that row
for row_dict in df.to_dict(orient='records'):
    print(row_dict)
</code></pre>

<p>Which results in:</p>

<pre><code>{'col1':1.0, 'col2':0.1}
{'col1':2.0, 'col2':0.2}
</code></pre>
","6951902","","4909087","","2019-04-13 23:06:06","2019-04-13 23:06:06","","","","0","","","","CC BY-SA 4.0"
"54264778","2","","16476924","2019-01-19 06:53:51","","6","","<p>There are so many ways to iterate over the rows in Pandas dataframe. One very simple and intuitive way is:</p>

<pre><code>df = pd.DataFrame({'A':[1, 2, 3], 'B':[4, 5, 6], 'C':[7, 8, 9]})
print(df)
for i in range(df.shape[0]):
    # For printing the second column
    print(df.iloc[i, 1])

    # For printing more than one columns
    print(df.iloc[i, [0, 2]])
</code></pre>
","8244390","","63550","","2020-06-11 13:38:59","2020-06-11 13:38:59","","","","0","","","","CC BY-SA 4.0"
"54896256","2","","16476924","2019-02-27 00:29:49","","9","","<p>For both viewing and modifying values, I would use <code>iterrows()</code>. In a for loop and by using tuple unpacking (see the example: <code>i, row</code>), I use the <code>row</code> for only viewing the value and use <code>i</code> with the <code>loc</code> method when I want to modify values. As stated in previous answers, here you should not modify something you are iterating over.</p>

<pre><code>for i, row in df.iterrows():
    df_column_A = df.loc[i, 'A']
    if df_column_A == 'Old_Value':
        df_column_A = 'New_value'  
</code></pre>

<p>Here the <code>row</code> in the loop is a copy of that row, and not a view of it. Therefore, you should NOT write something like <code>row['A'] = 'New_Value'</code>, it will not modify the DataFrame. However, you can use <code>i</code> and <code>loc</code> and specify the DataFrame to do the work.</p>
","10951905","","10951905","","2020-02-28 17:51:44","2020-02-28 17:51:44","","","","0","","","","CC BY-SA 4.0"
"55557758","2","","16476924","2019-04-07 10:03:54","","818","","<blockquote>
<h2>How to iterate over rows in a DataFrame in Pandas?</h2>
</blockquote>
<h1>Answer: DON'T<sup>*</sup>!</h1>
<p>Iteration in Pandas is an anti-pattern and is something you should only do when you have exhausted every other option. You should not use any function with &quot;<code>iter</code>&quot; in its name for more than a few thousand rows or you will have to get used to a <strong>lot</strong> of waiting.</p>
<p>Do you want to print a DataFrame? Use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_string.html"" rel=""noreferrer""><strong><code>DataFrame.to_string()</code></strong></a>.</p>
<p>Do you want to compute something? In that case, search for methods in this order (list modified from <a href=""https://stackoverflow.com/questions/24870953/does-iterrows-have-performance-issues"">here</a>):</p>
<ol>
<li>Vectorization</li>
<li><a href=""https://en.wikipedia.org/wiki/Cython"" rel=""noreferrer"">Cython</a> routines</li>
<li>List Comprehensions (vanilla <code>for</code> loop)</li>
<li><a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html"" rel=""noreferrer""><strong><code>DataFrame.apply()</code></strong></a>: i) Reductions that can be performed in Cython, ii) Iteration in Python space</li>
<li><a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.itertuples.html"" rel=""noreferrer""><strong><code>DataFrame.itertuples()</code></strong></a> and <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iteritems.html#pandas.DataFrame.iteritems"" rel=""noreferrer""><strong><code>iteritems()</code></strong></a></li>
<li><a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iterrows.html#pandas.DataFrame.iterrows"" rel=""noreferrer""><strong><code>DataFrame.iterrows()</code></strong></a></li>
</ol>
<p><code>iterrows</code> and <code>itertuples</code> (both receiving many votes in answers to this question) should be used in very rare circumstances, such as generating row objects/nametuples for sequential processing, which is really the only thing these functions are useful for.</p>
<p><strong>Appeal to Authority</strong></p>
<p><a href=""https://pandas.pydata.org/pandas-docs/stable/user_guide/basics.html#iteration"" rel=""noreferrer"">The documentation page</a> on iteration has a huge red warning box that says:</p>
<blockquote>
<p>Iterating through pandas objects is generally slow. In many cases, iterating manually over the rows is not needed [...].</p>
</blockquote>
<p><sub>* It's actually a little more complicated than &quot;don't&quot;. <code>df.iterrows()</code> is the correct answer to this question, but &quot;vectorize your ops&quot; is the better one. I will concede that there are circumstances where iteration cannot be avoided (for example, some operations where the result depends on the value computed for the previous row). However, it takes some familiarity with the library to know when. If you're not sure whether you need an iterative solution, you probably don't. PS: To know more about my rationale for writing this answer, skip to the very bottom.</sub></p>
<hr />
<h2>Faster than Looping: <a href=""https://stackoverflow.com/questions/1422149/what-is-vectorization"">Vectorization</a>, <a href=""https://cython.org"" rel=""noreferrer"">Cython</a></h2>
<p>A good number of basic operations and computations are &quot;vectorised&quot; by pandas (either through NumPy, or through Cythonized functions). This includes arithmetic, comparisons, (most) reductions, reshaping (such as pivoting), joins, and groupby operations. Look through the documentation on <a href=""https://pandas.pydata.org/pandas-docs/stable/user_guide/basics.html#essential-basic-functionality"" rel=""noreferrer"">Essential Basic Functionality</a> to find a suitable vectorised method for your problem.</p>
<p>If none exists, feel free to write your own using custom <a href=""https://pandas.pydata.org/pandas-docs/stable/user_guide/enhancingperf.html#cython-writing-c-extensions-for-pandas"" rel=""noreferrer"">Cython extensions</a>.</p>
<hr />
<h2>Next Best Thing: <a href=""https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions"" rel=""noreferrer"">List Comprehensions</a><sup>*</sup></h2>
<p>List comprehensions should be your next port of call if 1) there is no vectorized solution available, 2) performance is important, but not important enough to go through the hassle of cythonizing your code, and 3) you're trying to perform elementwise transformation on your code. There is a <a href=""https://stackoverflow.com/questions/54028199/for-loops-with-pandas-when-should-i-care"">good amount of evidence</a> to suggest that list comprehensions are sufficiently fast (and even sometimes faster) for many common Pandas tasks.</p>
<p>The formula is simple,</p>
<pre><code># Iterating over one column - `f` is some function that processes your data
result = [f(x) for x in df['col']]
# Iterating over two columns, use `zip`
result = [f(x, y) for x, y in zip(df['col1'], df['col2'])]
# Iterating over multiple columns - same data type
result = [f(row[0], ..., row[n]) for row in df[['col1', ...,'coln']].to_numpy()]
# Iterating over multiple columns - differing data type
result = [f(row[0], ..., row[n]) for row in zip(df['col1'], ..., df['coln'])]
</code></pre>
<p>If you can encapsulate your business logic into a function, you can use a list comprehension that calls it. You can make arbitrarily complex things work through the simplicity and speed of raw Python code.</p>
<p><strong>Caveats</strong></p>
<p>List comprehensions assume that your data is easy to work with - what that means is your data types are consistent and you don't have NaNs, but this cannot always be guaranteed.</p>
<ol>
<li>The first one is more obvious, but when dealing with NaNs, prefer in-built pandas methods if they exist (because they have much better corner-case handling logic), or ensure your business logic includes appropriate NaN handling logic.</li>
<li>When dealing with mixed data types you should iterate over <code>zip(df['A'], df['B'], ...)</code> instead of <code>df[['A', 'B']].to_numpy()</code> as the latter implicitly upcasts data to the most common type. As an example if A is numeric and B is string, <code>to_numpy()</code> will cast the entire array to string, which may not be what you want. Fortunately <code>zip</code>ping your columns together is the most straightforward workaround to this.</li>
</ol>
<p><sub>*Your mileage may vary for the reasons outlined in the <strong>Caveats</strong> section above.</sub></p>
<hr />
<h2>An Obvious Example</h2>
<p>Let's demonstrate the difference with a simple example of adding two pandas columns <code>A + B</code>. This is a vectorizable operaton, so it will be easy to contrast the performance of the methods discussed above.</p>
<img src=""https://i.stack.imgur.com/ZY7Ec.png"" width=""600"" />
<p><a href=""https://gist.github.com/Coldsp33d/948f96b384ca5bdf6e8ce203ac97c9a0"" rel=""noreferrer"">Benchmarking code, for your reference</a>. The line at the bottom measures a function written in numpandas, a style of Pandas that mixes heavily with NumPy to squeeze out maximum performance. Writing numpandas code should be avoided unless you know what you're doing. Stick to the API where you can (i.e., prefer <code>vec</code> over <code>vec_numpy</code>).</p>
<p>I should mention, however, that it isn't always this cut and dry. Sometimes the answer to &quot;what is the best method for an operation&quot; is &quot;it depends on your data&quot;. My advice is to test out different approaches on your data before settling on one.</p>
<hr />
<h2>Further Reading</h2>
<ul>
<li><p><a href=""https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html"" rel=""noreferrer"">10 Minutes to pandas</a>, and <a href=""https://pandas.pydata.org/pandas-docs/stable/user_guide/basics.html"" rel=""noreferrer"">Essential Basic Functionality</a> - Useful links that introduce you to Pandas and its library of vectorized*/cythonized functions.</p>
</li>
<li><p><a href=""https://pandas.pydata.org/pandas-docs/stable/user_guide/enhancingperf.html"" rel=""noreferrer"">Enhancing Performance</a> - A primer from the documentation on enhancing standard Pandas operations</p>
</li>
<li><p><em><a href=""https://stackoverflow.com/questions/54028199/for-loops-with-pandas-when-should-i-care"">Are for-loops in pandas really bad? When should I care?</a></em> - a detailed writeup by me on list comprehensions and their suitability for various operations (mainly ones involving non-numeric data)</p>
</li>
<li><p><em><a href=""https://stackoverflow.com/questions/54432583/when-should-i-ever-want-to-use-pandas-apply-in-my-code"">When should I ever want to use pandas apply() in my code?</a></em> - <code>apply</code> is slow (but not as slow as the <code>iter*</code> family. There are, however, situations where one can (or should) consider <code>apply</code> as a serious alternative, especially in some <code>GroupBy</code> operations).</p>
</li>
</ul>
<p><sub>* Pandas string methods are &quot;vectorized&quot; in the sense that they are specified on the series but operate on each element. The underlying mechanisms are still iterative, because string operations are inherently hard to vectorize.</sub></p>
<hr />
<h2>Why I Wrote this Answer</h2>
<p>A common trend I notice from new users is to ask questions of the form &quot;How can I iterate over my df to do X?&quot;. Showing code that calls <code>iterrows()</code> while doing something inside a <code>for</code> loop. Here is why. A new user to the library who has not been introduced to the concept of vectorization will likely envision the code that solves their problem as iterating over their data to do something. Not knowing how to iterate over a DataFrame, the first thing they do is Google it and end up here, at this question. They then see the accepted answer telling them how to, and they close their eyes and run this code without ever first questioning if iteration is not the right thing to do.</p>
<p>The aim of this answer is to help new users understand that iteration is not necessarily the solution to every problem, and that better, faster and more idiomatic solutions could exist, and that it is worth investing time in exploring them. I'm not trying to start a war of iteration vs. vectorization, but I want new users to be informed when developing solutions to their problems with this library.</p>
","4909087","","205034","","2020-08-13 13:18:30","2020-08-13 13:18:30","","","","16","","","","CC BY-SA 4.0"
"58436037","2","","16476924","2019-10-17 15:26:30","","8","","<p>There is a way to iterate throw rows while getting a DataFrame in return, and not a Series. I don't see anyone mentioning that you can pass index as a list for the row to be returned as a DataFrame:</p>

<pre><code>for i in range(len(df)):
    row = df.iloc[[i]]
</code></pre>

<p>Note the usage of double brackets. This returns a DataFrame with a single row.</p>
","1871385","","","","","2019-10-17 15:26:30","","","","1","","","","CC BY-SA 4.0"
"59413206","2","","16476924","2019-12-19 16:02:14","","45","","<h1>How to iterate efficiently</h1>

<p>If you really have to iterate a Pandas dataframe, you will probably want to <strong>avoid using iterrows()</strong>. There are different methods and the usual <code>iterrows()</code> is far from being the best. <strong>itertuples() can be 100 times faster.</strong></p>

<p><strong>In short:</strong></p>

<ul>
<li>As a general rule, use <code>df.itertuples(name=None)</code>. In particular, when you have a fixed number columns and less than 255 columns. <em>See point (3)</em></li>
<li>Otherwise, use <code>df.itertuples()</code> except if your columns have special characters such as spaces or '-'. <em>See point (2)</em></li>
<li>It is possible to use <code>itertuples()</code> even if your dataframe has strange columns by using the last example. <em>See point (4)</em></li>
<li>Only use <code>iterrows()</code> if you cannot the previous solutions. <em>See point (1)</em></li>
</ul>

<h1>Different methods to iterate over rows in a Pandas dataframe:</h1>

<p>Generate a random dataframe with a million rows and 4 columns:</p>

<pre><code>    df = pd.DataFrame(np.random.randint(0, 100, size=(1000000, 4)), columns=list('ABCD'))
    print(df)
</code></pre>

<p>1) The usual <code>iterrows()</code> is convenient, but damn slow:</p>

<pre><code>start_time = time.clock()
result = 0
for _, row in df.iterrows():
    result += max(row['B'], row['C'])

total_elapsed_time = round(time.clock() - start_time, 2)
print(""1. Iterrows done in {} seconds, result = {}"".format(total_elapsed_time, result))
</code></pre>

<p>2) The default <code>itertuples()</code> is already much faster, but it doesn't work with column names such as <code>My Col-Name is very Strange</code> (you should avoid this method if your columns are repeated or if a column name cannot be simply converted to a Python variable name).:</p>

<pre><code>start_time = time.clock()
result = 0
for row in df.itertuples(index=False):
    result += max(row.B, row.C)

total_elapsed_time = round(time.clock() - start_time, 2)
print(""2. Named Itertuples done in {} seconds, result = {}"".format(total_elapsed_time, result))
</code></pre>

<p>3) The default <code>itertuples()</code> using name=None is even faster but not really convenient as you have to define a variable per column.</p>

<pre><code>start_time = time.clock()
result = 0
for(_, col1, col2, col3, col4) in df.itertuples(name=None):
    result += max(col2, col3)

total_elapsed_time = round(time.clock() - start_time, 2)
print(""3. Itertuples done in {} seconds, result = {}"".format(total_elapsed_time, result))
</code></pre>

<p>4) Finally, the named <code>itertuples()</code> is slower than the previous point, but you do not have to define a variable per column and it works with column names such as <code>My Col-Name is very Strange</code>.</p>

<pre><code>start_time = time.clock()
result = 0
for row in df.itertuples(index=False):
    result += max(row[df.columns.get_loc('B')], row[df.columns.get_loc('C')])

total_elapsed_time = round(time.clock() - start_time, 2)
print(""4. Polyvalent Itertuples working even with special characters in the column name done in {} seconds, result = {}"".format(total_elapsed_time, result))
</code></pre>

<p>Output:</p>

<pre><code>         A   B   C   D
0       41  63  42  23
1       54   9  24  65
2       15  34  10   9
3       39  94  82  97
4        4  88  79  54
...     ..  ..  ..  ..
999995  48  27   4  25
999996  16  51  34  28
999997   1  39  61  14
999998  66  51  27  70
999999  51  53  47  99

[1000000 rows x 4 columns]

1. Iterrows done in 104.96 seconds, result = 66151519
2. Named Itertuples done in 1.26 seconds, result = 66151519
3. Itertuples done in 0.94 seconds, result = 66151519
4. Polyvalent Itertuples working even with special characters in the column name done in 2.94 seconds, result = 66151519
</code></pre>

<p><a href=""https://medium.com/swlh/why-pandas-itertuples-is-faster-than-iterrows-and-how-to-make-it-even-faster-bc50c0edd30d"" rel=""noreferrer"">This article is a very interesting comparison between iterrows and itertuples</a></p>
","3292591","","63550","","2020-06-11 13:43:59","2020-06-11 13:43:59","","","","0","","","","CC BY-SA 4.0"
"60836700","2","","16476924","2020-03-24 17:57:16","","6","","<p><a href=""https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas/55557758#55557758"">cs95 shows</a> that Pandas vectorization far outperforms other Pandas methods for computing stuff with dataframes.</p>

<p>I wanted to add that if you first convert the dataframe to a NumPy array and then use vectorization, it's even faster than Pandas dataframe vectorization, (and that includes the time to turn it back into a dataframe series).</p>

<p>If you add the following functions to cs95's benchmark code, this becomes pretty evident:</p>

<pre class=""lang-py prettyprint-override""><code>def np_vectorization(df):
    np_arr = df.to_numpy()
    return pd.Series(np_arr[:,0] + np_arr[:,1], index=df.index)

def just_np_vectorization(df):
    np_arr = df.to_numpy()
    return np_arr[:,0] + np_arr[:,1]
</code></pre>

<p><a href=""https://i.stack.imgur.com/L0u4A.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/L0u4A.png"" alt=""Enter image description here""></a></p>
","10801098","","63550","","2020-06-11 12:39:21","2020-06-11 12:39:21","","","","0","","","","CC BY-SA 4.0"
"17071908","2","","17071871","2013-06-12 17:44:20","","4323","","<p>To select rows whose column value equals a scalar, <code>some_value</code>, use <code>==</code>:</p>

<pre><code>df.loc[df['column_name'] == some_value]
</code></pre>

<p>To select rows whose column value is in an iterable, <code>some_values</code>, use <code>isin</code>:</p>

<pre><code>df.loc[df['column_name'].isin(some_values)]
</code></pre>

<p>Combine multiple conditions with <code>&amp;</code>: </p>

<pre><code>df.loc[(df['column_name'] &gt;= A) &amp; (df['column_name'] &lt;= B)]
</code></pre>

<p>Note the parentheses. Due to Python's <a href=""https://docs.python.org/3/reference/expressions.html#operator-precedence"" rel=""noreferrer"">operator precedence rules</a>, <code>&amp;</code> binds more tightly than <code>&lt;=</code> and <code>&gt;=</code>. Thus, the parentheses in the last example are necessary. Without the parentheses </p>

<pre><code>df['column_name'] &gt;= A &amp; df['column_name'] &lt;= B
</code></pre>

<p>is parsed as </p>

<pre><code>df['column_name'] &gt;= (A &amp; df['column_name']) &lt;= B
</code></pre>

<p>which results in a <a href=""https://stackoverflow.com/questions/36921951/truth-value-of-a-series-is-ambiguous-use-a-empty-a-bool-a-item-a-any-o"">Truth value of a Series is ambiguous error</a>.</p>

<hr>

<p>To select rows whose column value <em>does not equal</em> <code>some_value</code>, use <code>!=</code>:</p>

<pre><code>df.loc[df['column_name'] != some_value]
</code></pre>

<p><code>isin</code> returns a boolean Series, so to select rows whose value is <em>not</em> in <code>some_values</code>, negate the boolean Series using <code>~</code>:</p>

<pre><code>df.loc[~df['column_name'].isin(some_values)]
</code></pre>

<hr>

<p>For example,</p>

<pre><code>import pandas as pd
import numpy as np
df = pd.DataFrame({'A': 'foo bar foo bar foo bar foo foo'.split(),
                   'B': 'one one two three two two one three'.split(),
                   'C': np.arange(8), 'D': np.arange(8) * 2})
print(df)
#      A      B  C   D
# 0  foo    one  0   0
# 1  bar    one  1   2
# 2  foo    two  2   4
# 3  bar  three  3   6
# 4  foo    two  4   8
# 5  bar    two  5  10
# 6  foo    one  6  12
# 7  foo  three  7  14

print(df.loc[df['A'] == 'foo'])
</code></pre>

<p>yields</p>

<pre><code>     A      B  C   D
0  foo    one  0   0
2  foo    two  2   4
4  foo    two  4   8
6  foo    one  6  12
7  foo  three  7  14
</code></pre>

<hr>

<p>If you have multiple values you want to include, put them in a
list (or more generally, any iterable) and use <code>isin</code>:</p>

<pre><code>print(df.loc[df['B'].isin(['one','three'])])
</code></pre>

<p>yields</p>

<pre><code>     A      B  C   D
0  foo    one  0   0
1  bar    one  1   2
3  bar  three  3   6
6  foo    one  6  12
7  foo  three  7  14
</code></pre>

<hr>

<p>Note, however, that if you wish to do this many times, it is more efficient to
make an index first, and then use <code>df.loc</code>:</p>

<pre><code>df = df.set_index(['B'])
print(df.loc['one'])
</code></pre>

<p>yields</p>

<pre><code>       A  C   D
B              
one  foo  0   0
one  bar  1   2
one  foo  6  12
</code></pre>

<p>or, to include multiple values from the index use <code>df.index.isin</code>:</p>

<pre><code>df.loc[df.index.isin(['one','two'])]
</code></pre>

<p>yields</p>

<pre><code>       A  C   D
B              
one  foo  0   0
one  bar  1   2
two  foo  2   4
two  foo  4   8
two  bar  5  10
one  foo  6  12
</code></pre>
","190597","","190597","","2019-01-18 02:47:47","2019-01-18 02:47:47","","","","29","","","","CC BY-SA 4.0"
"17086321","2","","17071871","2013-06-13 11:49:00","","23","","<p>Here is a simple example  </p>

<pre><code>from pandas import DataFrame

# Create data set
d = {'Revenue':[100,111,222], 
     'Cost':[333,444,555]}
df = DataFrame(d)


# mask = Return True when the value in column ""Revenue"" is equal to 111
mask = df['Revenue'] == 111

print mask

# Result:
# 0    False
# 1     True
# 2    False
# Name: Revenue, dtype: bool


# Select * FROM df WHERE Revenue = 111
df[mask]

# Result:
#    Cost    Revenue
# 1  444     111
</code></pre>
","1821873","","","","","2013-06-13 11:49:00","","","","0","","","","CC BY-SA 3.0"
"31296878","2","","17071871","2015-07-08 15:17:38","","291","","<h3>tl;dr</h3>
<p>The Pandas equivalent to</p>
<pre><code>select * from table where column_name = some_value
</code></pre>
<p>is</p>
<pre><code>table[table.column_name == some_value]
</code></pre>
<p>Multiple conditions:</p>
<pre><code>table[(table.column_name == some_value) | (table.column_name2 == some_value2)]
</code></pre>
<p>or</p>
<pre><code>table.query('column_name == some_value | column_name2 == some_value2')
</code></pre>
<h3>Code example</h3>
<pre><code>import pandas as pd

# Create data set
d = {'foo':[100, 111, 222],
     'bar':[333, 444, 555]}
df = pd.DataFrame(d)

# Full dataframe:
df

# Shows:
#    bar   foo
# 0  333   100
# 1  444   111
# 2  555   222

# Output only the row(s) in df where foo is 222:
df[df.foo == 222]

# Shows:
#    bar  foo
# 2  555  222
</code></pre>
<p>In the above code it is the line <code>df[df.foo == 222]</code> that gives the rows based on the column value, <code>222</code> in this case.</p>
<p>Multiple conditions are also possible:</p>
<pre><code>df[(df.foo == 222) | (df.bar == 444)]
#    bar  foo
# 1  444  111
# 2  555  222
</code></pre>
<p>But at that point I would recommend using the <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.query.html"" rel=""noreferrer"">query</a> function, since it's less verbose and yields the same result:</p>
<pre><code>df.query('foo == 222 | bar == 444')
</code></pre>
","955014","","63550","","2020-10-05 18:26:21","2020-10-05 18:26:21","","","","5","","","","CC BY-SA 4.0"
"35282530","2","","17071871","2016-02-09 01:36:49","","70","","<p>I find the syntax of the previous answers to be redundant and difficult to remember. Pandas introduced the <code>query()</code> method in v0.13 and I much prefer it. For your question, you could do <code>df.query('col == val')</code></p>

<p>Reproduced from <a href=""http://pandas.pydata.org/pandas-docs/version/0.17.0/indexing.html#indexing-query"">http://pandas.pydata.org/pandas-docs/version/0.17.0/indexing.html#indexing-query</a></p>

<pre><code>In [167]: n = 10

In [168]: df = pd.DataFrame(np.random.rand(n, 3), columns=list('abc'))

In [169]: df
Out[169]: 
          a         b         c
0  0.687704  0.582314  0.281645
1  0.250846  0.610021  0.420121
2  0.624328  0.401816  0.932146
3  0.011763  0.022921  0.244186
4  0.590198  0.325680  0.890392
5  0.598892  0.296424  0.007312
6  0.634625  0.803069  0.123872
7  0.924168  0.325076  0.303746
8  0.116822  0.364564  0.454607
9  0.986142  0.751953  0.561512

# pure python
In [170]: df[(df.a &lt; df.b) &amp; (df.b &lt; df.c)]
Out[170]: 
          a         b         c
3  0.011763  0.022921  0.244186
8  0.116822  0.364564  0.454607

# query
In [171]: df.query('(a &lt; b) &amp; (b &lt; c)')
Out[171]: 
          a         b         c
3  0.011763  0.022921  0.244186
8  0.116822  0.364564  0.454607
</code></pre>

<p>You can also access variables in the environment by prepending an <code>@</code>.</p>

<pre><code>exclude = ('red', 'orange')
df.query('color not in @exclude')
</code></pre>
","3533440","","","","","2016-02-09 01:36:49","","","","3","","","","CC BY-SA 3.0"
"40676816","2","","17071871","2016-11-18 12:10:42","","15","","<p>To append to this famous question (though a bit too late): You can also do <code>df.groupby('column_name').get_group('column_desired_value').reset_index()</code> to make a new data frame with specified column having a particular value. E.g.</p>

<pre><code>import pandas as pd
df = pd.DataFrame({'A': 'foo bar foo bar foo bar foo foo'.split(),
                   'B': 'one one two three two two one three'.split()})
print(""Original dataframe:"")
print(df)

b_is_two_dataframe = pd.DataFrame(df.groupby('B').get_group('two').reset_index()).drop('index', axis = 1) 
#NOTE: the final drop is to remove the extra index column returned by groupby object
print('Sub dataframe where B is two:')
print(b_is_two_dataframe)
</code></pre>

<p>Run this gives:</p>

<pre><code>Original dataframe:
     A      B
0  foo    one
1  bar    one
2  foo    two
3  bar  three
4  foo    two
5  bar    two
6  foo    one
7  foo  three
Sub dataframe where B is two:
     A    B
0  foo  two
1  foo  two
2  bar  two
</code></pre>
","3713927","","","","","2016-11-18 12:10:42","","","","1","","","","CC BY-SA 3.0"
"44931669","2","","17071871","2017-07-05 16:34:57","","25","","<p>Faster results can be achieved using <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html"" rel=""noreferrer"">numpy.where</a>. </p>

<p>For example, with <a href=""https://stackoverflow.com/questions/17071871/select-rows-from-a-dataframe-based-on-values-in-a-column-in-pandas/17071908#17071908"">unubtu's setup</a> -</p>

<pre><code>In [76]: df.iloc[np.where(df.A.values=='foo')]
Out[76]: 
     A      B  C   D
0  foo    one  0   0
2  foo    two  2   4
4  foo    two  4   8
6  foo    one  6  12
7  foo  three  7  14
</code></pre>

<p>Timing comparisons:</p>

<pre><code>In [68]: %timeit df.iloc[np.where(df.A.values=='foo')]  # fastest
1000 loops, best of 3: 380 s per loop

In [69]: %timeit df.loc[df['A'] == 'foo']
1000 loops, best of 3: 745 s per loop

In [71]: %timeit df.loc[df['A'].isin(['foo'])]
1000 loops, best of 3: 562 s per loop

In [72]: %timeit df[df.A=='foo']
1000 loops, best of 3: 796 s per loop

In [74]: %timeit df.query('(A==""foo"")')  # slowest
1000 loops, best of 3: 1.71 ms per loop
</code></pre>
","5907870","","243392","","2017-10-03 16:17:21","2017-10-03 16:17:21","","","","0","","","","CC BY-SA 3.0"
"46165056","2","","17071871","2017-09-11 22:14:28","","401","","<p>There are several ways to select rows from a Pandas data frame:</p>
<ol>
<li><strong>Boolean indexing (<code>df[df['col'] == value</code>] )</strong></li>
<li><strong>Positional indexing (<code>df.iloc[...]</code>)</strong></li>
<li><strong>Label indexing (<code>df.xs(...)</code>)</strong></li>
<li><strong><code>df.query(...)</code> API</strong></li>
</ol>
<p>Below I show you examples of each, with advice when to use certain techniques. Assume our criterion is column <code>'A'</code> == <code>'foo'</code></p>
<p>(Note on performance: For each base type, we can keep things simple by using the Pandas API or we can venture outside the API, usually into NumPy, and speed things up.)</p>
<hr />
<p><strong>Setup</strong></p>
<p>The first thing we'll need is to identify a condition that will act as our criterion for selecting rows. We'll start with the OP's case <code>column_name == some_value</code>, and include some other common use cases.</p>
<p>Borrowing from @unutbu:</p>
<pre><code>import pandas as pd, numpy as np

df = pd.DataFrame({'A': 'foo bar foo bar foo bar foo foo'.split(),
                   'B': 'one one two three two two one three'.split(),
                   'C': np.arange(8), 'D': np.arange(8) * 2})
</code></pre>
<hr />
<h1><strong>1. Boolean indexing</strong></h1>
<p>... Boolean indexing requires finding the true value of each row's <code>'A'</code> column being equal to <code>'foo'</code>, then using those truth values to identify which rows to keep.  Typically, we'd name this series, an array of truth values, <code>mask</code>.  We'll do so here as well.</p>
<pre><code>mask = df['A'] == 'foo'
</code></pre>
<p>We can then use this mask to slice or index the data frame</p>
<pre><code>df[mask]

     A      B  C   D
0  foo    one  0   0
2  foo    two  2   4
4  foo    two  4   8
6  foo    one  6  12
7  foo  three  7  14
</code></pre>
<p>This is one of the simplest ways to accomplish this task and if performance or intuitiveness isn't an issue, this should be your chosen method.  However, if performance is a concern, then you might want to consider an alternative way of creating the <code>mask</code>.</p>
<hr />
<h1><strong>2. Positional indexing</strong></h1>
<p>Positional indexing (<code>df.iloc[...]</code>) has its use cases, but this isn't one of them.  In order to identify where to slice, we first need to perform the same boolean analysis we did above.  This leaves us performing one extra step to accomplish the same task.</p>
<pre><code>mask = df['A'] == 'foo'
pos = np.flatnonzero(mask)
df.iloc[pos]

     A      B  C   D
0  foo    one  0   0
2  foo    two  2   4
4  foo    two  4   8
6  foo    one  6  12
7  foo  three  7  14
</code></pre>
<h1><strong>3. Label indexing</strong></h1>
<p><em>Label</em> indexing can be very handy, but in this case, we are again doing more work for no benefit</p>
<pre><code>df.set_index('A', append=True, drop=False).xs('foo', level=1)

     A      B  C   D
0  foo    one  0   0
2  foo    two  2   4
4  foo    two  4   8
6  foo    one  6  12
7  foo  three  7  14
</code></pre>
<h1><strong>4. <code>df.query()</code> API</strong></h1>
<p><em><code>pd.DataFrame.query</code></em> is a very elegant/intuitive way to perform this task, but is often slower. <strong>However</strong>, if you pay attention to the timings below, for large data, the query is very efficient. More so than the standard approach and of similar magnitude as my best suggestion.</p>
<pre><code>df.query('A == &quot;foo&quot;')

     A      B  C   D
0  foo    one  0   0
2  foo    two  2   4
4  foo    two  4   8
6  foo    one  6  12
7  foo  three  7  14
</code></pre>
<hr />
<p>My preference is to use the <code>Boolean</code> <code>mask</code></p>
<p>Actual improvements can be made by modifying how we create our <code>Boolean</code> <code>mask</code>.</p>
<p><strong><code>mask</code> alternative 1</strong>
<em>Use the underlying NumPy array and forgo the overhead of creating another <code>pd.Series</code></em></p>
<pre><code>mask = df['A'].values == 'foo'
</code></pre>
<p>I'll show more complete time tests at the end, but just take a look at the performance gains we get using the sample data frame.  First, we look at the difference in creating the <code>mask</code></p>
<pre><code>%timeit mask = df['A'].values == 'foo'
%timeit mask = df['A'] == 'foo'

5.84 s  195 ns per loop (mean  std. dev. of 7 runs, 100000 loops each)
166 s  4.45 s per loop (mean  std. dev. of 7 runs, 10000 loops each)
</code></pre>
<p>Evaluating the <code>mask</code> with the NumPy array is ~ 30 times faster.  This is partly due to NumPy evaluation often being faster. It is also partly due to the lack of overhead necessary to build an index and a corresponding <code>pd.Series</code> object.</p>
<p>Next, we'll look at the timing for slicing with one <code>mask</code> versus the other.</p>
<pre><code>mask = df['A'].values == 'foo'
%timeit df[mask]
mask = df['A'] == 'foo'
%timeit df[mask]

219 s  12.3 s per loop (mean  std. dev. of 7 runs, 1000 loops each)
239 s  7.03 s per loop (mean  std. dev. of 7 runs, 1000 loops each)
</code></pre>
<p>The performance gains aren't as pronounced.  We'll see if this holds up over more robust testing.</p>
<hr />
<p><strong><code>mask</code> alternative 2</strong>
We could have reconstructed the data frame as well.  There is a big caveat when reconstructing a dataframeyou must take care of the <code>dtypes</code> when doing so!</p>
<p>Instead of <code>df[mask]</code> we will do this</p>
<pre><code>pd.DataFrame(df.values[mask], df.index[mask], df.columns).astype(df.dtypes)
</code></pre>
<p>If the data frame is of mixed type, which our example is, then when we get <code>df.values</code> the resulting array is of <code>dtype</code> <code>object</code> and consequently, all columns of the new data frame will be of <code>dtype</code> <code>object</code>.  Thus requiring the <code>astype(df.dtypes)</code> and killing any potential performance gains.</p>
<pre><code>%timeit df[m]
%timeit pd.DataFrame(df.values[mask], df.index[mask], df.columns).astype(df.dtypes)

216 s  10.4 s per loop (mean  std. dev. of 7 runs, 1000 loops each)
1.43 ms  39.6 s per loop (mean  std. dev. of 7 runs, 1000 loops each)
</code></pre>
<p>However, if the data frame is not of mixed type, this is a very useful way to do it.</p>
<p>Given</p>
<pre><code>np.random.seed([3,1415])
d1 = pd.DataFrame(np.random.randint(10, size=(10, 5)), columns=list('ABCDE'))

d1

   A  B  C  D  E
0  0  2  7  3  8
1  7  0  6  8  6
2  0  2  0  4  9
3  7  3  2  4  3
4  3  6  7  7  4
5  5  3  7  5  9
6  8  7  6  4  7
7  6  2  6  6  5
8  2  8  7  5  8
9  4  7  6  1  5
</code></pre>
<hr />
<pre><code>%%timeit
mask = d1['A'].values == 7
d1[mask]

179 s  8.73 s per loop (mean  std. dev. of 7 runs, 10000 loops each)
</code></pre>
<p>Versus</p>
<pre><code>%%timeit
mask = d1['A'].values == 7
pd.DataFrame(d1.values[mask], d1.index[mask], d1.columns)

87 s  5.12 s per loop (mean  std. dev. of 7 runs, 10000 loops each)
</code></pre>
<p>We cut the time in half.</p>
<hr />
<p><strong><code>mask</code> alternative 3</strong></p>
<p>@unutbu also shows us how to use <code>pd.Series.isin</code> to account for each element of <code>df['A']</code> being in a set of values.  This evaluates to the same thing if our set of values is a set of one value, namely <code>'foo'</code>.  But it also generalizes to include larger sets of values if needed.  Turns out, this is still pretty fast even though it is a more general solution.  The only real loss is in intuitiveness for those not familiar with the concept.</p>
<pre><code>mask = df['A'].isin(['foo'])
df[mask]

     A      B  C   D
0  foo    one  0   0
2  foo    two  2   4
4  foo    two  4   8
6  foo    one  6  12
7  foo  three  7  14
</code></pre>
<p>However, as before, we can utilize NumPy to improve performance while sacrificing virtually nothing. We'll use <code>np.in1d</code></p>
<pre><code>mask = np.in1d(df['A'].values, ['foo'])
df[mask]

     A      B  C   D
0  foo    one  0   0
2  foo    two  2   4
4  foo    two  4   8
6  foo    one  6  12
7  foo  three  7  14
</code></pre>
<hr />
<p><strong>Timing</strong></p>
<p>I'll include other concepts mentioned in other posts as well for reference.</p>
<p><em>Code Below</em></p>
<p>Each <em>column</em> in this table represents a different length data frame over which we test each function. Each column shows relative time taken, with the fastest function given a base index of <code>1.0</code>.</p>
<pre><code>res.div(res.min())

                         10        30        100       300       1000      3000      10000     30000
mask_standard         2.156872  1.850663  2.034149  2.166312  2.164541  3.090372  2.981326  3.131151
mask_standard_loc     1.879035  1.782366  1.988823  2.338112  2.361391  3.036131  2.998112  2.990103
mask_with_values      1.010166  1.000000  1.005113  1.026363  1.028698  1.293741  1.007824  1.016919
mask_with_values_loc  1.196843  1.300228  1.000000  1.000000  1.038989  1.219233  1.037020  1.000000
query                 4.997304  4.765554  5.934096  4.500559  2.997924  2.397013  1.680447  1.398190
xs_label              4.124597  4.272363  5.596152  4.295331  4.676591  5.710680  6.032809  8.950255
mask_with_isin        1.674055  1.679935  1.847972  1.724183  1.345111  1.405231  1.253554  1.264760
mask_with_in1d        1.000000  1.083807  1.220493  1.101929  1.000000  1.000000  1.000000  1.144175
</code></pre>
<p>You'll notice that the fastest times seem to be shared between <code>mask_with_values</code> and <code>mask_with_in1d</code>.</p>
<pre><code>res.T.plot(loglog=True)
</code></pre>
<p><a href=""https://i.stack.imgur.com/ljeTd.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/ljeTd.png"" alt=""Enter image description here"" /></a></p>
<p><strong>Functions</strong></p>
<pre><code>def mask_standard(df):
    mask = df['A'] == 'foo'
    return df[mask]

def mask_standard_loc(df):
    mask = df['A'] == 'foo'
    return df.loc[mask]

def mask_with_values(df):
    mask = df['A'].values == 'foo'
    return df[mask]

def mask_with_values_loc(df):
    mask = df['A'].values == 'foo'
    return df.loc[mask]

def query(df):
    return df.query('A == &quot;foo&quot;')

def xs_label(df):
    return df.set_index('A', append=True, drop=False).xs('foo', level=-1)

def mask_with_isin(df):
    mask = df['A'].isin(['foo'])
    return df[mask]

def mask_with_in1d(df):
    mask = np.in1d(df['A'].values, ['foo'])
    return df[mask]
</code></pre>
<hr />
<p><strong>Testing</strong></p>
<pre><code>res = pd.DataFrame(
    index=[
        'mask_standard', 'mask_standard_loc', 'mask_with_values', 'mask_with_values_loc',
        'query', 'xs_label', 'mask_with_isin', 'mask_with_in1d'
    ],
    columns=[10, 30, 100, 300, 1000, 3000, 10000, 30000],
    dtype=float
)

for j in res.columns:
    d = pd.concat([df] * j, ignore_index=True)
    for i in res.index:a
        stmt = '{}(d)'.format(i)
        setp = 'from __main__ import d, {}'.format(i)
        res.at[i, j] = timeit(stmt, setp, number=50)
</code></pre>
<hr />
<p><strong>Special Timing</strong></p>
<p>Looking at the special case when we have a single non-object <code>dtype</code> for the entire data frame.</p>
<p><em>Code Below</em></p>
<pre><code>spec.div(spec.min())

                     10        30        100       300       1000      3000      10000     30000
mask_with_values  1.009030  1.000000  1.194276  1.000000  1.236892  1.095343  1.000000  1.000000
mask_with_in1d    1.104638  1.094524  1.156930  1.072094  1.000000  1.000000  1.040043  1.027100
reconstruct       1.000000  1.142838  1.000000  1.355440  1.650270  2.222181  2.294913  3.406735
</code></pre>
<p>Turns out, reconstruction isn't worth it past a few hundred rows.</p>
<pre><code>spec.T.plot(loglog=True)
</code></pre>
<p><a href=""https://i.stack.imgur.com/K1bNc.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/K1bNc.png"" alt=""Enter image description here"" /></a></p>
<p><strong>Functions</strong></p>
<pre><code>np.random.seed([3,1415])
d1 = pd.DataFrame(np.random.randint(10, size=(10, 5)), columns=list('ABCDE'))

def mask_with_values(df):
    mask = df['A'].values == 'foo'
    return df[mask]

def mask_with_in1d(df):
    mask = np.in1d(df['A'].values, ['foo'])
    return df[mask]

def reconstruct(df):
    v = df.values
    mask = np.in1d(df['A'].values, ['foo'])
    return pd.DataFrame(v[mask], df.index[mask], df.columns)

spec = pd.DataFrame(
    index=['mask_with_values', 'mask_with_in1d', 'reconstruct'],
    columns=[10, 30, 100, 300, 1000, 3000, 10000, 30000],
    dtype=float
)
</code></pre>
<p><strong>Testing</strong></p>
<pre><code>for j in spec.columns:
    d = pd.concat([df] * j, ignore_index=True)
    for i in spec.index:
        stmt = '{}(d)'.format(i)
        setp = 'from __main__ import d, {}'.format(i)
        spec.at[i, j] = timeit(stmt, setp, number=50)
</code></pre>
","2336654","","63550","","2020-10-05 18:32:13","2020-10-05 18:32:13","","","","5","","","","CC BY-SA 4.0"
"47693145","2","","17071871","2017-12-07 10:39:50","","18","","<p>For selecting only specific columns out of multiple columns for a given value in Pandas:</p>
<pre><code>select col_name1, col_name2 from table where column_name = some_value.
</code></pre>
<p>Options:</p>
<pre><code>df.loc[df['column_name'] == some_value][[col_name1, col_name2]]
</code></pre>
<p>or</p>
<pre><code>df.query['column_name' == 'some_value'][[col_name1, col_name2]]
</code></pre>
","8714519","","63550","","2020-10-05 18:33:02","2020-10-05 18:33:02","","","","0","","","","CC BY-SA 4.0"
"53674430","2","","17071871","2018-12-07 17:38:58","","8","","<p>You can also use .apply:</p>

<pre><code>df.apply(lambda row: row[df['B'].isin(['one','three'])])
</code></pre>

<p>It actually works row-wise (i.e., applies the function to each row).</p>

<p>The output is </p>

<pre><code>   A      B  C   D
0  foo    one  0   0
1  bar    one  1   2
3  bar  three  3   6
6  foo    one  6  12
7  foo  three  7  14
</code></pre>

<p>The results is the same as using as mentioned by @unutbu</p>

<pre><code>df[[df['B'].isin(['one','three'])]]
</code></pre>
","4979951","","","","","2018-12-07 17:38:58","","","","0","","","","CC BY-SA 4.0"
"57338153","2","","17071871","2019-08-03 12:05:15","","35","","<h3>More flexibility using <code>.query</code> with Pandas &gt;= 0.25.0:</h3>
<p><em>August 2019 updated answer</em></p>
<p>Since Pandas &gt;= 0.25.0 we can use the <code>query</code> method to filter dataframes with Pandas methods and even column names which have spaces. Normally the spaces in column names would give an error, but now we can solve that using a backtick (`) - see <a href=""https://github.com/pandas-dev/pandas/issues/6508"" rel=""nofollow noreferrer"">GitHub</a>:</p>
<pre><code># Example dataframe
df = pd.DataFrame({'Sender email':['ex@example.com', &quot;reply@shop.com&quot;, &quot;buy@shop.com&quot;]})

     Sender email
0  ex@example.com
1  reply@shop.com
2    buy@shop.com
</code></pre>
<p>Using <code>.query</code> with method <code>str.endswith</code>:</p>
<pre><code>df.query('`Sender email`.str.endswith(&quot;@shop.com&quot;)')
</code></pre>
<p><strong>Output</strong></p>
<pre><code>     Sender email
1  reply@shop.com
2    buy@shop.com
</code></pre>
<hr />
<p>Also we can use local variables by prefixing it with an <code>@</code> in our query:</p>
<pre><code>domain = 'shop.com'
df.query('`Sender email`.str.endswith(@domain)')
</code></pre>
<p><strong>Output</strong></p>
<pre><code>     Sender email
1  reply@shop.com
2    buy@shop.com
</code></pre>
","9081267","","63550","","2020-10-05 18:34:56","2020-10-05 18:34:56","","","","0","","","","CC BY-SA 4.0"
"19482988","2","","19482970","2013-10-20 21:20:06","","25","","<p>That's available as <code>my_dataframe.columns</code>.</p>
","1427416","","1427416","","2014-01-23 18:50:27","2014-01-23 18:50:27","","","","2","","","","CC BY-SA 3.0"
"19483025","2","","19482970","2013-10-20 21:23:07","","1719","","<p>You can get the values as a list by doing:</p>

<pre><code>list(my_dataframe.columns.values)
</code></pre>

<p>Also you can simply use: (as shown in <a href=""https://stackoverflow.com/a/19483602/4909087"">Ed Chum's answer</a>):</p>

<pre><code>list(my_dataframe)
</code></pre>
","1267329","","4909087","","2019-04-03 09:51:08","2019-04-03 09:51:08","","","","11","","","","CC BY-SA 4.0"
"19483602","2","","19482970","2013-10-20 22:25:15","","424","","<p>There is a built in method which is the most performant:</p>

<pre><code>my_dataframe.columns.values.tolist()
</code></pre>

<p><code>.columns</code> returns an Index, <code>.columns.values</code> returns an array and this has a helper function <code>.tolist</code> to return a list.</p>

<p>If performance is not as important to you, <code>Index</code> objects define a <code>.tolist()</code> method that you can call directly:</p>

<pre><code>my_dataframe.columns.tolist()
</code></pre>

<p>The difference in performance is obvious:</p>

<pre><code>%timeit df.columns.tolist()
16.7 s  317 ns per loop (mean  std. dev. of 7 runs, 100000 loops each)

%timeit df.columns.values.tolist()
1.24 s  12.3 ns per loop (mean  std. dev. of 7 runs, 1000000 loops each)
</code></pre>

<hr>

<p>For those who hate typing, you can just call <code>list</code> on <code>df</code>, as so:</p>

<pre><code>list(df)
</code></pre>
","704848","","4909087","","2019-04-03 09:26:22","2019-04-03 09:26:22","","","","0","","","","CC BY-SA 4.0"
"21315199","2","","19482970","2014-01-23 17:23:40","","19","","<p>A <a href=""https://pandas.pydata.org/pandas-docs/stable/api.html#dataframe"" rel=""noreferrer"">DataFrame</a> follows the dict-like convention of iterating over the keys of the objects.</p>

<pre><code>my_dataframe.keys()
</code></pre>

<p>Create a list of keys/columns - object method <code>to_list()</code> and pythonic way</p>

<pre><code>my_dataframe.keys().to_list()
list(my_dataframe.keys())
</code></pre>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/basics.html#iteration"" rel=""noreferrer"">Basic iteration</a> on a DataFrame returns column labels</p>

<pre><code>[column for column in my_dataframe]
</code></pre>

<p>Do not convert a DataFrame into a list, just to get the column labels. Do not stop thinking while looking for convenient code samples.</p>

<pre class=""lang-py prettyprint-override""><code>xlarge = pd.DataFrame(np.arange(100000000).reshape(10000,10000))
list(xlarge) #compute time and memory consumption depend on dataframe size - O(N)
list(xlarge.keys()) #constant time operation - O(1)
</code></pre>
","1126386","","1126386","","2018-05-31 11:25:37","2018-05-31 11:25:37","","","","2","","","","CC BY-SA 4.0"
"27236748","2","","19482970","2014-12-01 20:31:56","","94","","<p>Did some quick tests, and perhaps unsurprisingly the built-in version using <code>dataframe.columns.values.tolist()</code> is the fastest:</p>

<pre><code>In [1]: %timeit [column for column in df]
1000 loops, best of 3: 81.6 s per loop

In [2]: %timeit df.columns.values.tolist()
10000 loops, best of 3: 16.1 s per loop

In [3]: %timeit list(df)
10000 loops, best of 3: 44.9 s per loop

In [4]: % timeit list(df.columns.values)
10000 loops, best of 3: 38.4 s per loop
</code></pre>

<p>(I still really like the <code>list(dataframe)</code> though, so thanks EdChum!)</p>
","4174485","","4174485","","2015-03-13 16:01:05","2015-03-13 16:01:05","","","","0","","","","CC BY-SA 3.0"
"29494537","2","","19482970","2015-04-07 14:50:33","","53","","<p>Its gets even simpler (by pandas 0.16.0) : </p>

<pre><code>df.columns.tolist()
</code></pre>

<p>will give you the column names in a nice list.</p>
","170005","","170005","","2015-04-26 20:23:02","2015-04-26 20:23:02","","","","0","","","","CC BY-SA 3.0"
"30511605","2","","19482970","2015-05-28 15:58:05","","39","","<pre><code>&gt;&gt;&gt; list(my_dataframe)
['y', 'gdp', 'cap']
</code></pre>

<p>To list the columns of a dataframe while in debugger mode, use a list comprehension:</p>

<pre><code>&gt;&gt;&gt; [c for c in my_dataframe]
['y', 'gdp', 'cap']
</code></pre>

<p>By the way, you can get a sorted list simply by using <code>sorted</code>:</p>

<pre><code>&gt;&gt;&gt; sorted(my_dataframe)
['cap', 'gdp', 'y']
</code></pre>
","2411802","","2411802","","2018-01-26 22:25:24","2018-01-26 22:25:24","","","","2","","","","CC BY-SA 3.0"
"34097939","2","","19482970","2015-12-04 21:41:53","","19","","<p>It's interesting but <code>df.columns.values.tolist()</code> is almost 3 times faster then <code>df.columns.tolist()</code> but I thought that they are the same:</p>

<pre><code>In [97]: %timeit df.columns.values.tolist()
100000 loops, best of 3: 2.97 s per loop

In [98]: %timeit df.columns.tolist()
10000 loops, best of 3: 9.67 s per loop
</code></pre>
","4542359","","","","","2015-12-04 21:41:53","","","","1","","","","CC BY-SA 3.0"
"36302134","2","","19482970","2016-03-30 07:19:35","","14","","<h2>In the Notebook</h2>
<p>For data exploration in the IPython notebook, my preferred way is this:</p>
<pre><code>sorted(df)
</code></pre>
<p>Which will produce an easy to read alphabetically ordered list.</p>
<h2>In a code repository</h2>
<p>In code I find it more explicit to do</p>
<pre><code>df.columns
</code></pre>
<p>Because it tells others reading your code what you are doing.</p>
","3730397","","-1","","2020-06-20 09:12:55","2016-03-30 07:19:35","","","","2","","","","CC BY-SA 3.0"
"55491499","2","","19482970","2019-04-03 09:18:29","","35","","<p>Surprised I haven't seen this posted so far, so I'll just leave this here.</p>

<h1>Extended Iterable Unpacking (python3.5+): <code>[*df]</code> and Friends</h1>

<p><a href=""https://www.python.org/dev/peps/pep-0448/"" rel=""noreferrer"">Unpacking generalizations (PEP 448)</a> have been introduced with Python 3.5. So, the following operations are all possible.</p>

<pre><code>df = pd.DataFrame('x', columns=['A', 'B', 'C'], index=range(5))
df

   A  B  C
0  x  x  x
1  x  x  x
2  x  x  x
3  x  x  x
4  x  x  x 
</code></pre>

<p></p>

<p>If you want a <code>list</code>....</p>

<pre><code>[*df]
# ['A', 'B', 'C']
</code></pre>

<p>Or, if you want a <code>set</code>,</p>

<pre><code>{*df}
# {'A', 'B', 'C'}
</code></pre>

<p>Or, if you want a <code>tuple</code>,</p>

<pre><code>*df,  # Please note the trailing comma
# ('A', 'B', 'C')
</code></pre>

<p>Or, if you want to store the result somewhere, </p>

<pre><code>*cols, = df  # A wild comma appears, again
cols
# ['A', 'B', 'C']
</code></pre>

<p>... if you're the kind of person who converts coffee to typing sounds, well, this is going consume your coffee more efficiently ;)</p>

<blockquote>
  <p>P.S.: if performance is important, you will want to ditch the
  solutions above in favour of</p>

<pre><code>df.columns.to_numpy().tolist()
# ['A', 'B', 'C']
</code></pre>
  
  <p>This is similar to <a href=""https://stackoverflow.com/a/19483602/4909087"">Ed Chum's
  answer</a>, but updated for
  v0.24 where <code>.to_numpy()</code> is preferred to the use of <code>.values</code>. See
  <a href=""https://stackoverflow.com/a/54324513/4909087"">this answer</a> (by me)
  for more information.</p>
</blockquote>

<p><strong>Visual Check</strong><br>
Since I've seen this discussed in other answers, you can utilise iterable unpacking (no need for explicit loops).</p>

<pre><code>print(*df)
A B C

print(*df, sep='\n')
A
B
C
</code></pre>

<hr>

<h1>Critique of Other Methods</h1>

<p>Don't use an explicit <code>for</code> loop for an operation that can be done in a single line (List comprehensions are okay). </p>

<p>Next, using <code>sorted(df)</code> <strong>does not preserve the original order</strong> of the columns. For that, you should use <code>list(df)</code> instead.  </p>

<p>Next, <code>list(df.columns)</code> and <code>list(df.columns.values)</code> are poor suggestions (as of the current version, v0.24). Both <code>Index</code> (returned from <code>df.columns</code>) and NumPy arrays (returned by <code>df.columns.values</code>) define <code>.tolist()</code> method which is faster and more idiomatic. </p>

<p>Lastly, listification i.e., <code>list(df)</code> should only be used as a concise alternative to the aforementioned methods for python &lt;= 3.4 where extended unpacking is not available.</p>
","4909087","","4909087","","2020-03-02 07:50:06","2020-03-02 07:50:06","","","","0","","","","CC BY-SA 4.0"
"55701903","2","","19482970","2019-04-16 06:32:43","","10","","<pre><code>%%timeit
final_df.columns.values.tolist()
948 ns  19.2 ns per loop (mean  std. dev. of 7 runs, 1000000 loops each)
</code></pre>

<pre><code>%%timeit
list(final_df.columns)
14.2 s  79.1 ns per loop (mean  std. dev. of 7 runs, 100000 loops each)
</code></pre>

<pre><code>%%timeit
list(final_df.columns.values)
1.88 s  11.7 ns per loop (mean  std. dev. of 7 runs, 1000000 loops each)
</code></pre>

<pre><code>%%timeit
final_df.columns.tolist()
12.3 s  27.4 ns per loop (mean  std. dev. of 7 runs, 100000 loops each)
</code></pre>

<pre><code>%%timeit
list(final_df.head(1).columns)
163 s  20.6 s per loop (mean  std. dev. of 7 runs, 10000 loops each)
</code></pre>
","9709555","","","","","2019-04-16 06:32:43","","","","0","","","","CC BY-SA 4.0"